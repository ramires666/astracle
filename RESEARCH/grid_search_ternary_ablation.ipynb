{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Body Ablation Search for Ternary Classification\n",
    "\n",
    "This notebook performs **body ablation analysis** for the ternary classification model.\n",
    "\n",
    "**Goal**: Find which astrological bodies hurt/help the model by testing all combinations of excluding 0-2 bodies.\n",
    "\n",
    "## Fixed Parameters (from best previous search):\n",
    "- **Labeling**: horizon=1, gauss_window=201\n",
    "- **Features**: helio, orb_multiplier=0.1\n",
    "- **Model**: depth=9, lr=0.03, sideways_penalty=0.3\n",
    "\n",
    "## What it does:\n",
    "1. **Baseline**: No body exclusion\n",
    "2. **Exclude 1 body**: 11 tests (Sun, Moon, Mercury, ...)\n",
    "3. **Exclude 2 bodies**: 55 tests (all pairs)\n",
    "\n",
    "**Total: 67 combinations**\n",
    "\n",
    "## Output:\n",
    "- TOP 10 exclusions by UP/DOWN balance\n",
    "- Which bodies are IMPORTANT (hurt when removed)\n",
    "- Which bodies are NOISE (help when removed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:25:55) [GCC 14.3.0]\n",
      "PROJECT_ROOT: /home/rut/ostrofun\n"
     ]
    }
   ],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# ENVIRONMENT CHECK & PATH SETUP\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# We need to find the project root directory and add it to Python's path.\n",
    "# This allows us to import our custom RESEARCH modules from anywhere.\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Find project root by looking for RESEARCH folder\n",
    "PROJECT_ROOT = Path.cwd().resolve()\n",
    "if not (PROJECT_ROOT / \"RESEARCH\").exists():\n",
    "    for parent in PROJECT_ROOT.parents:\n",
    "        if (parent / \"RESEARCH\").exists():\n",
    "            PROJECT_ROOT = parent\n",
    "            break\n",
    "\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"PROJECT_ROOT: {PROJECT_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All modules imported\n"
     ]
    }
   ],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# IMPORTS\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "import itertools\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# NOTE: tqdm removed - causes output blocking in notebooks\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# RESEARCH modules\n",
    "from RESEARCH.config import cfg, PROJECT_ROOT\n",
    "from RESEARCH.data_loader import load_market_data\n",
    "from RESEARCH.labeling import create_ternary_labels\n",
    "from RESEARCH.astro_engine import (\n",
    "    init_ephemeris,\n",
    "    calculate_bodies_for_dates_multi,\n",
    "    calculate_aspects_for_dates,\n",
    "    calculate_phases_for_dates,\n",
    ")\n",
    "from RESEARCH.features import (\n",
    "    build_full_features,\n",
    "    merge_features_with_labels,\n",
    "    get_feature_columns,\n",
    ")\n",
    "from RESEARCH.model_training import (\n",
    "    split_dataset,\n",
    "    prepare_xy,\n",
    "    train_xgb_model,\n",
    "    calc_metrics,\n",
    "    check_cuda_available,\n",
    "    compute_stronger_weights,\n",
    ")\n",
    "from RESEARCH.cache_utils import (\n",
    "    save_cache,\n",
    "    load_cache,\n",
    "    cache_exists,\n",
    "    list_cache,\n",
    "    get_cache_path,\n",
    ")\n",
    "\n",
    "print(\"‚úì All modules imported\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total body exclusion combinations: 67\n",
      "  - Baseline (no exclusion): 1\n",
      "  - Exclude 1 body: 11\n",
      "  - Exclude 2 bodies: 55\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# BODY ABLATION GRID SEARCH FOR TERNARY MODEL\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# This notebook tests removing combinations of 2 bodies to find which hurt/help performance.\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "DATA_START = \"2017-11-01\"\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# FIXED LABELING PARAMS (use best from previous search)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "LABEL_GRID = {\n",
    "    \"horizon\": [1],\n",
    "    \"gauss_window\": [201],\n",
    "    \"gauss_std\": [50.0],\n",
    "}\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# FIXED FEATURE PARAMS (use best from previous search)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "FEATURE_GRID = {\n",
    "    \"coord_mode\": [\"both\"],\n",
    "    \"orb_multiplier\": [0.15],\n",
    "    \"include_phases\": [True],\n",
    "}\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# FIXED MODEL PARAMS (use best from previous search)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "MODEL_GRID = {\n",
    "    \"max_depth\": [9],\n",
    "    \"learning_rate\": [0.03],\n",
    "    \"weight_power\": [2.0],\n",
    "    \"sideways_penalty\": [0.3],\n",
    "    \"n_estimators\": [500],\n",
    "    \"subsample\": [0.8],\n",
    "    \"colsample_bytree\": [0.8],\n",
    "}\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# BODY ABLATION CONFIGURATION\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# All available bodies in our astro system\n",
    "ALL_BODIES = [\n",
    "    \"Sun\", \"Moon\", \"Mercury\", \"Venus\", \"Mars\", \n",
    "    \"Jupiter\", \"Saturn\", \"Uranus\", \"Neptune\", \"Pluto\", \"Node\"\n",
    "]\n",
    "\n",
    "# Generate all combinations of 2 bodies to EXCLUDE\n",
    "MAX_EXCLUDE = 2\n",
    "EXCLUDE_COMBOS = [[]]  # Start with baseline (no exclusion)\n",
    "for n in range(1, MAX_EXCLUDE + 1):\n",
    "    for combo in combinations(ALL_BODIES, n):\n",
    "        EXCLUDE_COMBOS.append(list(combo))\n",
    "\n",
    "print(f\"Total body exclusion combinations: {len(EXCLUDE_COMBOS)}\")\n",
    "print(f\"  - Baseline (no exclusion): 1\")\n",
    "print(f\"  - Exclude 1 body: {len(ALL_BODIES)}\")\n",
    "print(f\"  - Exclude 2 bodies: {len(ALL_BODIES) * (len(ALL_BODIES)-1) // 2}\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# SEARCH SETTINGS\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "TEST_MODE = False  # Run full ablation search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search Statistics:\n",
      "  Label combinations:   1\n",
      "  Feature combinations: 1\n",
      "  Model combinations:   1\n",
      "  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  TOTAL COMBINATIONS:   1\n"
     ]
    }
   ],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# CALCULATE TOTAL COMBINATIONS\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "def count_combinations(grid: Dict[str, List]) -> int:\n",
    "    \"\"\"Count total combinations in a parameter grid.\"\"\"\n",
    "    total = 1\n",
    "    for values in grid.values():\n",
    "        total *= len(values)\n",
    "    return total\n",
    "\n",
    "label_combos = count_combinations(LABEL_GRID)\n",
    "feature_combos = count_combinations(FEATURE_GRID)\n",
    "model_combos = count_combinations(MODEL_GRID)\n",
    "total_combos = label_combos * feature_combos * model_combos\n",
    "\n",
    "print(f\"\\nGrid Search Statistics:\")\n",
    "print(f\"  Label combinations:   {label_combos}\")\n",
    "print(f\"  Feature combinations: {feature_combos}\")\n",
    "print(f\"  Model combinations:   {model_combos}\")\n",
    "print(f\"  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "print(f\"  TOTAL COMBINATIONS:   {total_combos}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Load Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading from cache: market__data__btc_2017-11-01__8d63944e.parquet\n",
      "\n",
      "Market data: 3014 rows\n",
      "Date range: 2017-11-01 ‚Üí 2026-01-31\n"
     ]
    }
   ],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# LOAD MARKET DATA (WITH CACHING)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "MARKET_CACHE_PARAMS = {\n",
    "    \"subject_id\": cfg.active_subject_id,\n",
    "    \"start_date\": DATA_START,\n",
    "}\n",
    "\n",
    "df_market = load_cache(\"market\", \"data\", MARKET_CACHE_PARAMS, verbose=True)\n",
    "\n",
    "if df_market is None:\n",
    "    print(\"Loading from database...\")\n",
    "    df_market = load_market_data()\n",
    "    df_market = df_market[df_market[\"date\"] >= DATA_START].reset_index(drop=True)\n",
    "    save_cache(df_market, \"market\", \"data\", MARKET_CACHE_PARAMS)\n",
    "\n",
    "print(f\"\\nMarket data: {len(df_market)} rows\")\n",
    "print(f\"Date range: {df_market['date'].min().date()} ‚Üí {df_market['date'].max().date()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3. Initialize Ephemeris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bodies: ['Sun', 'Moon', 'Mercury', 'Venus', 'Mars', 'Jupiter', 'Saturn', 'Uranus', 'Neptune', 'Pluto', 'TrueNode', 'MeanNode', 'Lilith']\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# INITIALIZE ASTRO SETTINGS\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "settings = init_ephemeris()\n",
    "print(f\"Bodies: {[b.name for b in settings.bodies]}\")\n",
    "\n",
    "# Check CUDA availability\n",
    "use_cuda, device = check_cuda_available()\n",
    "print(f\"Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4. Grid Search Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# GRID SEARCH HELPER FUNCTIONS\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "def generate_grid_combinations(grid: Dict[str, List]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generate all combinations of parameter values from a grid.\n",
    "    \n",
    "    Args:\n",
    "        grid: Dictionary of parameter_name -> list of values\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries, each containing one combination\n",
    "    \n",
    "    Example:\n",
    "        grid = {\"a\": [1, 2], \"b\": [\"x\", \"y\"]}\n",
    "        result = [{\"a\": 1, \"b\": \"x\"}, {\"a\": 1, \"b\": \"y\"}, ...]\n",
    "    \"\"\"\n",
    "    keys = list(grid.keys())\n",
    "    values = list(grid.values())\n",
    "    \n",
    "    combinations = []\n",
    "    for combo in itertools.product(*values):\n",
    "        combinations.append(dict(zip(keys, combo)))\n",
    "    \n",
    "    return combinations\n",
    "\n",
    "\n",
    "def get_or_compute_labels(\n",
    "    df_market: pd.DataFrame,\n",
    "    label_params: Dict,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get labels from cache or compute them.\n",
    "    \n",
    "    This is the first step where caching saves the most time,\n",
    "    since label computation involves detrending which is slow.\n",
    "    \"\"\"\n",
    "    cache_params = {\n",
    "        \"subject_id\": cfg.active_subject_id,\n",
    "        \"start_date\": DATA_START,\n",
    "        \"type\": \"ternary\",\n",
    "        **label_params,\n",
    "    }\n",
    "    \n",
    "    # Try cache\n",
    "    df_labels = load_cache(\"labels\", \"ternary\", cache_params, verbose=False)\n",
    "    \n",
    "    if df_labels is None:\n",
    "        # Compute\n",
    "        df_labels, threshold = create_ternary_labels(\n",
    "            df_market,\n",
    "            horizon=label_params[\"horizon\"],\n",
    "            gauss_window=label_params[\"gauss_window\"],\n",
    "            gauss_std=label_params[\"gauss_std\"],\n",
    "            balance_classes=True,\n",
    "            verbose=False,\n",
    "        )\n",
    "        save_cache(df_labels, \"labels\", \"ternary\", cache_params, verbose=False)\n",
    "    \n",
    "    return df_labels\n",
    "\n",
    "\n",
    "def get_or_compute_features(\n",
    "    df_market: pd.DataFrame,\n",
    "    settings,\n",
    "    feature_params: Dict,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get features from cache or compute them.\n",
    "    \n",
    "    Features are the most expensive to compute (bodies + aspects + phases).\n",
    "    Caching here saves significant time in grid search.\n",
    "    \"\"\"\n",
    "    date_range = f\"{df_market['date'].min().date()}_{df_market['date'].max().date()}\"\n",
    "    \n",
    "    cache_params = {\n",
    "        \"coord_mode\": feature_params[\"coord_mode\"],\n",
    "        \"orb_mult\": feature_params[\"orb_multiplier\"],\n",
    "        \"include_phases\": feature_params[\"include_phases\"],\n",
    "        \"date_range\": date_range,\n",
    "    }\n",
    "    \n",
    "    # Try cache for complete feature set\n",
    "    df_features = load_cache(\"features\", \"ternary\", cache_params, verbose=False)\n",
    "    \n",
    "    if df_features is None:\n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        # Compute bodies (most time-consuming)\n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        bodies_cache_params = {\n",
    "            \"coord_mode\": feature_params[\"coord_mode\"],\n",
    "            \"date_range\": date_range,\n",
    "        }\n",
    "        \n",
    "        df_bodies = load_cache(\"astro\", \"bodies\", bodies_cache_params, verbose=False)\n",
    "        geo_by_date = load_cache(\"astro\", \"bodies_geo_dict\", bodies_cache_params, verbose=False)\n",
    "        helio_by_date = load_cache(\"astro\", \"bodies_helio_dict\", bodies_cache_params, verbose=False)\n",
    "        \n",
    "        # If bodies not cached, compute them\n",
    "        if df_bodies is None:\n",
    "            df_bodies, geo_by_date, helio_by_date = calculate_bodies_for_dates_multi(\n",
    "                df_market[\"date\"],\n",
    "                settings,\n",
    "                coord_mode=feature_params[\"coord_mode\"],\n",
    "                progress=False,\n",
    "            )\n",
    "            save_cache(df_bodies, \"astro\", \"bodies\", bodies_cache_params, verbose=False)\n",
    "            save_cache(geo_by_date, \"astro\", \"bodies_geo_dict\", bodies_cache_params, verbose=False)\n",
    "            if helio_by_date:\n",
    "                save_cache(helio_by_date, \"astro\", \"bodies_helio_dict\", bodies_cache_params, verbose=False)\n",
    "        \n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        # Compute aspects (needs geo_by_date)\n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        aspects_cache_params = {\n",
    "            \"orb_mult\": feature_params[\"orb_multiplier\"],\n",
    "            \"coord_mode\": feature_params[\"coord_mode\"],\n",
    "            \"date_range\": date_range,\n",
    "        }\n",
    "        \n",
    "        df_aspects = load_cache(\"astro\", \"aspects\", aspects_cache_params, verbose=False)\n",
    "        \n",
    "        # Only compute if not cached AND we have geo_by_date\n",
    "        if df_aspects is None:\n",
    "            if geo_by_date is not None:\n",
    "                df_aspects = calculate_aspects_for_dates(\n",
    "                    geo_by_date,\n",
    "                    settings,\n",
    "                    orb_mult=feature_params[\"orb_multiplier\"],\n",
    "                    progress=False,\n",
    "                )\n",
    "                save_cache(df_aspects, \"astro\", \"aspects\", aspects_cache_params, verbose=False)\n",
    "            else:\n",
    "                # Create empty aspects DataFrame if geo_by_date not available\n",
    "                df_aspects = pd.DataFrame(columns=[\"date\"])\n",
    "        \n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        # Compute phases\n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        phases_cache_params = {\"date_range\": date_range}\n",
    "        \n",
    "        df_phases = None\n",
    "        if feature_params[\"include_phases\"]:\n",
    "            df_phases = load_cache(\"astro\", \"phases\", phases_cache_params, verbose=False)\n",
    "            \n",
    "            if df_phases is None and geo_by_date is not None:\n",
    "                df_phases = calculate_phases_for_dates(geo_by_date, progress=False)\n",
    "                save_cache(df_phases, \"astro\", \"phases\", phases_cache_params, verbose=False)\n",
    "        \n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        # Build feature matrix\n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        df_features = build_full_features(\n",
    "            df_bodies,\n",
    "            df_aspects,\n",
    "            df_phases=df_phases,\n",
    "            include_pair_aspects=True,\n",
    "        )\n",
    "        \n",
    "        save_cache(df_features, \"features\", \"ternary\", cache_params, verbose=False)\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "\n",
    "def run_single_experiment(\n",
    "    df_dataset: pd.DataFrame,\n",
    "    model_params: Dict,\n",
    "    device: str,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Run a single training experiment with given parameters.\n",
    "    \n",
    "    Returns metrics on test set.\n",
    "    \"\"\"\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # Split data\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    train_df, val_df, test_df = split_dataset(df_dataset, train_ratio=0.7, val_ratio=0.15)\n",
    "    \n",
    "    feature_cols = get_feature_columns(df_dataset)\n",
    "    X_train, y_train = prepare_xy(train_df, feature_cols)\n",
    "    X_val, y_val = prepare_xy(val_df, feature_cols)\n",
    "    X_test, y_test = prepare_xy(test_df, feature_cols)\n",
    "    \n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # Extract weight_power from model_params\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    weight_power = model_params.pop(\"weight_power\", 2.0)\n",
    "    \n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # Train model\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    model = train_xgb_model(\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        feature_cols,\n",
    "        n_classes=3,\n",
    "        device=device,\n",
    "        weight_power=weight_power,\n",
    "        verbose=False,\n",
    "        **model_params,\n",
    "    )\n",
    "    \n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # Evaluate on test set\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    X_test_scaled = model.scaler.transform(X_test)\n",
    "    y_proba = model.model.predict_proba(X_test_scaled)\n",
    "    y_pred = y_proba.argmax(axis=1)\n",
    "    \n",
    "    metrics = calc_metrics(y_test, y_pred, labels=[0, 1, 2])\n",
    "    \n",
    "    # Add weight_power back for logging\n",
    "    model_params[\"weight_power\"] = weight_power\n",
    "    \n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# EVALUATION & VISUALIZATION FUNCTION\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# This function can be called:\n",
    "#   - BEFORE grid search as baseline\n",
    "#   - AFTER grid search with best model\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "def evaluate_and_visualize_model(\n",
    "    model,\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    df_test: pd.DataFrame,\n",
    "    title: str = \"Model Evaluation\",\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    COMPREHENSIVE MODEL EVALUATION WITH VISUALIZATIONS\n",
    "    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    \n",
    "    Displays:\n",
    "    1. All metrics (accuracy, balanced accuracy, MCC, F1, per-class recalls)\n",
    "    2. Confusion matrix heatmap\n",
    "    3. Two price charts with class labels:\n",
    "       - Top: TRUE labels (from data)\n",
    "       - Bottom: PREDICTED labels (from model)\n",
    "    \n",
    "    Args:\n",
    "        model: Trained XGBBaseline model\n",
    "        X_test: Test features (NOT scaled)\n",
    "        y_test: True labels\n",
    "        df_test: Test DataFrame with 'date' and 'close' columns\n",
    "        title: Title for the evaluation\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with all metrics\n",
    "    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    \"\"\"\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # Get predictions\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    X_test_scaled = model.scaler.transform(X_test)\n",
    "    y_proba = model.model.predict_proba(X_test_scaled)\n",
    "    y_pred = y_proba.argmax(axis=1)\n",
    "    \n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # Calculate metrics\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    metrics = calc_metrics(y_test, y_pred, labels=[0, 1, 2])\n",
    "    \n",
    "    CLASS_NAMES = [\"DOWN\", \"SIDEWAYS\", \"UP\"]\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"  {title}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\nüìä METRICS:\")\n",
    "    print(f\"  Accuracy:          {metrics['acc']:.4f}\")\n",
    "    print(f\"  Balanced Accuracy: {metrics['bal_acc']:.4f}\")\n",
    "    print(f\"  MCC:               {metrics['mcc']:.4f}\")\n",
    "    print(f\"  F1 Macro:          {metrics['f1_macro']:.4f}\")\n",
    "    \n",
    "    print(\"\\nüìà PER-CLASS RECALL:\")\n",
    "    print(f\"  DOWN (0):     {metrics['recall_down']:.4f}\")\n",
    "    \n",
    "    # Calculate SIDEWAYS recall manually\n",
    "    mask_sideways = (y_test == 1)\n",
    "    recall_sideways = (y_pred[mask_sideways] == 1).sum() / mask_sideways.sum() if mask_sideways.sum() > 0 else 0\n",
    "    print(f\"  SIDEWAYS (1): {recall_sideways:.4f}\")\n",
    "    print(f\"  UP (2):       {metrics['recall_up']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nüéØ RECALL MIN:  {metrics['recall_min']:.4f} (quality)\")\n",
    "    print(f\"   RECALL GAP:  {metrics['recall_gap']:.4f} (balance)\")\n",
    "    \n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # Classification report\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    print(\"\\nüìã CLASSIFICATION REPORT:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=CLASS_NAMES, zero_division=0))\n",
    "    \n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # Create figure with 3 subplots\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    plt.style.use('default')\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 12), facecolor='white')\n",
    "    for ax in axes:\n",
    "        ax.set_facecolor('white')\n",
    "    \n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # Subplot 1: Confusion Matrix\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=[0, 1, 2])\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "        xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,\n",
    "        ax=axes[0]\n",
    "    )\n",
    "    axes[0].set_xlabel(\"Predicted\")\n",
    "    axes[0].set_ylabel(\"True\")\n",
    "    axes[0].set_title(f\"Confusion Matrix | Bal.Acc={metrics['bal_acc']:.3f}, Recall MIN={metrics['recall_min']:.3f}\")\n",
    "    \n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # Subplot 2: Price with TRUE labels\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    dates = df_test[\"date\"].values\n",
    "    prices = df_test[\"close\"].values\n",
    "    \n",
    "    # Color mapping: DOWN=red, SIDEWAYS=gray, UP=green\n",
    "    COLORS = {0: \"#e63946\", 1: \"#FFFAFA\", 2: \"#2d6a4f\"}\n",
    "    \n",
    "    ax = axes[1]\n",
    "    ax.plot(dates, prices, color=\"black\", linewidth=1, alpha=0.8)\n",
    "    \n",
    "    # Fill background with true class colors\n",
    "    for i in range(len(dates) - 1):\n",
    "        ax.axvspan(dates[i], dates[i+1], alpha=0.3, color=COLORS[y_test[i]], linewidth=0)\n",
    "    \n",
    "    ax.set_title(\"TRUE Labels (Ground Truth)\")\n",
    "    ax.set_ylabel(\"Price\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor=COLORS[0], alpha=0.5, label=\"DOWN\"),\n",
    "        Patch(facecolor=COLORS[1], alpha=0.5, label=\"SIDEWAYS\"),\n",
    "        Patch(facecolor=COLORS[2], alpha=0.5, label=\"UP\"),\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc=\"upper left\")\n",
    "    \n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # Subplot 3: Price with PREDICTED labels\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    ax = axes[2]\n",
    "    ax.plot(dates, prices, color=\"black\", linewidth=1, alpha=0.8)\n",
    "    \n",
    "    # Fill background with predicted class colors\n",
    "    for i in range(len(dates) - 1):\n",
    "        ax.axvspan(dates[i], dates[i+1], alpha=0.3, color=COLORS[y_pred[i]], linewidth=0)\n",
    "    \n",
    "    ax.set_title(\"PREDICTED Labels (Model Output)\")\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Price\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(handles=legend_elements, loc=\"upper left\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(title, fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_and_evaluate_full(\n",
    "    df_dataset: pd.DataFrame,\n",
    "    model_params: Dict,\n",
    "    device: str,\n",
    "    title: str = \"Model Evaluation\",\n",
    "    df_market: pd.DataFrame = None,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Convenience function: train model and run full evaluation.\n",
    "    \n",
    "    Use this for baseline comparison before grid search\n",
    "    and for final evaluation after grid search.\n",
    "    \n",
    "    Args:\n",
    "        df_dataset: Dataset with features and labels\n",
    "        model_params: Model hyperparameters\n",
    "        device: 'cpu' or 'cuda'\n",
    "        title: Title for plots\n",
    "        df_market: Original market data with 'close' column (for visualization)\n",
    "    \"\"\"\n",
    "    # Split data\n",
    "    train_df, val_df, test_df = split_dataset(df_dataset, train_ratio=0.7, val_ratio=0.15)\n",
    "    \n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # Add 'close' column if missing (needed for price chart)\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    if \"close\" not in test_df.columns and df_market is not None:\n",
    "        test_df = test_df.merge(\n",
    "            df_market[[\"date\", \"close\"]], \n",
    "            on=\"date\", \n",
    "            how=\"left\"\n",
    "        )\n",
    "    elif \"close\" not in test_df.columns:\n",
    "        # Create dummy price column if df_market not provided\n",
    "        test_df = test_df.copy()\n",
    "        test_df[\"close\"] = 1.0  # Placeholder\n",
    "    \n",
    "    feature_cols = get_feature_columns(df_dataset)\n",
    "    X_train, y_train = prepare_xy(train_df, feature_cols)\n",
    "    X_val, y_val = prepare_xy(val_df, feature_cols)\n",
    "    X_test, y_test = prepare_xy(test_df, feature_cols)\n",
    "    \n",
    "    # Extract weight_power\n",
    "    weight_power = model_params.pop(\"weight_power\", 2.0) if \"weight_power\" in model_params else 2.0\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"Training model with weight_power={weight_power}...\")\n",
    "    model = train_xgb_model(\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        feature_cols,\n",
    "        n_classes=3,\n",
    "        device=device,\n",
    "        weight_power=weight_power,\n",
    "        verbose=False,\n",
    "        **model_params,\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = evaluate_and_visualize_model(model, X_test, y_test, test_df, title)\n",
    "    \n",
    "    return metrics, model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5. Run Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached features approach\n"
     ]
    }
   ],
   "source": [
    "# PRE-COMPUTE cell removed - using get_or_compute_features instead\n",
    "# (it has its own caching)\n",
    "print(\"Using cached features approach\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì filter_features_by_exclusion() defined\n"
     ]
    }
   ],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# SIMPLE FEATURE FILTERING FUNCTION\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "def filter_features_by_exclusion(df_features: pd.DataFrame, exclude_bodies: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter out feature columns related to excluded bodies.\n",
    "    \n",
    "    Args:\n",
    "        df_features: Full features DataFrame\n",
    "        exclude_bodies: List of body names to exclude (e.g., ['Moon', 'Venus'])\n",
    "    \n",
    "    Returns:\n",
    "        Filtered DataFrame\n",
    "    \"\"\"\n",
    "    if not exclude_bodies:\n",
    "        return df_features\n",
    "    \n",
    "    df_filtered = df_features.copy()\n",
    "    \n",
    "    for body in exclude_bodies:\n",
    "        body_lower = body.lower()\n",
    "        # Find columns containing body name (case-insensitive)\n",
    "        cols_to_drop = [c for c in df_filtered.columns \n",
    "                        if body_lower in c.lower() and c != 'date' and c != 'target']\n",
    "        df_filtered = df_filtered.drop(columns=cols_to_drop, errors='ignore')\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "print(\"‚úì filter_features_by_exclusion() defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting ablation with 67 combinations...\n",
      "Device: cuda\n",
      "Computing full features (will be cached)...\n",
      "Merged dataset: 3014 samples (ALL days, forward-filled)\n",
      "Features: 834\n",
      "Full dataset: 3014 samples, 834 features\n",
      "Split: Train=2109, Val=452, Test=453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/w/WSL/btc/lib/python3.12/site-packages/xgboost/core.py:774: UserWarning: [02:10:21] WARNING: /workspace/src/common/error_msg.cc:62: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1/67] D=0.61 U=0.00 | ud_min=0.00 | f=834 | BASELINE\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[  2/67] D=0.65 U=0.00 | ud_min=0.00 | f=720 | Sun\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[  3/67] D=0.57 U=0.00 | ud_min=0.00 | f=701 | Moon\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[  4/67] D=0.56 U=0.00 | ud_min=0.00 | f=716 | Mercury\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[  5/67] D=0.62 U=0.00 | ud_min=0.00 | f=716 | Venus\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[  6/67] D=0.55 U=0.00 | ud_min=0.00 | f=702 | Mars\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[  7/67] D=0.56 U=0.00 | ud_min=0.00 | f=714 | Jupiter\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[  8/67] D=0.73 U=0.02 | ud_min=0.02 | f=724 | Saturn\n",
      "  ‚≠ê NEW BEST!\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[  9/67] D=0.71 U=0.00 | ud_min=0.00 | f=732 | Uranus\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 10/67] D=0.68 U=0.00 | ud_min=0.00 | f=730 | Neptune\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 11/67] D=0.59 U=0.04 | ud_min=0.04 | f=732 | Pluto\n",
      "  ‚≠ê NEW BEST!\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 12/67] D=0.80 U=0.00 | ud_min=0.00 | f=604 | Node\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 13/67] D=0.62 U=0.00 | ud_min=0.00 | f=597 | Sun,Moon\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 14/67] D=0.58 U=0.00 | ud_min=0.00 | f=604 | Sun,Mercury\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 15/67] D=0.58 U=0.00 | ud_min=0.00 | f=604 | Sun,Venus\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 16/67] D=0.68 U=0.00 | ud_min=0.00 | f=598 | Sun,Mars\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 17/67] D=0.74 U=0.00 | ud_min=0.00 | f=610 | Sun,Jupiter\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 18/67] D=0.52 U=0.08 | ud_min=0.08 | f=620 | Sun,Saturn\n",
      "  ‚≠ê NEW BEST!\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 19/67] D=0.72 U=0.00 | ud_min=0.00 | f=628 | Sun,Uranus\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 20/67] D=0.70 U=0.00 | ud_min=0.00 | f=626 | Sun,Neptune\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 21/67] D=0.59 U=0.02 | ud_min=0.02 | f=628 | Sun,Pluto\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 22/67] D=0.66 U=0.02 | ud_min=0.02 | f=510 | Sun,Node\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 23/67] D=0.53 U=0.00 | ud_min=0.00 | f=593 | Moon,Mercury\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 24/67] D=0.46 U=0.00 | ud_min=0.00 | f=593 | Moon,Venus\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 25/67] D=0.74 U=0.00 | ud_min=0.00 | f=579 | Moon,Mars\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 26/67] D=0.61 U=0.00 | ud_min=0.00 | f=591 | Moon,Jupiter\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 27/67] D=0.58 U=0.00 | ud_min=0.00 | f=601 | Moon,Saturn\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 28/67] D=0.47 U=0.02 | ud_min=0.02 | f=609 | Moon,Uranus\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 29/67] D=0.67 U=0.00 | ud_min=0.00 | f=607 | Moon,Neptune\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 30/67] D=0.50 U=0.00 | ud_min=0.00 | f=609 | Moon,Pluto\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 31/67] D=0.70 U=0.00 | ud_min=0.00 | f=491 | Moon,Node\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 32/67] D=0.53 U=0.00 | ud_min=0.00 | f=602 | Mercury,Venus\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 33/67] D=0.47 U=0.00 | ud_min=0.00 | f=594 | Mercury,Mars\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 34/67] D=0.54 U=0.00 | ud_min=0.00 | f=606 | Mercury,Jupiter\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 35/67] D=0.44 U=0.00 | ud_min=0.00 | f=616 | Mercury,Saturn\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 36/67] D=0.65 U=0.00 | ud_min=0.00 | f=624 | Mercury,Uranus\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 37/67] D=0.48 U=0.00 | ud_min=0.00 | f=622 | Mercury,Neptune\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 38/67] D=0.53 U=0.06 | ud_min=0.06 | f=624 | Mercury,Pluto\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 39/67] D=0.48 U=0.00 | ud_min=0.00 | f=506 | Mercury,Node\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 40/67] D=0.59 U=0.00 | ud_min=0.00 | f=594 | Venus,Mars\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 41/67] D=0.58 U=0.00 | ud_min=0.00 | f=606 | Venus,Jupiter\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 42/67] D=0.59 U=0.00 | ud_min=0.00 | f=616 | Venus,Saturn\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 43/67] D=0.61 U=0.00 | ud_min=0.00 | f=624 | Venus,Uranus\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 44/67] D=0.57 U=0.00 | ud_min=0.00 | f=622 | Venus,Neptune\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 45/67] D=0.49 U=0.00 | ud_min=0.00 | f=624 | Venus,Pluto\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 46/67] D=0.69 U=0.00 | ud_min=0.00 | f=506 | Venus,Node\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 47/67] D=0.50 U=0.00 | ud_min=0.00 | f=592 | Mars,Jupiter\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 48/67] D=0.69 U=0.00 | ud_min=0.00 | f=602 | Mars,Saturn\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 49/67] D=0.58 U=0.10 | ud_min=0.10 | f=610 | Mars,Uranus\n",
      "  ‚≠ê NEW BEST!\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 50/67] D=0.67 U=0.00 | ud_min=0.00 | f=608 | Mars,Neptune\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 51/67] D=0.56 U=0.00 | ud_min=0.00 | f=610 | Mars,Pluto\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 52/67] D=0.56 U=0.00 | ud_min=0.00 | f=492 | Mars,Node\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 53/67] D=0.59 U=0.00 | ud_min=0.00 | f=612 | Jupiter,Saturn\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 54/67] D=0.59 U=0.00 | ud_min=0.00 | f=620 | Jupiter,Uranus\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 55/67] D=0.67 U=0.00 | ud_min=0.00 | f=618 | Jupiter,Neptune\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 56/67] D=0.68 U=0.00 | ud_min=0.00 | f=620 | Jupiter,Pluto\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 57/67] D=0.56 U=0.00 | ud_min=0.00 | f=504 | Jupiter,Node\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 58/67] D=0.78 U=0.00 | ud_min=0.00 | f=628 | Saturn,Uranus\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 59/67] D=0.48 U=0.00 | ud_min=0.00 | f=624 | Saturn,Neptune\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 60/67] D=0.50 U=0.00 | ud_min=0.00 | f=624 | Saturn,Pluto\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 61/67] D=0.72 U=0.00 | ud_min=0.00 | f=514 | Saturn,Node\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 62/67] D=0.62 U=0.00 | ud_min=0.00 | f=630 | Uranus,Neptune\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 63/67] D=0.53 U=0.00 | ud_min=0.00 | f=632 | Uranus,Pluto\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 64/67] D=0.69 U=0.00 | ud_min=0.00 | f=514 | Uranus,Node\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 65/67] D=0.49 U=0.00 | ud_min=0.00 | f=630 | Neptune,Pluto\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 66/67] D=0.52 U=0.00 | ud_min=0.00 | f=516 | Neptune,Node\n",
      "Split: Train=2109, Val=452, Test=453\n",
      "[ 67/67] D=0.58 U=0.02 | ud_min=0.02 | f=518 | Pluto,Node\n",
      "\n",
      "‚úì Ablation completed in 2.2 minutes\n",
      "\n",
      "üèÜ BEST: up_down_min=0.104\n",
      "   Exclude: ['Mars', 'Uranus']\n"
     ]
    }
   ],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# RUN BODY ABLATION SEARCH\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# Fixed params\n",
    "label_params = {\n",
    "    \"horizon\": LABEL_GRID[\"horizon\"][0],\n",
    "    \"gauss_window\": LABEL_GRID[\"gauss_window\"][0],\n",
    "    \"gauss_std\": LABEL_GRID[\"gauss_std\"][0],\n",
    "}\n",
    "feature_params = {\n",
    "    \"coord_mode\": FEATURE_GRID[\"coord_mode\"][0],\n",
    "    \"orb_multiplier\": FEATURE_GRID[\"orb_multiplier\"][0],\n",
    "    \"include_phases\": FEATURE_GRID[\"include_phases\"][0],\n",
    "}\n",
    "model_params = {\n",
    "    \"max_depth\": MODEL_GRID[\"max_depth\"][0],\n",
    "    \"learning_rate\": MODEL_GRID[\"learning_rate\"][0],\n",
    "    \"weight_power\": MODEL_GRID[\"weight_power\"][0],\n",
    "    \"sideways_penalty\": MODEL_GRID[\"sideways_penalty\"][0],\n",
    "    \"n_estimators\": MODEL_GRID[\"n_estimators\"][0],\n",
    "    \"subsample\": MODEL_GRID[\"subsample\"][0],\n",
    "    \"colsample_bytree\": MODEL_GRID[\"colsample_bytree\"][0],\n",
    "}\n",
    "\n",
    "total_combos = len(EXCLUDE_COMBOS)\n",
    "print(f\"\\nStarting ablation with {total_combos} combinations...\")\n",
    "print(f\"Device: {device}\")\n",
    "start_time = time.time()\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Pre-compute FULL features (no exclusion) - this is cached\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"Computing full features (will be cached)...\")\n",
    "df_features_full = get_or_compute_features(df_market, settings, feature_params)\n",
    "df_labels = get_or_compute_labels(df_market, label_params)\n",
    "df_dataset_full = merge_features_with_labels(df_features_full, df_labels)\n",
    "print(f\"Full dataset: {len(df_dataset_full)} samples, {len(get_feature_columns(df_dataset_full))} features\")\n",
    "\n",
    "results = []\n",
    "best_up_down_min = 0.0\n",
    "best_exclude = []\n",
    "\n",
    "for idx, exclude_list in enumerate(EXCLUDE_COMBOS):\n",
    "    try:\n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        # Filter features by removing excluded body columns\n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        df_dataset = filter_features_by_exclusion(df_dataset_full, exclude_list)\n",
    "        \n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        # Run experiment\n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        metrics = run_single_experiment(df_dataset, model_params.copy(), device)\n",
    "        \n",
    "        up_down_min = min(metrics[\"recall_down\"], metrics[\"recall_up\"])\n",
    "        up_down_gap = abs(metrics[\"recall_down\"] - metrics[\"recall_up\"])\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            \"exclude_bodies\": \",\".join(exclude_list) if exclude_list else \"NONE\",\n",
    "            \"n_excluded\": len(exclude_list),\n",
    "            \"n_features\": len(get_feature_columns(df_dataset)),\n",
    "            \"bal_acc\": metrics[\"bal_acc\"],\n",
    "            \"recall_down\": metrics[\"recall_down\"],\n",
    "            \"recall_up\": metrics[\"recall_up\"],\n",
    "            \"up_down_min\": up_down_min,\n",
    "            \"up_down_gap\": up_down_gap,\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        # Print progress\n",
    "        exclude_str = \",\".join(exclude_list) if exclude_list else \"BASELINE\"\n",
    "        print(\n",
    "            f\"[{idx+1:3d}/{total_combos}] D={metrics['recall_down']:.2f} U={metrics['recall_up']:.2f} \"\n",
    "            f\"| ud_min={up_down_min:.2f} | f={len(get_feature_columns(df_dataset)):3d} | {exclude_str}\"\n",
    "        )\n",
    "        \n",
    "        if up_down_min > best_up_down_min:\n",
    "            best_up_down_min = up_down_min\n",
    "            best_exclude = exclude_list\n",
    "            print(f\"  ‚≠ê NEW BEST!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"[{idx+1:3d}/{total_combos}] ‚ö†Ô∏è Error: {e}\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n‚úì Ablation completed in {elapsed/60:.1f} minutes\")\n",
    "print(f\"\\nüèÜ BEST: up_down_min={best_up_down_min:.3f}\")\n",
    "print(f\"   Exclude: {best_exclude if best_exclude else 'NONE'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üèÜ TOP 10 BODY EXCLUSIONS BY UP/DOWN BALANCE\n",
      "================================================================================\n",
      "exclude_bodies  n_excluded  n_features  recall_down  recall_up  up_down_min  up_down_gap  bal_acc\n",
      "   Mars,Uranus           2         610     0.579208   0.104167     0.104167     0.475041 0.365723\n",
      "    Sun,Saturn           2         620     0.524752   0.083333     0.083333     0.441419 0.358689\n",
      " Mercury,Pluto           2         624     0.529703   0.062500     0.062500     0.467203 0.358321\n",
      "         Pluto           1         732     0.594059   0.041667     0.041667     0.552393 0.374470\n",
      "   Moon,Uranus           2         609     0.470297   0.020833     0.020833     0.449464 0.344334\n",
      "    Pluto,Node           2         518     0.579208   0.020833     0.020833     0.558375 0.339587\n",
      "     Sun,Pluto           2         628     0.589109   0.020833     0.020833     0.568276 0.357665\n",
      "      Sun,Node           2         510     0.658416   0.020833     0.020833     0.637583 0.352853\n",
      "        Saturn           1         724     0.732673   0.020833     0.020833     0.711840 0.333271\n",
      "Mercury,Saturn           2         616     0.440594   0.000000     0.000000     0.440594 0.357045\n",
      "\n",
      "================================================================================\n",
      "BASELINE vs BEST COMPARISON\n",
      "================================================================================\n",
      "\n",
      "BASELINE (no exclusion):\n",
      "  D=0.609, U=0.000, ud_min=0.000\n",
      "\n",
      "BEST (exclude: Mars,Uranus):\n",
      "  D=0.579, U=0.104, ud_min=0.104\n",
      "\n",
      "\n",
      "================================================================================\n",
      "SINGLE BODY EXCLUSION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Best to exclude (improves model):\n",
      "exclude_bodies  recall_down  recall_up  up_down_min\n",
      "         Pluto     0.594059   0.041667     0.041667\n",
      "        Saturn     0.732673   0.020833     0.020833\n",
      "          Mars     0.549505   0.000000     0.000000\n",
      "       Jupiter     0.559406   0.000000     0.000000\n",
      "       Mercury     0.564356   0.000000     0.000000\n",
      "\n",
      "Worst to exclude (hurts model - these are IMPORTANT bodies):\n",
      "exclude_bodies  recall_down  recall_up  up_down_min\n",
      "         Venus     0.618812        0.0          0.0\n",
      "           Sun     0.653465        0.0          0.0\n",
      "       Neptune     0.683168        0.0          0.0\n",
      "        Uranus     0.707921        0.0          0.0\n",
      "          Node     0.797030        0.0          0.0\n"
     ]
    }
   ],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# ANALYZE ABLATION RESULTS\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "if not df_results.empty:\n",
    "    # Sort by UP/DOWN balance\n",
    "    df_results = df_results.sort_values(\n",
    "        [\"up_down_min\", \"up_down_gap\"], \n",
    "        ascending=[False, True]\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üèÜ TOP 10 BODY EXCLUSIONS BY UP/DOWN BALANCE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    display_cols = [\n",
    "        \"exclude_bodies\", \"n_excluded\", \"n_features\",\n",
    "        \"recall_down\", \"recall_up\", \"up_down_min\", \"up_down_gap\", \"bal_acc\"\n",
    "    ]\n",
    "    print(df_results[display_cols].head(10).to_string(index=False))\n",
    "    \n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # Compare baseline vs best\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    baseline = df_results[df_results[\"exclude_bodies\"] == \"NONE\"]\n",
    "    best = df_results.iloc[0]\n",
    "    \n",
    "    if not baseline.empty:\n",
    "        bl = baseline.iloc[0]\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"BASELINE vs BEST COMPARISON\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\nBASELINE (no exclusion):\")\n",
    "        print(f\"  D={bl['recall_down']:.3f}, U={bl['recall_up']:.3f}, ud_min={bl['up_down_min']:.3f}\")\n",
    "        print(f\"\\nBEST (exclude: {best['exclude_bodies']}):\")\n",
    "        print(f\"  D={best['recall_down']:.3f}, U={best['recall_up']:.3f}, ud_min={best['up_down_min']:.3f}\")\n",
    "        \n",
    "        improvement = best['up_down_min'] - bl['up_down_min']\n",
    "        print(f\"\\nüìà Improvement: {improvement:+.3f} ({improvement/bl['up_down_min']*100:+.1f}%)\" if bl['up_down_min'] > 0 else \"\")\n",
    "    \n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # Which bodies hurt when removed?\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SINGLE BODY EXCLUSION ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    single_exclusions = df_results[df_results[\"n_excluded\"] == 1].copy()\n",
    "    single_exclusions = single_exclusions.sort_values(\"up_down_min\", ascending=False)\n",
    "    print(\"\\nBest to exclude (improves model):\")\n",
    "    print(single_exclusions[[\"exclude_bodies\", \"recall_down\", \"recall_up\", \"up_down_min\"]].head(5).to_string(index=False))\n",
    "    \n",
    "    print(\"\\nWorst to exclude (hurts model - these are IMPORTANT bodies):\")\n",
    "    print(single_exclusions[[\"exclude_bodies\", \"recall_down\", \"recall_up\", \"up_down_min\"]].tail(5).to_string(index=False))\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results to analyze\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Sorted by up_down_min. Best: 0.104\n",
      "\n",
      "‚úì Results saved to: /home/rut/ostrofun/RESEARCH/reports/grid_search_ternary_20260204_021228.csv\n"
     ]
    }
   ],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# SAVE RESULTS TO CSV\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# SORT BY UP/DOWN BALANCE\n",
    "if not df_results.empty and \"up_down_min\" in df_results.columns:\n",
    "    df_results = df_results.sort_values(\n",
    "        [\"up_down_min\", \"up_down_gap\"], \n",
    "        ascending=[False, True]\n",
    "    ).reset_index(drop=True)\n",
    "    print(f\"‚úì Sorted by up_down_min. Best: {df_results['up_down_min'].iloc[0]:.3f}\")\n",
    "    \n",
    "if not df_results.empty:\n",
    "    reports_dir = PROJECT_ROOT / \"RESEARCH\" / \"reports\"\n",
    "    reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    report_path = reports_dir / f\"grid_search_ternary_{timestamp}.csv\"\n",
    "    \n",
    "    df_results.to_csv(report_path, index=False)\n",
    "    print(f\"\\n‚úì Results saved to: {report_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üèÜ BEST BODY EXCLUSION\n",
      "================================================================================\n",
      "\n",
      "Exclude: Mars,Uranus\n",
      "Features: 610\n",
      "\n",
      "Metrics:\n",
      "  Recall DOWN:  0.5792\n",
      "  Recall UP:    0.1042\n",
      "  UP/DOWN min:  0.1042\n",
      "  UP/DOWN gap:  0.4750\n",
      "  Balanced Acc: 0.3657\n"
     ]
    }
   ],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# BEST BODY EXCLUSION SUMMARY\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "if not df_results.empty:\n",
    "    best = df_results.iloc[0]\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üèÜ BEST BODY EXCLUSION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nExclude: {best['exclude_bodies']}\")\n",
    "    print(f\"Features: {best['n_features']}\")\n",
    "    \n",
    "    print(f\"\\nMetrics:\")\n",
    "    print(f\"  Recall DOWN:  {best['recall_down']:.4f}\")\n",
    "    print(f\"  Recall UP:    {best['recall_up']:.4f}\")\n",
    "    print(f\"  UP/DOWN min:  {best['up_down_min']:.4f}\")\n",
    "    print(f\"  UP/DOWN gap:  {best['up_down_gap']:.4f}\")\n",
    "    print(f\"  Balanced Acc: {best['bal_acc']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ Cached files after grid search:\n",
      "category              name                               params  size_mb\n",
      "   astro           aspects          2017-11-01_2026_both_orb0.5 0.308278\n",
      "   astro           aspects         2017-11-01_2026_both_orb0.25 0.181644\n",
      "   astro           aspects         2017-11-01_2026_both_orb0.15 0.117138\n",
      "   astro           aspects          2017-11-01_2026_both_orb0.1 0.080457\n",
      "   astro           aspects         2017-11-01_2026_helio_orb0.5 0.000607\n",
      "   astro           aspects        2017-11-01_2026_helio_orb0.25 0.000607\n",
      "   astro           aspects        2017-11-01_2026_helio_orb0.15 0.000607\n",
      "   astro           aspects         2017-11-01_2026_helio_orb0.1 0.000607\n",
      "   astro           aspects          2017-11-01_2026_both_orb1.0 0.551617\n",
      "   astro           aspects           2017-11-01_2026_geo_orb1.0 0.551617\n",
      "   astro            bodies                2017-11-01_2026_helio 1.081944\n",
      "   astro            bodies                 2017-11-01_2026_both 2.508477\n",
      "   astro            bodies                  2017-11-01_2026_geo 1.425784\n",
      "   astro       bodies_dict                  2017-11-01_2026_geo 3.086050\n",
      "   astro   bodies_geo_dict                2017-11-01_2026_helio 0.000005\n",
      "   astro   bodies_geo_dict                 2017-11-01_2026_both 3.086050\n",
      "   astro bodies_helio_dict                2017-11-01_2026_helio 3.057394\n",
      "   astro bodies_helio_dict                 2017-11-01_2026_both 3.057394\n",
      "   astro            phases                      2017-11-01_2026 0.743544\n",
      "features           ternary   2017-11-01_2026_both_orb0.5_phases 3.374214\n",
      "features           ternary  2017-11-01_2026_both_orb0.25_phases 3.262551\n",
      "features           ternary  2017-11-01_2026_both_orb0.15_phases 3.207394\n",
      "features           ternary   2017-11-01_2026_both_orb0.1_phases 3.172903\n",
      "features           ternary  2017-11-01_2026_helio_orb0.5_phases 1.527964\n",
      "features           ternary 2017-11-01_2026_helio_orb0.25_phases 1.527964\n",
      "features           ternary 2017-11-01_2026_helio_orb0.15_phases 1.527964\n",
      "features           ternary  2017-11-01_2026_helio_orb0.1_phases 1.527964\n",
      "features           ternary    2017-11-01_2026_geo_orb0.5_phases 1.800094\n",
      "features           ternary   2017-11-01_2026_geo_orb0.25_phases 1.800094\n",
      "features           ternary   2017-11-01_2026_geo_orb0.15_phases 1.800094\n",
      "features           ternary    2017-11-01_2026_geo_orb0.1_phases 1.800094\n",
      "features           ternary   2017-11-01_2026_both_orb1.0_phases 3.594608\n",
      "features           ternary    2017-11-01_2026_geo_orb1.0_phases 2.814006\n",
      "  labels           ternary              btc_2017-11-01_h1_gw201 0.043697\n",
      "  labels           ternary              btc_2017-11-01_h1_gw101 0.043695\n",
      "  labels           ternary              btc_2017-11-01_h1_gw101 0.043695\n",
      "  labels           ternary              btc_2017-11-01_h1_gw101 0.043695\n",
      "  labels           ternary              btc_2017-11-01_h1_gw201 0.043696\n",
      "  labels           ternary              btc_2017-11-01_h1_gw201 0.043696\n",
      "  market              data                       btc_2017-11-01 0.042440\n",
      "\n",
      "Total cache size: 52.88 MB\n"
     ]
    }
   ],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# SHOW CACHED FILES\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "print(\"\\nüìÅ Cached files after grid search:\")\n",
    "cache_df = list_cache()\n",
    "if not cache_df.empty:\n",
    "    print(cache_df[[\"category\", \"name\", \"params\", \"size_mb\"]].to_string(index=False))\n",
    "    print(f\"\\nTotal cache size: {cache_df['size_mb'].sum():.2f} MB\")\n",
    "else:\n",
    "    print(\"  No cached files found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Summary\n",
    "\n",
    "\n",
    "\n",
    " Grid search complete! The results show the best parameter combinations for:\n",
    "\n",
    "\n",
    "\n",
    " - **Labeling**: horizon, gauss_window (trend smoothing)\n",
    "\n",
    " - **Features**: coord_mode (geo/helio), orb_multiplier (aspect strictness)\n",
    "\n",
    " - **Model**: max_depth, learning_rate, weight_power (class balance)\n",
    "\n",
    "\n",
    "\n",
    " Use the best parameters in `main_pipeline_ternary.py` for production model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING BEST MODEL (exclude: Mars,Uranus)\n",
      "================================================================================\n",
      "Label params: {'horizon': 1, 'gauss_window': 201, 'gauss_std': 50.0}\n",
      "Exclude: ['Mars', 'Uranus']\n",
      "Model: {'max_depth': 9, 'learning_rate': 0.03, 'n_estimators': 500, 'weight_power': 2.0, 'sideways_penalty': 0.3}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'compute_features_fast' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Compute features with best exclusion\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m df_features_best = \u001b[43mcompute_features_fast\u001b[49m(\n\u001b[32m     39\u001b[39m     df_bodies,\n\u001b[32m     40\u001b[39m     bodies_by_date,\n\u001b[32m     41\u001b[39m     angles_cache,\n\u001b[32m     42\u001b[39m     settings,\n\u001b[32m     43\u001b[39m     orb_mult=orb_mult,\n\u001b[32m     44\u001b[39m     exclude_bodies=best_exclude_list,\n\u001b[32m     45\u001b[39m )\n\u001b[32m     47\u001b[39m df_labels_best = get_or_compute_labels(df_market, best_label_params)\n\u001b[32m     48\u001b[39m df_dataset_best = merge_features_with_labels(df_features_best, df_labels_best)\n",
      "\u001b[31mNameError\u001b[39m: name 'compute_features_fast' is not defined"
     ]
    }
   ],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# FINAL EVALUATION OF BEST EXCLUSION\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "if not df_results.empty:\n",
    "    best = df_results.iloc[0]\n",
    "    best_exclude_str = best[\"exclude_bodies\"]\n",
    "    best_exclude_list = best_exclude_str.split(\",\") if best_exclude_str != \"NONE\" else []\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"TRAINING BEST MODEL (exclude: {best_exclude_str})\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    best_label_params = {\n",
    "        \"horizon\": LABEL_GRID[\"horizon\"][0],\n",
    "        \"gauss_window\": LABEL_GRID[\"gauss_window\"][0],\n",
    "        \"gauss_std\": LABEL_GRID[\"gauss_std\"][0],\n",
    "    }\n",
    "    \n",
    "    best_model_params = {\n",
    "        \"max_depth\": MODEL_GRID[\"max_depth\"][0],\n",
    "        \"learning_rate\": MODEL_GRID[\"learning_rate\"][0],\n",
    "        \"n_estimators\": MODEL_GRID[\"n_estimators\"][0],\n",
    "        \"weight_power\": MODEL_GRID[\"weight_power\"][0],\n",
    "        \"sideways_penalty\": MODEL_GRID[\"sideways_penalty\"][0],\n",
    "    }\n",
    "    \n",
    "    print(f\"Exclude: {best_exclude_list}\")\n",
    "    \n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # Use filter_features_by_exclusion on full dataset\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    df_dataset_best = filter_features_by_exclusion(df_dataset_full, best_exclude_list)\n",
    "    \n",
    "    print(f\"Dataset: {len(df_dataset_best)} samples, {len(get_feature_columns(df_dataset_best))} features\")\n",
    "    \n",
    "    # Train and evaluate\n",
    "    final_metrics, final_model = train_and_evaluate_full(\n",
    "        df_dataset_best,\n",
    "        best_model_params.copy(),\n",
    "        device,\n",
    "        title=f\"BEST (exclude: {best_exclude_str})\",\n",
    "        df_market=df_market,\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úì Evaluation complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'up_down_min'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m/tmp/ipykernel_301920/4077312403.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# EVALUATE TOP 2 CONFIGURATIONS\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Sort by UP/DOWN balance\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m df_best = df_results.sort_values(\n\u001b[32m      7\u001b[39m     [\u001b[33m\"up_down_min\"\u001b[39m, \u001b[33m\"up_down_gap\"\u001b[39m],\n\u001b[32m      8\u001b[39m     ascending=[\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m]\n\u001b[32m      9\u001b[39m ).reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[32m/mnt/w/WSL/btc/lib/python3.12/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[39m\n\u001b[32m   8335\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   8336\u001b[39m                 \u001b[38;5;66;03m# error: Argument 1 to \"list\" has incompatible type\u001b[39;00m\n\u001b[32m   8337\u001b[39m                 \u001b[38;5;66;03m# \"Generator[ExtensionArray | ndarray[Any, Any], None, None]\";\u001b[39;00m\n\u001b[32m   8338\u001b[39m                 \u001b[38;5;66;03m# expected \"Iterable[Series]\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m8339\u001b[39m                 keys_data = list(keys)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   8340\u001b[39m \n\u001b[32m   8341\u001b[39m             indexer = lexsort_indexer(\n\u001b[32m   8342\u001b[39m                 keys_data, orders=ascending, na_position=na_position, key=key\n",
      "\u001b[32m/mnt/w/WSL/btc/lib/python3.12/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m8328\u001b[39m         >>> df.sort_values(by=\u001b[33m\"col4\"\u001b[39m, key=\u001b[38;5;28;01mlambda\u001b[39;00m col: col.str.lower())\n",
      "\u001b[32m/mnt/w/WSL/btc/lib/python3.12/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1772\u001b[39m             values = self.xs(key, axis=first_other_axes)._values\n\u001b[32m   1773\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1774\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1775\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1777\u001b[39m \n\u001b[32m   1778\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1779\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'up_down_min'"
     ]
    }
   ],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# EVALUATE TOP 2 CONFIGURATIONS\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# Sort by UP/DOWN balance\n",
    "df_best = df_results.sort_values(\n",
    "    [\"up_down_min\", \"up_down_gap\"], \n",
    "    ascending=[False, True]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(\"TOP 5 by UP/DOWN balance:\")\n",
    "print(df_best[[\"horizon\", \"orb_multiplier\", \"coord_mode\", \"sideways_penalty\", \n",
    "               \"max_depth\", \"learning_rate\", \"recall_down\", \"recall_up\", \"up_down_min\"]].head())\n",
    "\n",
    "# Evaluate TOP 2\n",
    "for rank in range(5):\n",
    "    best = df_best.iloc[rank]\n",
    "    \n",
    "    print(f\"\\n\\n{'='*80}\")\n",
    "    print(f\"EVALUATING RANK #{rank+1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    best_label_params = {\n",
    "        \"horizon\": int(best[\"horizon\"]),\n",
    "        \"gauss_window\": int(best[\"gauss_window\"]),\n",
    "        \"gauss_std\": float(best.get(\"gauss_std\", 50.0)),\n",
    "    }\n",
    "    \n",
    "    best_feature_params = {\n",
    "        \"coord_mode\": best[\"coord_mode\"],\n",
    "        \"orb_multiplier\": float(best[\"orb_multiplier\"]),\n",
    "        \"include_phases\": True,\n",
    "    }\n",
    "    \n",
    "    best_model_params = {\n",
    "        \"max_depth\": int(best[\"max_depth\"]),\n",
    "        \"learning_rate\": float(best[\"learning_rate\"]),\n",
    "        \"n_estimators\": 500,\n",
    "        \"weight_power\": float(best.get(\"weight_power\", 2.0)),\n",
    "        \"sideways_penalty\": float(best.get(\"sideways_penalty\", 1.0)),\n",
    "    }\n",
    "    \n",
    "    print(f\"Label: {best_label_params}\")\n",
    "    print(f\"Feature: {best_feature_params}\")\n",
    "    print(f\"Model: {best_model_params}\")\n",
    "    \n",
    "    # Rebuild dataset\n",
    "    df_labels_best = get_or_compute_labels(df_market, best_label_params)\n",
    "    df_features_best = get_or_compute_features(df_market, settings, best_feature_params)\n",
    "    df_dataset_best = merge_features_with_labels(df_features_best, df_labels_best)\n",
    "    \n",
    "    # Train and evaluate\n",
    "    metrics, model = train_and_evaluate_full(\n",
    "        df_dataset_best,\n",
    "        best_model_params.copy(),\n",
    "        device,\n",
    "        title=f\"RANK #{rank+1} (up_down_min={best['up_down_min']:.3f})\",\n",
    "        df_market=df_market,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "btc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
