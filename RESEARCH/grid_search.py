"""
Grid search module for RESEARCH pipeline.
Hyperparameter optimization for orb multiplier, gaussian params, etc.

Saves best results to disk for later use.
"""
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Any
from datetime import datetime

# DataFrame display settings - no line wrapping
pd.set_option('display.max_columns', None)
pd.set_option('display.expand_frame_repr', False)
pd.set_option('display.max_colwidth', None)
pd.set_option('display.width', None)

from .config import cfg
from .data_loader import load_market_data
from .labeling import create_balanced_labels, gaussian_smooth_centered
from .astro_engine import (
    init_ephemeris,
    calculate_bodies_for_dates,
    calculate_aspects_for_dates,
    calculate_transits_for_dates,
    get_natal_bodies,
    precompute_angles_for_dates,      # NEW: ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑƒĞ³Ğ»Ğ¾Ğ²
    calculate_aspects_from_cache,     # NEW: Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ¸Ğ· ĞºÑÑˆĞ°
)
from .features import build_full_features, merge_features_with_labels
from .model_training import (
    split_dataset,
    prepare_xy,
    train_xgb_model,
    tune_threshold,
    predict_with_threshold,
    calc_metrics,
    check_cuda_available,
)


class GridSearchConfig:
    """
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    ĞšĞĞĞ¤Ğ˜Ğ“Ğ£Ğ ĞĞ¦Ğ˜Ğ¯ GRID SEARCH (ĞŸĞ•Ğ Ğ•Ğ‘ĞĞ  ĞŸĞĞ ĞĞœĞ•Ğ¢Ğ ĞĞ’)
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Grid Search â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿ĞµÑ€ĞµĞ±Ğ¾Ñ€Ğ° Ğ²ÑĞµÑ… ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹.
    ĞĞ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, ĞµÑĞ»Ğ¸ Ñƒ Ğ½Ğ°Ñ 3 Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ orb Ğ¸ 3 Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ gauss_std, Ğ¼Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ğ¼ 3Ã—3=9 ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹.
    
    ĞŸĞĞ ĞĞœĞ•Ğ¢Ğ Ğ«:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    â€¢ orb_multipliers - Ğ¼Ğ½Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»Ğ¸ Ğ¾Ñ€Ğ±Ğ¸ÑĞ° Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ²
      [0.5] â€” ÑƒĞ·ĞºĞ¸Ğµ Ğ¾Ñ€Ğ±Ğ¸ÑÑ‹ (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹)
      [1.0] â€” ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ñ€Ğ±Ğ¸ÑÑ‹
      [1.5] â€” ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğµ Ğ¾Ñ€Ğ±Ğ¸ÑÑ‹ (Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ²)
      
    â€¢ gauss_windows - Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¾ĞºĞ½Ğ° Ğ´Ğ»Ñ ÑĞ³Ğ»Ğ°Ğ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ“Ğ°ÑƒÑÑĞ° (Ğ´Ğ½ĞµĞ¹)
      [51]  â€” ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¾Ğµ Ğ¾ĞºĞ½Ğ¾ (Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğº Ğ¼ĞµĞ»ĞºĞ¸Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼)
      [201] â€” Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğµ Ğ¾ĞºĞ½Ğ¾ (Ğ»Ğ¾Ğ²Ğ¸Ñ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ñ‚Ñ€ĞµĞ½Ğ´Ñ‹)
      
    â€¢ gauss_stds - ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ğµ Ğ“Ğ°ÑƒÑÑĞ°
      [30.0] â€” ÑƒĞ·ĞºĞ¸Ğ¹ ĞºĞ¾Ğ»Ğ¾ĞºĞ¾Ğ» (Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ°)
      [70.0] â€” ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ĞºĞ¾Ğ»Ğ¾ĞºĞ¾Ğ» (Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ°)
      
    â€¢ coord_modes - ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑ‡Ñ‘Ñ‚Ğ° Ğ¿Ğ»Ğ°Ğ½ĞµÑ‚
      ["geo"]   â€” Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ³ĞµĞ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ (Ğ—ĞµĞ¼Ğ»Ñ Ğ² Ñ†ĞµĞ½Ñ‚Ñ€Ğµ, ĞºĞ»Ğ°ÑÑĞ¸ĞºĞ°)
      ["helio"] â€” Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ³ĞµĞ»Ğ¸Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ (Ğ¡Ğ¾Ğ»Ğ½Ñ†Ğµ Ğ² Ñ†ĞµĞ½Ñ‚Ñ€Ğµ)
      ["both"]  â€” ĞĞ‘Ğ• ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ (ÑƒĞ´Ğ²Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²!)
      ["geo", "helio", "both"] â€” Ğ¿ĞµÑ€ĞµĞ±Ñ€Ğ°Ñ‚ÑŒ Ğ²ÑĞµ Ñ‚Ñ€Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°
      
    â€¢ max_exclude - ABLATION: Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ°ÑÑ‚Ñ€Ğ¾-Ñ‚ĞµĞ» (NEW!)
      0 â€” Ğ½Ğµ Ğ¸ÑĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒ Ñ‚ĞµĞ»Ğ° (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ orb/gauss/coord)
      1 â€” Ğ¿Ñ€Ğ¾Ğ±Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ 1 Ñ‚ĞµĞ»Ñƒ
      2 â€” Ğ¿Ñ€Ğ¾Ğ±Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒ Ğ´Ğ¾ 2 Ñ‚ĞµĞ»
      4 â€” Ğ¿Ñ€Ğ¾Ğ±Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒ Ğ´Ğ¾ 4 Ñ‚ĞµĞ» (Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ)
      
      Ğ­Ñ‚Ğ¾ ĞšĞĞœĞ‘Ğ˜ĞĞĞ¢ĞĞ ĞĞ«Ğ™ Ğ²Ğ·Ñ€Ñ‹Ğ²! Ğ¡ 11 Ñ‚ĞµĞ»Ğ°Ğ¼Ğ¸:
      - max_exclude=1: 11 Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ²
      - max_exclude=2: 66 Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ²
      - max_exclude=3: 231 Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚
      - max_exclude=4: 561 Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚
      
    â€¢ max_combos - Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ (Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ²)
    
    â€¢ model_params - Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ XGBoost Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    
    def __init__(
        self,
        orb_multipliers: List[float] = [0.8, 1.0, 1.2],
        gauss_windows: List[int] = [101, 151, 201],
        gauss_stds: List[float] = [30.0, 50.0, 70.0],
        coord_modes: List[str] = ["geo"],  # geo, helio, both
        max_exclude: int = 0,  # NEW: 0 = Ğ±ĞµĞ· ablation, 4 = Ğ¸ÑĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒ Ğ´Ğ¾ 4 Ñ‚ĞµĞ»
        max_combos: Optional[int] = None,
        model_params: Optional[Dict] = None,
    ):
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ²ÑĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        self.orb_multipliers = orb_multipliers
        self.gauss_windows = gauss_windows
        self.gauss_stds = gauss_stds
        self.coord_modes = coord_modes
        self.max_exclude = max_exclude  # NEW: Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ÑĞºĞ»ÑÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ñ‚ĞµĞ»
        self.max_combos = max_combos
        self.model_params = model_params or {
            "n_estimators": 500,    # ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´ĞµÑ€ĞµĞ²ÑŒĞµĞ² Ğ² Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»Ğµ
            "max_depth": 3,         # Ğ“Ğ»ÑƒĞ±Ğ¸Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ğ° (Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¾Ñ‚ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ)
            "learning_rate": 0.03,  # Ğ¡ĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (Ğ¼ĞµĞ½ÑŒÑˆĞµ = ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½ĞµĞµ)
            "subsample": 0.8,       # Ğ”Ğ¾Ğ»Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ğ°
            "colsample_bytree": 0.8,  # Ğ”Ğ¾Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ğ°
        }


def evaluate_combo(
    df_market: pd.DataFrame,
    df_bodies: pd.DataFrame,
    bodies_by_date: dict,
    settings: Any,
    orb_mult: float,
    gauss_window: int,
    gauss_std: float,
    exclude_bodies: Optional[List[str]] = None,
    angles_cache: Optional[dict] = None,  # NEW: Ğ¿Ñ€ĞµĞ´Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ğµ ÑƒĞ³Ğ»Ñ‹
    device: str = "cpu",
    model_params: Optional[Dict] = None,
) -> Dict:
    """
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    ĞĞ¦Ğ•ĞĞšĞ ĞĞ”ĞĞĞ™ ĞšĞĞœĞ‘Ğ˜ĞĞĞ¦Ğ˜Ğ˜ Ğ“Ğ˜ĞŸĞ•Ğ ĞŸĞĞ ĞĞœĞ•Ğ¢Ğ ĞĞ’
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Ğ­Ñ‚Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ:
    1. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºÑƒ (UP/DOWN) Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ gauss_window Ğ¸ gauss_std
    2. Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ orb_mult (Ğ¸Ğ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºÑÑˆ!)
    3. Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ Ñ„Ğ°Ğ·Ñ‹ Ğ›ÑƒĞ½Ñ‹ Ğ¸ ÑĞ»Ğ¾Ğ½Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ»Ğ°Ğ½ĞµÑ‚
    4. Ğ¡Ñ‚Ñ€Ğ¾Ğ¸Ñ‚ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ (Ğ¸ÑĞºĞ»ÑÑ‡Ğ°Ñ exclude_bodies!) Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹
    5. ĞĞ±ÑƒÑ‡Ğ°ĞµÑ‚ XGBoost Ğ¸ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸
    
    ĞĞŸĞ¢Ğ˜ĞœĞ˜Ğ—ĞĞ¦Ğ˜Ğ¯:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Ğ•ÑĞ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ½ angles_cache, Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ñ€Ğ°ÑÑÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ Ğ¸Ğ· ĞºÑÑˆĞ° (~3-5x Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ).
    ĞšÑÑˆ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ÑÑ Ğ¾Ğ´Ğ¸Ğ½ Ñ€Ğ°Ğ· Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ precompute_angles_for_dates().
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Returns:
        Dictionary with combo params and metrics
    """
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Ğ¨ĞĞ“ 1: Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºÑƒ (Ğ¼ĞµÑ‚ĞºĞ¸ UP/DOWN) Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ Ğ“Ğ°ÑƒÑÑĞ°
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    df_labels = create_balanced_labels(
        df_market,
        gauss_window=gauss_window,
        gauss_std=gauss_std,
    )
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Ğ¨ĞĞ“ 2: Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ (Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ĞºÑÑˆ ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ!)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if angles_cache is not None:
        # Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ: Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ğµ ÑƒĞ³Ğ»Ñ‹
        df_aspects = calculate_aspects_from_cache(
            angles_cache, settings, orb_mult=orb_mult, progress=False
        )
    else:
        # ĞœĞµĞ´Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ: Ğ¿ĞµÑ€ĞµÑÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ ÑƒĞ³Ğ»Ñ‹
        df_aspects = calculate_aspects_for_dates(
            bodies_by_date, settings, orb_mult=orb_mult, progress=False
        )
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Ğ¨ĞĞ“ 3: Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ Ñ„Ğ°Ğ·Ñ‹ Ğ›ÑƒĞ½Ñ‹ Ğ¸ ÑĞ»Ğ¾Ğ½Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ»Ğ°Ğ½ĞµÑ‚
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    from .astro_engine import calculate_phases_for_dates
    df_phases = calculate_phases_for_dates(bodies_by_date, progress=False)
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Ğ¨ĞĞ“ 4: Ğ¡Ñ‚Ñ€Ğ¾Ğ¸Ğ¼ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñƒ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² (Ğ¸ÑĞºĞ»ÑÑ‡Ğ°Ñ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµĞ»Ğ°!)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    df_features = build_full_features(
        df_bodies, df_aspects, 
        df_phases=df_phases,
        exclude_bodies=exclude_bodies  # NEW: Ğ¸ÑĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµĞ»Ğ°
    )
    
    # Merge with labels
    df_dataset = merge_features_with_labels(df_features, df_labels)
    
    if len(df_dataset) < 100:
        return {"error": "Too few samples"}
    
    # Split
    train_df, val_df, test_df = split_dataset(df_dataset)
    
    feature_cols = [c for c in df_dataset.columns if c not in ["date", "target"]]
    X_train, y_train = prepare_xy(train_df, feature_cols)
    X_val, y_val = prepare_xy(val_df, feature_cols)
    X_test, y_test = prepare_xy(test_df, feature_cols)
    
    # Train
    params = model_params or {}
    model = train_xgb_model(
        X_train, y_train, X_val, y_val,
        feature_cols, n_classes=2, device=device,
        **params
    )
    
    # Tune threshold Ğ¿Ğ¾ recall_min (Ğ° Ğ½Ğµ bal_acc)
    best_t, _ = tune_threshold(model, X_val, y_val, metric="recall_min")
    
    # Predict on test
    y_pred = predict_with_threshold(model, X_test, threshold=best_t)
    
    # Metrics
    metrics = calc_metrics(y_test, y_pred, [0, 1])
    
    # Per-class metrics
    from sklearn.metrics import classification_report
    report = classification_report(
        y_test, y_pred, labels=[0, 1],
        target_names=["DOWN", "UP"], output_dict=True, zero_division=0
    )
    
    f1_down = report["DOWN"]["f1-score"]
    f1_up = report["UP"]["f1-score"]
    recall_down = report["DOWN"]["recall"]
    recall_up = report["UP"]["recall"]
    
    return {
        "orb_mult": orb_mult,
        "gauss_window": gauss_window,
        "gauss_std": gauss_std,
        "exclude_bodies": exclude_bodies or [],  # NEW: ĞºĞ°ĞºĞ¸Ğµ Ñ‚ĞµĞ»Ğ° Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ñ‹
        "threshold": best_t,
        "recall_down": recall_down,
        "recall_up": recall_up,
        "recall_min": min(recall_down, recall_up),
        "recall_gap": abs(recall_down - recall_up),
        "f1_down": f1_down,
        "f1_up": f1_up,
        "f1_min": min(f1_down, f1_up),
        "f1_gap": abs(f1_down - f1_up),
        "f1_macro": metrics["f1_macro"],
        "bal_acc": metrics["bal_acc"],
        "mcc": metrics["mcc"],  # NEW: MCC Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°
        "summary": metrics["summary"],
    }


def run_grid_search(
    df_market: pd.DataFrame,
    config: Optional[GridSearchConfig] = None,
    save_results: bool = True,
    n_workers: int = 1,  # NEW: Number of parallel workers (1 = sequential)
) -> pd.DataFrame:
    """
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    GRID SEARCH - ĞŸĞ•Ğ Ğ•Ğ‘ĞĞ  Ğ’Ğ¡Ğ•Ğ¥ ĞšĞĞœĞ‘Ğ˜ĞĞĞ¦Ğ˜Ğ™ ĞŸĞĞ ĞĞœĞ•Ğ¢Ğ ĞĞ’
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Ğ­Ñ‚Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ²ÑĞµ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²:
    â€¢ orb_multipliers - Ğ¼Ğ½Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»Ğ¸ Ğ¾Ñ€Ğ±Ğ¸ÑĞ° (ÑˆĞ¸Ñ€Ğ¾Ñ‚Ğ° Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ²)
    â€¢ gauss_windows - Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¾ĞºĞ½Ğ° ÑĞ³Ğ»Ğ°Ğ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ
    â€¢ gauss_stds - ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ğµ Ğ“Ğ°ÑƒÑÑĞ°
    â€¢ coord_modes - ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚ (geo/helio/both)
    
    Ğ”Ğ»Ñ ĞšĞĞ–Ğ”ĞĞ™ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸:
    1. Ğ Ğ°ÑÑÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¿Ğ»Ğ°Ğ½ĞµÑ‚ Ğ¸ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹
    2. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºÑƒ (UP/DOWN) Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ ÑĞ³Ğ»Ğ°Ğ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ
    3. ĞĞ±ÑƒÑ‡Ğ°ĞµĞ¼ XGBoost Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
    4. ĞÑ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµĞ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
    5. Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚
    
    Ğ’ ĞºĞ¾Ğ½Ñ†Ğµ ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ (recall_min) Ğ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑÑƒ (recall_gap).
    
    MULTIPROCESSING:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    n_workers=1  â€” Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ (Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ)
    n_workers=4  â€” Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ° 4 ÑĞ´Ñ€Ğ°Ñ…
    n_workers=-1 â€” Ğ²ÑĞµ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ ÑĞ´Ñ€Ğ°
    
    âš ï¸ Ğ’ĞĞ˜ĞœĞĞĞ˜Ğ•: XGBoost ÑĞ°Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ!
    Ğ•ÑĞ»Ğ¸ Ñƒ Ğ²Ğ°Ñ GPU, Ğ»ÑƒÑ‡ÑˆĞµ n_workers=1 (GPU Ğ¸ Ñ‚Ğ°Ğº Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾).
    Ğ•ÑĞ»Ğ¸ CPU, Ğ¿Ğ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ n_workers=2-4.
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Args:
        df_market: Market data DataFrame
        config: GridSearchConfig (uses defaults if None)
        save_results: Save results to reports directory
        n_workers: Number of parallel workers (1=sequential, -1=all cores)
    
    Returns:
        DataFrame with all results sorted by balance
    """
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Ğ¨ĞĞ“ 1: Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    config = config or GridSearchConfig()
    
    print("=" * 80)
    print("ğŸ” GRID SEARCH: ĞŸĞ•Ğ Ğ•Ğ‘ĞĞ  ORB + GAUSSIAN + COORD_MODE")
    print("=" * 80)
    print(f"""
    ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ°:
    â€¢ ORB Ğ¼Ğ½Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»Ğ¸:    {config.orb_multipliers}
    â€¢ GAUSS Ğ¾ĞºĞ½Ğ°:       {config.gauss_windows}
    â€¢ GAUSS std:        {config.gauss_stds}
    â€¢ COORD Ñ€ĞµĞ¶Ğ¸Ğ¼Ñ‹:     {config.coord_modes}
    """)
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Ğ¨ĞĞ“ 2: ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ CUDA (GPU) Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ XGBoost
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    _, device = check_cuda_available()
    print(f"ğŸ–¥ï¸ Ğ£ÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾: {device}")
    
    # Run timestamp for checkpoints
    run_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Ğ¨ĞĞ“ 3: Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Swiss Ephemeris
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    settings = init_ephemeris()
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Ğ¨ĞĞ“ 4: Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ Ğ’Ğ¡Ğ• ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² (Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ablation!)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    from itertools import combinations
    
    # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ²ÑĞµÑ… Ñ‚ĞµĞ» Ğ´Ğ»Ñ ablation
    all_bodies = get_all_body_names(settings)
    
    # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¹ (ĞµÑĞ»Ğ¸ max_exclude > 0)
    exclusion_combos = [[]]  # ĞĞ°Ñ‡Ğ¸Ğ½Ğ°ĞµĞ¼ Ñ Ğ¿ÑƒÑÑ‚Ğ¾Ğ³Ğ¾ ÑĞ¿Ğ¸ÑĞºĞ° (baseline)
    
    if config.max_exclude > 0:
        for n_exclude in range(1, config.max_exclude + 1):
            for combo in combinations(all_bodies, n_exclude):
                exclusion_combos.append(list(combo))
    
    # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ Ğ’Ğ¡Ğ• ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸: (coord_mode, orb, gw, gs, exclude_bodies)
    combos = []
    for coord_mode in config.coord_modes:
        for orb in config.orb_multipliers:
            for gw in config.gauss_windows:
                for gs in config.gauss_stds:
                    for excl in exclusion_combos:
                        combos.append((coord_mode, orb, gw, gs, excl))
    
    # ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ ĞµÑĞ»Ğ¸ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¾
    if config.max_combos and len(combos) > config.max_combos:
        combos = combos[:config.max_combos]
    
    print(f"\nğŸ“Š Ğ’ÑĞµĞ³Ğ¾ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ±Ğ¾Ñ€Ğ°: {len(combos)}")
    if config.max_exclude > 0:
        print(f"   (Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ {len(exclusion_combos)} Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² ablation: Ğ´Ğ¾ {config.max_exclude} Ğ¸ÑĞºĞ»ÑÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ñ‚ĞµĞ»)")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Ğ¨ĞĞ“ 5: ĞŸÑ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°ÑÑÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¿Ğ»Ğ°Ğ½ĞµÑ‚ Ğ´Ğ»Ñ Ğ’Ğ¡Ğ•Ğ¥ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ¾Ğ²
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Ğ­Ñ‚Ğ¾ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‚ Ğ²Ñ€ĞµĞ¼Ñ - Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¿Ğ»Ğ°Ğ½ĞµÑ‚ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… orb/gauss ĞºĞ¾Ğ¼Ğ±Ğ¾
    print("\nğŸ“ ĞŸÑ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°ÑÑ‡Ñ‘Ñ‚ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ Ğ¿Ğ»Ğ°Ğ½ĞµÑ‚...")
    
    from .astro_engine import calculate_bodies_for_dates_multi
    
    cached_bodies = {}  # ĞšÑÑˆ: coord_mode -> (df_bodies, geo_by_date, helio_by_date)
    
    for coord_mode in config.coord_modes:
        if coord_mode not in cached_bodies:
            print(f"\n  â†’ Ğ ĞµĞ¶Ğ¸Ğ¼ {coord_mode.upper()}:")
            df_bodies, geo_by_date, helio_by_date = calculate_bodies_for_dates_multi(
                df_market["date"], settings, coord_mode=coord_mode, progress=True
            )
            cached_bodies[coord_mode] = (df_bodies, geo_by_date, helio_by_date)
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Ğ¨ĞĞ“ 5.5: ĞŸÑ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°ÑÑÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ£Ğ“Ğ›Ğ« Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ»Ğ°Ğ½ĞµÑ‚Ğ°Ğ¼Ğ¸ (ĞĞŸĞ¢Ğ˜ĞœĞ˜Ğ—ĞĞ¦Ğ˜Ğ¯!)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Ğ£Ğ³Ğ»Ñ‹ Ğ½Ğµ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‚ Ğ¾Ñ‚ orb_mult â€” ÑÑ‡Ğ¸Ñ‚Ğ°ĞµĞ¼ Ğ¾Ğ´Ğ¸Ğ½ Ñ€Ğ°Ğ·, Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµĞ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾
    print("\nğŸ“ ĞŸÑ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°ÑÑ‡Ñ‘Ñ‚ ÑƒĞ³Ğ»Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ»Ğ°Ğ½ĞµÑ‚Ğ°Ğ¼Ğ¸...")
    
    cached_angles = {}  # ĞšÑÑˆ: coord_mode -> angles_cache
    
    for coord_mode in config.coord_modes:
        if coord_mode not in cached_angles:
            _, geo_by_date, helio_by_date = cached_bodies[coord_mode]
            bodies_by_date = geo_by_date if geo_by_date else helio_by_date
            
            print(f"  â†’ Ğ£Ğ³Ğ»Ñ‹ Ğ´Ğ»Ñ {coord_mode.upper()}...")
            cached_angles[coord_mode] = precompute_angles_for_dates(
                bodies_by_date, progress=True
            )
    
    print("âœ… Ğ£Ğ³Ğ»Ñ‹ Ğ·Ğ°ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹! Ğ¢ĞµĞ¿ĞµÑ€ÑŒ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ ÑÑ‡Ğ¸Ñ‚Ğ°ÑÑ‚ÑÑ ~3-5x Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ.")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Ğ¨ĞĞ“ 6: Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ±Ğ¾Ñ€ Ğ²ÑĞµÑ… ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "=" * 80)
    print("ğŸš€ ĞĞĞ§Ğ˜ĞĞĞ•Ğœ ĞŸĞ•Ğ Ğ•Ğ‘ĞĞ  ĞšĞĞœĞ‘Ğ˜ĞĞĞ¦Ğ˜Ğ™")
    print("=" * 80)
    
    results = []
    
    # Track best result so far
    best_so_far = {
        "score": -1.0,
        "gap": 1.0,
        "combo": None,
        "metrics": {}
    }

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # WORKER FUNCTION Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _evaluate_one_combo(combo_data):
        """Evaluate single combo - used for parallel execution."""
        idx, coord_mode, orb, gw, gs, excl = combo_data
        
        excl_str = f"-[{len(excl)}]" if excl else ""
        if excl and len(excl) <= 2:
            excl_str = f"-[{','.join(excl)}]"
        params_str = f"[{idx+1}/{len(combos)}] {coord_mode} | O={orb} W={gw} S={gs} {excl_str}"
        
        try:
            df_bodies, geo_by_date, helio_by_date = cached_bodies[coord_mode]
            bodies_by_date = geo_by_date if geo_by_date else helio_by_date
            
            res = evaluate_combo(
                df_market, df_bodies, bodies_by_date, settings,
                orb, gw, gs,
                exclude_bodies=excl if excl else None,
                angles_cache=cached_angles.get(coord_mode),
                device=device,
                model_params=config.model_params,
            )
            res["coord_mode"] = coord_mode
            return idx, params_str, excl_str, res, None
        except Exception as e:
            return idx, params_str, excl_str, {
                "coord_mode": coord_mode, "orb_mult": orb, "gauss_window": gw, 
                "gauss_std": gs, "exclude_bodies": excl, "error": str(e)
            }, str(e)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # PARALLEL or SEQUENTIAL execution
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    combo_data_list = [(i, *combo) for i, combo in enumerate(combos)]
    
    if n_workers > 1 or n_workers == -1:
        # PARALLEL EXECUTION with ThreadPoolExecutor
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import os
        
        actual_workers = n_workers if n_workers > 0 else os.cpu_count() or 4
        print(f"\nâš¡ ĞŸĞĞ ĞĞ›Ğ›Ğ•Ğ›Ğ¬ĞĞĞ• Ğ’Ğ«ĞŸĞĞ›ĞĞ•ĞĞ˜Ğ•: {actual_workers} Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²")
        
        with ThreadPoolExecutor(max_workers=actual_workers) as executor:
            futures = {executor.submit(_evaluate_one_combo, cd): cd for cd in combo_data_list}
            
            for future in as_completed(futures):
                idx, params_str, excl_str, res, error = future.result()
                results.append(res)
                
                if error is None and "error" not in res:
                    r_min = res['recall_min']
                    r_gap = res['recall_gap']
                    mcc = res.get('mcc', 0)
                    coord_mode = res.get('coord_mode', '?')
                    orb = res.get('orb_mult', 0)
                    gw = res.get('gauss_window', 0)
                    gs = res.get('gauss_std', 0)
                    
                    is_best = r_min > best_so_far["score"] or (
                        r_min == best_so_far["score"] and r_gap < best_so_far["gap"]
                    )
                    if is_best:
                        best_so_far["score"] = r_min
                        best_so_far["gap"] = r_gap
                        best_so_far["combo"] = f"{coord_mode} O={orb} W={gw} S={gs} {excl_str}"
                        best_so_far["metrics"] = f"R_MIN={r_min:.3f} GAP={r_gap:.3f} MCC={mcc:.3f}"
                    
                    msg = f"{params_str:<60} â†’ R_UP={res['recall_up']:.3f} R_DOWN={res['recall_down']:.3f} MCC={mcc:.3f}"
                    print(msg)
                    print(f"   ğŸ† BEST: {best_so_far['metrics']} ({best_so_far['combo']})")
                    print()
                else:
                    print(f"{params_str:<60} â†’ ERROR: {error or res.get('error')}")
                
                # Checkpoint every 100
                if len(results) % 100 == 0:
                    try:
                        ckpt_dir = cfg.reports_dir / "checkpoints"
                        ckpt_dir.mkdir(exist_ok=True, parents=True)
                        ckpt_path = ckpt_dir / f"grid_search_{run_timestamp}_checkpoint.parquet"
                        pd.DataFrame(results).to_parquet(ckpt_path, index=False)
                        print(f"   ğŸ’¾ Checkpoint: {len(results)} combos saved")
                    except Exception as e:
                        print(f"   âš ï¸ Checkpoint error: {e}")
    else:
        # SEQUENTIAL EXECUTION (original behavior)
        for i, (coord_mode, orb, gw, gs, excl) in enumerate(combos):
            excl_str = f"-[{len(excl)}]" if excl else ""
            if excl and len(excl) <= 2:
                excl_str = f"-[{','.join(excl)}]"
                
            params_str = f"[{i+1}/{len(combos)}] {coord_mode} | O={orb} W={gw} S={gs} {excl_str}"
            
            try:
                df_bodies, geo_by_date, helio_by_date = cached_bodies[coord_mode]
                bodies_by_date = geo_by_date if geo_by_date else helio_by_date
                
                res = evaluate_combo(
                    df_market, df_bodies, bodies_by_date, settings,
                    orb, gw, gs,
                    exclude_bodies=excl if excl else None,
                    angles_cache=cached_angles.get(coord_mode),
                    device=device,
                    model_params=config.model_params,
                )
                res["coord_mode"] = coord_mode
                results.append(res)
                
                if "error" not in res:
                    r_min = res['recall_min']
                    r_gap = res['recall_gap']
                    mcc = res.get('mcc', 0)
                    
                    is_best = False
                    if r_min > best_so_far["score"]:
                        is_best = True
                    elif r_min == best_so_far["score"] and r_gap < best_so_far["gap"]:
                        is_best = True
                        
                    if is_best:
                        best_so_far["score"] = r_min
                        best_so_far["gap"] = r_gap
                        best_so_far["combo"] = f"{coord_mode} O={orb} W={gw} S={gs} {excl_str}"
                        best_so_far["metrics"] = f"R_MIN={r_min:.3f} GAP={r_gap:.3f} MCC={mcc:.3f}"
                    
                    msg = f"{params_str:<60} â†’ R_UP={res['recall_up']:.3f} R_DOWN={res['recall_down']:.3f} MCC={mcc:.3f}"
                    print(msg)
                    print(f"   ğŸ† BEST: {best_so_far['metrics']} ({best_so_far['combo']})")
                    print()
                else:
                    print(f"{params_str:<60} â†’ ERROR: {res.get('error')}")

                # CHECKPOINT: Save every 100 iterations
                if (i + 1) % 100 == 0:
                    try:
                        ckpt_dir = cfg.reports_dir / "checkpoints"
                        ckpt_dir.mkdir(exist_ok=True, parents=True)
                        ckpt_path = ckpt_dir / f"grid_search_{run_timestamp}_checkpoint.parquet"
                        pd.DataFrame(results).to_parquet(ckpt_path, index=False)
                        print(f"   ğŸ’¾ Checkpoint saved: {ckpt_path.name}")
                    except Exception as e:
                        print(f"   âš ï¸ Checkpoint error: {e}")

            except Exception as e:
                print(f"{params_str:<60} â†’ CRASH: {e}")
                results.append({
                    "coord_mode": coord_mode, "orb_mult": orb, "gauss_window": gw, "gauss_std": gs,
                    "exclude_bodies": excl, "error": str(e)
                })
    
    # Convert to DataFrame
    results_df = pd.DataFrame(results)
    # Sort by QUALITY first, then BALANCE: maximize recall_min, then minimize recall_gap
    if "recall_down" in results_df.columns and "recall_up" in results_df.columns:
        results_df = results_df.sort_values(
            ["recall_min", "recall_gap", "bal_acc"],
            ascending=[False, True, False]
        ).reset_index(drop=True)
    
    # Save results
    if save_results:
        save_grid_search_results(results_df)
    
    # Print best
    print("\n" + "=" * 60)
    print("TOP 5 COMBOS BY BALANCE:")
    print(results_df.head(5).to_string(index=False))
    
    return results_df


def save_grid_search_results(results_df: pd.DataFrame) -> Path:
    """Save grid search results to reports directory."""
    reports_dir = cfg.reports_dir
    reports_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    path = reports_dir / f"grid_search_{timestamp}.csv"
    
    results_df.to_csv(path, index=False)
    print(f"\nResults saved to: {path}")
    
    return path


def get_best_params(results_df: pd.DataFrame) -> Dict:
    """Extract best parameters from grid search results."""
    if results_df.empty:
        return {}
    
    best = results_df.iloc[0].to_dict()
    return {
        "orb_mult": float(best.get("orb_mult", 1.0)),
        "gauss_window": int(best.get("gauss_window", 201)),
        "gauss_std": float(best.get("gauss_std", 50.0)),
        "threshold": float(best.get("threshold", 0.5)),
    }


def save_best_params(params: Dict, name: str = "best") -> Path:
    """Save best parameters to YAML file."""
    import yaml
    
    reports_dir = cfg.reports_dir
    path = reports_dir / f"{name}_params.yaml"
    
    with open(path, "w") as f:
        yaml.dump(params, f) 
    
    print(f"Best params saved to: {path}")
    return path


# =============================================================================
# BODY ABLATION SEARCH
# =============================================================================

def get_all_body_names(settings: Any) -> List[str]:
    """Get list of all body names from settings."""
    return [b.name for b in settings.bodies]


def evaluate_body_exclusion(
    df_market: pd.DataFrame,
    df_bodies: pd.DataFrame,
    df_aspects: pd.DataFrame,
    df_labels: pd.DataFrame,
    exclude_bodies: List[str],
    device: str = "cpu",
    model_params: Optional[Dict] = None,
) -> Dict:
    """
    Evaluate model performance when excluding specific bodies.
    
    Args:
        df_market: Market data
        df_bodies: Pre-calculated body positions
        df_aspects: Pre-calculated aspects
        df_labels: Pre-created labels
        exclude_bodies: List of body names to exclude
        device: 'cpu' or 'cuda'
        model_params: XGBoost parameters
    
    Returns:
        Dictionary with exclusion params and metrics
    """
    try:
        # Build features with body exclusion
        df_features = build_full_features(
            df_bodies,
            df_aspects,
            df_transits=None,
            include_pair_aspects=True,
            include_transit_aspects=False,
            exclude_bodies=exclude_bodies,
        )
        
        # Merge with labels
        df_dataset = merge_features_with_labels(df_features, df_labels)
        
        if len(df_dataset) < 100:
            return {"exclude_bodies": exclude_bodies, "error": "Too few samples"}
        
        # Split
        train_df, val_df, test_df = split_dataset(df_dataset)
        
        feature_cols = [c for c in df_dataset.columns if c not in ["date", "target"]]
        X_train, y_train = prepare_xy(train_df, feature_cols)
        X_val, y_val = prepare_xy(val_df, feature_cols)
        X_test, y_test = prepare_xy(test_df, feature_cols)
        
        # Train
        params = model_params or {
            "n_estimators": 300,  # Faster for grid search
            "max_depth": 3,
            "learning_rate": 0.05,
            "verbosity": 0,
        }
        
        model = train_xgb_model(
            X_train, y_train, X_val, y_val,
            feature_cols, n_classes=2, device=device,
            **params
        )
        
        # Tune threshold
        best_t, _ = tune_threshold(model, X_val, y_val, metric="bal_acc")
        
        # Predict on test
        y_pred = predict_with_threshold(model, X_test, threshold=best_t)
        
        # Metrics
        metrics = calc_metrics(y_test, y_pred, [0, 1])
        
        from sklearn.metrics import classification_report
        report = classification_report(
            y_test, y_pred, labels=[0, 1],
            target_names=["DOWN", "UP"], output_dict=True, zero_division=0
        )
        
        f1_down = report["DOWN"]["f1-score"]
        f1_up = report["UP"]["f1-score"]
        recall_down = report["DOWN"]["recall"]
        recall_up = report["UP"]["recall"]
        
        return {
            "exclude_bodies": exclude_bodies,
            "n_excluded": len(exclude_bodies),
            "n_features": len(feature_cols),
            "n_samples": len(df_dataset),
            "threshold": best_t,
            "recall_down": recall_down,
            "recall_up": recall_up,
            "recall_min": min(recall_down, recall_up),
            "recall_gap": abs(recall_down - recall_up),
            "f1_down": f1_down,
            "f1_up": f1_up,
            "f1_min": min(f1_down, f1_up),
            "f1_gap": abs(f1_down - f1_up),
            "f1_macro": metrics["f1_macro"],
            "mcc": metrics["mcc"],
            "bal_acc": metrics["bal_acc"],
        }
    except Exception as e:
        return {"exclude_bodies": exclude_bodies, "error": str(e)}


def run_body_ablation_search(
    df_market: pd.DataFrame,
    orb_mult: float = 1.0,
    gauss_window: int = 201,
    gauss_std: float = 50.0,
    max_exclude: int = 3,
    n_workers: Optional[int] = None,
    save_results: bool = True,
) -> pd.DataFrame:
    """
    Run ablation study on astro bodies to find most influential ones.
    
    Tries all combinations of excluding 1, 2, ... max_exclude bodies
    and measures impact on model performance.
    
    Args:
        df_market: Market data DataFrame
        orb_mult: Orb multiplier to use
        gauss_window: Gaussian window for labeling
        gauss_std: Gaussian std for labeling  
        max_exclude: Maximum number of bodies to exclude (1, 2, 3...)
        n_workers: Number of parallel workers (default: CPU count - 1)
        save_results: Save results to reports directory
    
    Returns:
        DataFrame with all results sorted by balanced accuracy
    """
    from itertools import combinations
    from concurrent.futures import ProcessPoolExecutor, as_completed
    import multiprocessing as mp
    
    print("=" * 60)
    print("BODY ABLATION SEARCH")
    print("=" * 60)
    
    # Check CUDA
    _, device = check_cuda_available()
    print(f"Device: {device}")
    
    # Workers
    if n_workers is None:
        n_workers = max(1, mp.cpu_count() - 1)
    print(f"Workers: {n_workers}")
    
    # Initialize astro
    settings = init_ephemeris()
    all_bodies = get_all_body_names(settings)
    print(f"Bodies: {all_bodies}")
    
    # Calculate bodies once
    print("\nCalculating body positions...")
    df_bodies, bodies_by_date = calculate_bodies_for_dates(
        df_market["date"], settings, progress=True
    )
    print(f"  df_bodies.shape: {df_bodies.shape}")
    
    # Calculate aspects once with specified orb
    print(f"\nCalculating aspects (orb_mult={orb_mult})...")
    df_aspects = calculate_aspects_for_dates(
        bodies_by_date, settings, orb_mult=orb_mult, progress=True
    )
    print(f"  df_aspects.shape: {df_aspects.shape}")
    
    # Create labels once
    print(f"\nCreating labels (window={gauss_window}, std={gauss_std})...")
    df_labels = create_balanced_labels(
        df_market,
        gauss_window=gauss_window,
        gauss_std=gauss_std,
    )
    print(f"  df_labels.shape: {df_labels.shape}")
    
    # Generate all exclusion combinations
    exclusion_combos = []
    
    # Baseline: no exclusion
    exclusion_combos.append([])
    
    # 1, 2, ... max_exclude bodies
    for n_exclude in range(1, max_exclude + 1):
        for combo in combinations(all_bodies, n_exclude):
            exclusion_combos.append(list(combo))
    
    print(f"\nTotal combinations to test: {len(exclusion_combos)}")
    print(f"  - Baseline: 1")
    for n in range(1, max_exclude + 1):
        count = len(list(combinations(all_bodies, n)))
        print(f"  - Exclude {n}: {count}")
    
    # Run search (sequential for now - multiprocessing has pickle issues with XGBoost)
    print("\nRunning ablation search...")
    results = []
    
    for i, exclude in enumerate(exclusion_combos):
        exclude_str = ", ".join(exclude) if exclude else "BASELINE"
        print(f"[{i+1}/{len(exclusion_combos)}] Exclude: {exclude_str}", end=" ")
        
        res = evaluate_body_exclusion(
            df_market, df_bodies, df_aspects, df_labels,
            exclude_bodies=exclude,
            device=device,
        )
        results.append(res)
        
        if "error" not in res:
            print(f"â†’ RECALL_MIN={res['recall_min']:.3f} | RECALL_GAP={res['recall_gap']:.3f} | bal_acc={res['bal_acc']:.3f}")
        else:
            print(f"â†’ ERROR: {res['error']}")
    
    # Convert to DataFrame
    results_df = pd.DataFrame(results)
    
    # Add readable exclude column
    results_df["exclude_str"] = results_df["exclude_bodies"].apply(
        lambda x: ", ".join(x) if x else "BASELINE"
    )
    
    # Sort by QUALITY first, then BALANCE: maximize recall_min, then minimize recall_gap
    if "recall_gap" in results_df.columns:
        results_df = results_df.sort_values(
            ["recall_min", "recall_gap", "bal_acc"],
            ascending=[False, True, False]
        ).reset_index(drop=True)
    
    # Save results
    if save_results:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        path = cfg.reports_dir / f"body_ablation_{timestamp}.csv"
        results_df.to_csv(path, index=False)
        print(f"\nResults saved to: {path}")
    
    # Print summary
    print("\n" + "=" * 60)
    print("TOP 10 COMBINATIONS BY BALANCED ACCURACY:")
    print("=" * 60)
    top_cols = ["exclude_str", "n_excluded", "n_features", "bal_acc", "f1_min", "f1_down", "f1_up"]
    available_cols = [c for c in top_cols if c in results_df.columns]
    print(results_df[available_cols].head(10).to_string(index=False))
    
    # Identify most influential bodies
    print("\n" + "=" * 60)
    print("BODY INFLUENCE ANALYSIS:")
    print("=" * 60)
    
    baseline = results_df[results_df["exclude_str"] == "BASELINE"]
    if not baseline.empty:
        baseline_acc = baseline.iloc[0]["bal_acc"]
        print(f"Baseline accuracy: {baseline_acc:.3f}")
        
        # For single-body exclusions, show impact
        single_exclusions = results_df[results_df["n_excluded"] == 1].copy()
        if not single_exclusions.empty:
            single_exclusions["impact"] = baseline_acc - single_exclusions["bal_acc"]
            single_exclusions = single_exclusions.sort_values("impact", ascending=False)
            
            print("\nImpact of removing each body (positive = body helps model):")
            for _, row in single_exclusions.iterrows():
                impact = row["impact"]
                sign = "+" if impact > 0 else ""
                body = row["exclude_str"]
                print(f"  {body:12s}: {sign}{impact:.3f}")
    
    return results_df


def run_comprehensive_search(
    df_market: pd.DataFrame,
    orb_multipliers: List[float] = [0.5, 0.8, 1.0, 1.2],
    gauss_windows: List[int] = [101, 151, 201],
    gauss_stds: List[float] = [30.0, 50.0, 70.0],
    max_exclude_bodies: int = 2,
    save_results: bool = True,
) -> Dict[str, pd.DataFrame]:
    """
    Run comprehensive grid search: first gauss+orb, then body ablation with best params.
    
    Args:
        df_market: Market data
        orb_multipliers: List of orb values to try
        gauss_windows: List of gaussian window sizes
        gauss_stds: List of gaussian stds
        max_exclude_bodies: Max bodies to exclude in ablation
        save_results: Save results to reports
    
    Returns:
        Dictionary with 'gauss_orb' and 'ablation' DataFrames
    """
    print("=" * 70)
    print("COMPREHENSIVE GRID SEARCH")
    print("=" * 70)
    
    # Phase 1: Find best gauss + orb
    print("\n" + "=" * 70)
    print("PHASE 1: GAUSSIAN + ORB PARAMETER SEARCH")
    print("=" * 70)
    
    config = GridSearchConfig(
        orb_multipliers=orb_multipliers,
        gauss_windows=gauss_windows,
        gauss_stds=gauss_stds,
    )
    
    gauss_orb_results = run_grid_search(df_market, config, save_results=save_results)
    best_params = get_best_params(gauss_orb_results)
    
    print(f"\nBest params from Phase 1:")
    print(f"  orb_mult: {best_params['orb_mult']}")
    print(f"  gauss_window: {best_params['gauss_window']}")
    print(f"  gauss_std: {best_params['gauss_std']}")
    
    # Phase 2: Body ablation with best params
    print("\n" + "=" * 70)
    print("PHASE 2: BODY ABLATION SEARCH")
    print("=" * 70)
    
    ablation_results = run_body_ablation_search(
        df_market,
        orb_mult=best_params["orb_mult"],
        gauss_window=best_params["gauss_window"],
        gauss_std=best_params["gauss_std"],
        max_exclude=max_exclude_bodies,
        save_results=save_results,
    )
    
    return {
        "gauss_orb": gauss_orb_results,
        "ablation": ablation_results,
        "best_params": best_params,
    }


def run_full_grid_search(
    df_market: pd.DataFrame,
    orb_multipliers: List[float] = [0.5, 0.8, 1.0, 1.2],
    gauss_windows: List[int] = [101, 151, 201],
    gauss_stds: List[float] = [30.0, 50.0, 70.0],
    test_mode: bool = True,
    test_limit: int = 20,
    save_results: bool = True,
) -> pd.DataFrame:
    """
    Run FULL grid search over ALL combinations: gauss Ã— orb Ã— body exclusions.
    
    WARNING: Full search is VERY slow! Use test_mode=True first.
    
    Full search space:
    - gauss: 3 Ã— orb: 4 = 12 param combos
    - bodies: 2^13 = 8192 exclusion combos
    - Total: ~98,000 combinations
    
    Args:
        df_market: Market data
        orb_multipliers: List of orb values
        gauss_windows: List of gaussian windows
        gauss_stds: List of gaussian stds
        test_mode: If True, run only test_limit combinations
        test_limit: Number of combos to test in test_mode
        save_results: Save to reports directory
    
    Returns:
        DataFrame with all results
    """
    from itertools import combinations, product
    import time
    
    print("=" * 70)
    print("FULL GRID SEARCH: GAUSS Ã— ORB Ã— BODY EXCLUSIONS")
    print("=" * 70)
    
    # Check CUDA
    _, device = check_cuda_available()
    print(f"Device: {device}")
    
    # Initialize astro
    settings = init_ephemeris()
    all_bodies = get_all_body_names(settings)
    n_bodies = len(all_bodies)
    print(f"Bodies ({n_bodies}): {all_bodies}")
    
    # Calculate bodies once
    print("\nCalculating body positions (one-time)...")
    df_bodies, bodies_by_date = calculate_bodies_for_dates(
        df_market["date"], settings, progress=True
    )
    print(f"  df_bodies.shape: {df_bodies.shape}")
    
    # Generate ALL body exclusion combinations (2^n)
    all_exclusions = [[]]  # Start with no exclusion
    for r in range(1, n_bodies + 1):
        for combo in combinations(all_bodies, r):
            all_exclusions.append(list(combo))
    
    print(f"\nTotal body exclusion combinations: {len(all_exclusions)}")
    
    # Generate all param combinations
    param_combos = list(product(orb_multipliers, gauss_windows, gauss_stds))
    print(f"Param combinations (orb Ã— gauss): {len(param_combos)}")
    
    # Generate full grid
    full_grid = []
    for orb, gw, gs in param_combos:
        for exclude in all_exclusions:
            full_grid.append({
                "orb_mult": orb,
                "gauss_window": gw,
                "gauss_std": gs,
                "exclude_bodies": exclude,
            })
    
    total_combos = len(full_grid)
    print(f"\nTOTAL COMBINATIONS: {total_combos:,}")
    
    if test_mode:
        print(f"\n*** TEST MODE: Running only {test_limit} combinations ***")
        # Sample diverse combinations
        import random
        random.seed(42)
        full_grid = random.sample(full_grid, min(test_limit, total_combos))
    
    print(f"\nRunning {len(full_grid)} combinations...")
    
    # Pre-compute aspects for each orb (cache)
    aspects_cache = {}
    unique_orbs = list(set(c["orb_mult"] for c in full_grid))
    print(f"\nPre-computing aspects for {len(unique_orbs)} orb values...")
    for orb in unique_orbs:
        df_aspects = calculate_aspects_for_dates(
            bodies_by_date, settings, orb_mult=orb, progress=False
        )
        aspects_cache[orb] = df_aspects
        print(f"  orb={orb}: {len(df_aspects)} aspects")
    
    # Pre-compute labels for each gauss config (cache)
    labels_cache = {}
    unique_gauss = list(set((c["gauss_window"], c["gauss_std"]) for c in full_grid))
    print(f"\nPre-computing labels for {len(unique_gauss)} gauss configs...")
    for gw, gs in unique_gauss:
        df_labels = create_balanced_labels(df_market, gauss_window=gw, gauss_std=gs)
        labels_cache[(gw, gs)] = df_labels
        print(f"  window={gw}, std={gs}: {len(df_labels)} labels")
    
    # Run search
    print("\n" + "=" * 70)
    print("RUNNING GRID SEARCH...")
    print("=" * 70)
    
    results = []
    start_time = time.time()
    
    for i, combo in enumerate(full_grid):
        orb = combo["orb_mult"]
        gw = combo["gauss_window"]
        gs = combo["gauss_std"]
        exclude = combo["exclude_bodies"]
        
        exclude_str = ", ".join(exclude) if exclude else "NONE"
        
        # Get cached data
        df_aspects = aspects_cache[orb]
        df_labels = labels_cache[(gw, gs)]
        
        # Progress
        elapsed = time.time() - start_time
        if i > 0:
            avg_per_combo = elapsed / i
            remaining = avg_per_combo * (len(full_grid) - i)
            eta_min = remaining / 60
        else:
            eta_min = 0
        
        print(f"[{i+1}/{len(full_grid)}] orb={orb}, gw={gw}, gs={gs}, exclude=[{exclude_str[:30]}...]", end=" ")
        
        # Evaluate
        res = evaluate_body_exclusion(
            df_market, df_bodies, df_aspects, df_labels,
            exclude_bodies=exclude,
            device=device,
        )
        
        # Add params to result
        res["orb_mult"] = orb
        res["gauss_window"] = gw
        res["gauss_std"] = gs
        res["exclude_str"] = exclude_str
        res["n_excluded"] = len(exclude)
        
        results.append(res)
        
        if "error" not in res:
            print(f"â†’ RECALL_MIN={res['recall_min']:.3f} | RECALL_GAP={res['recall_gap']:.3f} | mcc={res.get('mcc', 0):.3f} (ETA: {eta_min:.1f}min)")
        else:
            print(f"â†’ ERROR")
    
    total_time = time.time() - start_time
    print(f"\nTotal time: {total_time/60:.1f} minutes")
    
    # Convert to DataFrame
    results_df = pd.DataFrame(results)
    
    # Sort by QUALITY first, then BALANCE: maximize recall_min, then minimize recall_gap
    if "recall_gap" in results_df.columns:
        results_df = results_df.sort_values(
            ["recall_min", "recall_gap", "bal_acc"],
            ascending=[False, True, False]
        ).reset_index(drop=True)
    
    # Save
    if save_results:
        mode_str = "test" if test_mode else "full"
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        path = cfg.reports_dir / f"full_grid_{mode_str}_{timestamp}.csv"
        results_df.to_csv(path, index=False)
        print(f"\nResults saved to: {path}")
    
    # Print top results
    print("\n" + "=" * 70)
    print("TOP 15 COMBINATIONS:")
    print("=" * 70)
    top_cols = ["orb_mult", "gauss_window", "gauss_std", "exclude_str", "n_excluded", "bal_acc", "f1_min"]
    available = [c for c in top_cols if c in results_df.columns]
    print(results_df[available].head(15).to_string(index=False))
    
    return results_df


def evaluate_and_plot_best(
    df_market: pd.DataFrame,
    best_row: pd.Series,
) -> Dict:
    """
    Fully evaluate and visualize the best grid search result.
    
    Args:
        df_market: Market data
        best_row: Row from grid search results DataFrame (results_df.iloc[0])
    
    Returns:
        Dictionary with model, predictions, and metrics
    """
    from .visualization import plot_predictions, plot_confusion_matrix
    from sklearn.metrics import classification_report
    
    print("=" * 70)
    print("EVALUATING BEST GRID SEARCH RESULT")
    print("=" * 70)
    
    # Extract params
    orb_mult = float(best_row.get("orb_mult", 1.0))
    gauss_window = int(best_row.get("gauss_window", 201))
    gauss_std = float(best_row.get("gauss_std", 50.0))
    
    # Handle exclude_bodies - can be list or string
    exclude_bodies = best_row.get("exclude_bodies", [])
    if isinstance(exclude_bodies, str):
        exclude_bodies = [b.strip() for b in exclude_bodies.split(",") if b.strip() and b.strip() != "NONE"]
    
    print(f"\nBest params:")
    print(f"  orb_mult: {orb_mult}")
    print(f"  gauss_window: {gauss_window}")
    print(f"  gauss_std: {gauss_std}")
    print(f"  exclude_bodies: {exclude_bodies if exclude_bodies else 'NONE'}")
    
    # Check CUDA
    _, device = check_cuda_available()
    print(f"  device: {device}")
    
    # Initialize astro
    settings = init_ephemeris()
    
    # Calculate bodies
    print("\nCalculating body positions...")
    df_bodies, bodies_by_date = calculate_bodies_for_dates(
        df_market["date"], settings, progress=True
    )
    
    # Calculate aspects
    print(f"\nCalculating aspects (orb_mult={orb_mult})...")
    df_aspects = calculate_aspects_for_dates(
        bodies_by_date, settings, orb_mult=orb_mult, progress=True
    )
    
    # Create labels
    print(f"\nCreating labels (window={gauss_window}, std={gauss_std})...")
    df_labels = create_balanced_labels(
        df_market,
        gauss_window=gauss_window,
        gauss_std=gauss_std,
    )
    
    # Build features with exclusion
    print(f"\nBuilding features (exclude: {exclude_bodies if exclude_bodies else 'NONE'})...")
    df_features = build_full_features(
        df_bodies, df_aspects,
        exclude_bodies=exclude_bodies if exclude_bodies else None,
    )
    
    # Merge with labels
    df_dataset = merge_features_with_labels(df_features, df_labels)
    print(f"Dataset shape: {df_dataset.shape}")
    
    # Split
    train_df, val_df, test_df = split_dataset(df_dataset)
    feature_cols = [c for c in df_dataset.columns if c not in ["date", "target"]]
    
    X_train, y_train = prepare_xy(train_df, feature_cols)
    X_val, y_val = prepare_xy(val_df, feature_cols)
    X_test, y_test = prepare_xy(test_df, feature_cols)
    
    print(f"\nSplits:")
    print(f"  Train: {len(train_df)} samples")
    print(f"  Val:   {len(val_df)} samples")
    print(f"  Test:  {len(test_df)} samples")
    
    # Train model
    print("\nTraining model...")
    model = train_xgb_model(
        X_train, y_train, X_val, y_val,
        feature_cols, n_classes=2, device=device,
        n_estimators=500,
        max_depth=3,
        learning_rate=0.03,
    )
    
    # Tune threshold
    best_t, _ = tune_threshold(model, X_val, y_val, metric="bal_acc")
    print(f"\nBest threshold: {best_t:.3f}")
    
    # Predict
    y_pred = predict_with_threshold(model, X_test, threshold=best_t)
    
    # Full evaluation
    print("\n" + "=" * 80)
    print("FULL EVALUATION ON TEST SET")
    print("=" * 80)
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  Ğ§Ğ¢Ğ Ğ—Ğ”Ğ•Ğ¡Ğ¬ ĞŸĞ ĞĞ˜Ğ¡Ğ¥ĞĞ”Ğ˜Ğ¢:                                                       â•‘
â•‘  ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° ĞŸĞ ĞĞ¨Ğ›Ğ«Ğ¥ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ‚ĞµĞ¿ĞµÑ€ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ‘Ğ£Ğ”Ğ£Ğ©Ğ˜Ğ• Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ.   â•‘
â•‘  Ğ¢ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ - ÑÑ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞĞ˜ĞšĞĞ“Ğ”Ğ ĞĞ• Ğ’Ğ˜Ğ”Ğ•Ğ›Ğ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. â•‘
â•‘  Ğ­Ñ‚Ğ¾ Ñ‡ĞµÑÑ‚Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ°: ÑĞ¼Ğ¾Ğ¶ĞµÑ‚ Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…?       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
    
    metrics = calc_metrics(y_test, y_pred, [0, 1])
    
    # Get per-class metrics from classification report
    report_dict = classification_report(
        y_test, y_pred, labels=[0, 1],
        target_names=["DOWN", "UP"], output_dict=True, zero_division=0
    )
    
    f1_down = report_dict["DOWN"]["f1-score"]
    f1_up = report_dict["UP"]["f1-score"]
    f1_min = min(f1_down, f1_up)
    f1_gap = abs(f1_down - f1_up)
    
    recall_down = report_dict["DOWN"]["recall"]
    recall_up = report_dict["UP"]["recall"]
    recall_min = min(recall_down, recall_up)
    recall_gap = abs(recall_down - recall_up)
    
    precision_down = report_dict["DOWN"]["precision"]
    precision_up = report_dict["UP"]["precision"]
    
    # Count predictions
    n_pred_down = int((y_pred == 0).sum())
    n_pred_up = int((y_pred == 1).sum())
    n_true_down = int((y_test == 0).sum())
    n_true_up = int((y_test == 1).sum())
    
    # Correct predictions
    correct_down = int(((y_pred == 0) & (y_test == 0)).sum())  # True Negatives
    correct_up = int(((y_pred == 1) & (y_test == 1)).sum())    # True Positives
    
    print("=" * 80)
    print("ğŸ“Š Ğ¡Ğ¢ĞĞ¢Ğ˜Ğ¡Ğ¢Ğ˜ĞšĞ ĞŸĞ Ğ•Ğ”Ğ¡ĞšĞĞ—ĞĞĞ˜Ğ™:")
    print("=" * 80)
    print(f"""
  Ğ’ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ {len(y_test)} Ğ´Ğ½ĞµĞ¹:
    â€¢ Ğ ĞµĞ°Ğ»ÑŒĞ½Ğ¾ Ğ±Ñ‹Ğ»Ğ¾ DOWN (Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ): {n_true_down} Ğ´Ğ½ĞµĞ¹
    â€¢ Ğ ĞµĞ°Ğ»ÑŒĞ½Ğ¾ Ğ±Ñ‹Ğ»Ğ¾ UP (Ñ€Ğ¾ÑÑ‚):      {n_true_up} Ğ´Ğ½ĞµĞ¹
    
  ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ»Ğ°:
    â€¢ DOWN (Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ): {n_pred_down} Ñ€Ğ°Ğ·
    â€¢ UP (Ñ€Ğ¾ÑÑ‚):      {n_pred_up} Ñ€Ğ°Ğ·
    
  ĞŸÑ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹:
    â€¢ DOWNâ†’DOWN (Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ ÑƒĞ³Ğ°Ğ´Ğ°Ğ»Ğ¸ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ): {correct_down} Ğ¸Ğ· {n_true_down} = {recall_down*100:.1f}%
    â€¢ UPâ†’UP (Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ ÑƒĞ³Ğ°Ğ´Ğ°Ğ»Ğ¸ Ñ€Ğ¾ÑÑ‚):        {correct_up} Ğ¸Ğ· {n_true_up} = {recall_up*100:.1f}%
""")
    
    print("=" * 80)
    print("ğŸ¯ Ğ“Ğ›ĞĞ’ĞĞ«Ğ• ĞœĞ•Ğ¢Ğ Ğ˜ĞšĞ˜ (Ğ§Ğ•Ğœ Ğ’Ğ«Ğ¨Ğ• - Ğ¢Ğ•Ğœ Ğ›Ğ£Ğ§Ğ¨Ğ•):")
    print("=" * 80)
    print(f"""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ RECALL (ĞŸĞ¾Ğ»Ğ½Ğ¾Ñ‚Ğ°) - ĞºĞ°ĞºĞ¾Ğ¹ % Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ¹Ğ¼Ğ°Ğ»Ğ°:                â”‚
  â”‚   â€¢ RECALL DOWN = {recall_down:.1%}  (Ğ¸Ğ· Ğ²ÑĞµÑ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğ¹ ÑƒĞ³Ğ°Ğ´Ğ°Ğ»Ğ¸ {recall_down:.1%})     â”‚
  â”‚   â€¢ RECALL UP   = {recall_up:.1%}  (Ğ¸Ğ· Ğ²ÑĞµÑ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾ÑÑ‚Ğ¾Ğ² ÑƒĞ³Ğ°Ğ´Ğ°Ğ»Ğ¸ {recall_up:.1%})       â”‚
  â”‚   â€¢ RECALL MIN  = {recall_min:.1%}  â† Ğ¥Ğ£Ğ”Ğ¨Ğ˜Ğ™ ĞšĞ›ĞĞ¡Ğ¡ (Ğ³Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°!)            â”‚
  â”‚   â€¢ RECALL GAP  = {recall_gap:.1%}  â† Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ»Ğ°ÑÑĞ°Ğ¼Ğ¸ (Ñ‡ĞµĞ¼ Ğ¼ĞµĞ½ÑŒÑˆĞµ = Ğ»ÑƒÑ‡ÑˆĞµ) â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ PRECISION (Ğ¢Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ) - ĞºĞ°ĞºĞ¾Ğ¹ % Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»ÑÑ Ğ²ĞµÑ€Ğ½Ñ‹Ğ¼:                â”‚
  â”‚   â€¢ PRECISION DOWN = {precision_down:.1%}  (Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ñ… DOWN Ğ²ĞµÑ€Ğ½Ñ‹Ñ… {precision_down:.1%})    â”‚
  â”‚   â€¢ PRECISION UP   = {precision_up:.1%}  (Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ñ… UP Ğ²ĞµÑ€Ğ½Ñ‹Ñ… {precision_up:.1%})       â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ ĞĞ‘Ğ©Ğ˜Ğ• ĞœĞ•Ğ¢Ğ Ğ˜ĞšĞ˜:                                                              â”‚
  â”‚   â€¢ BALANCED ACCURACY = {metrics['bal_acc']:.1%}  (ÑÑ€ĞµĞ´Ğ½ĞµĞµ recall Ğ¾Ğ±Ğ¾Ğ¸Ñ… ĞºĞ»Ğ°ÑÑĞ¾Ğ²)      â”‚
  â”‚   â€¢ MCC = {metrics['mcc']:+.3f}  (Ğ¾Ñ‚ -1 Ğ´Ğ¾ +1, Ğ³Ğ´Ğµ 0 = ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğµ ÑƒĞ³Ğ°Ğ´Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ)       â”‚
  â”‚   â€¢ F1 MACRO = {metrics['f1_macro']:.1%}  (Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑÑ€ĞµĞ´Ğ½ĞµĞµ precision Ğ¸ recall)â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")
    
    # Quality assessment
    print("=" * 80)
    print("ğŸ“‹ ĞĞ¦Ğ•ĞĞšĞ ĞšĞĞ§Ğ•Ğ¡Ğ¢Ğ’Ğ:")
    print("=" * 80)
    
    if recall_min > 0.55:
        quality = "âœ… Ğ¥ĞĞ ĞĞ¨Ğ"
        quality_msg = "ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ° ĞºĞ»Ğ°ÑÑĞ° Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ³Ğ¾!"
    elif recall_min > 0.50:
        quality = "âš ï¸ Ğ¡Ğ Ğ•Ğ”ĞĞ•"
        quality_msg = "ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‡ÑƒÑ‚ÑŒ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ³Ğ°Ğ´Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ."
    else:
        quality = "âŒ ĞŸĞ›ĞĞ¥Ğ"
        quality_msg = "ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ…ÑƒĞ¶Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ³Ğ°Ğ´Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ· ĞºĞ»Ğ°ÑÑĞ¾Ğ²!"
    
    if recall_gap < 0.10:
        balance = "âœ… Ğ¡Ğ‘ĞĞ›ĞĞĞ¡Ğ˜Ğ ĞĞ’ĞĞĞ"
        balance_msg = "ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ğ¾ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ° ĞºĞ»Ğ°ÑÑĞ°."
    elif recall_gap < 0.20:
        balance = "âš ï¸ ĞĞ•Ğ‘ĞĞ›Ğ¬Ğ¨ĞĞ™ Ğ”Ğ˜Ğ¡Ğ‘ĞĞ›ĞĞĞ¡"
        balance_msg = "ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½ĞµĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ´Ğ¸Ğ½ ĞºĞ»Ğ°ÑÑ."
    else:
        balance = "âŒ Ğ¡Ğ˜Ğ›Ğ¬ĞĞ«Ğ™ Ğ”Ğ˜Ğ¡Ğ‘ĞĞ›ĞĞĞ¡"
        balance_msg = "ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ° Ğº Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ»Ğ°ÑÑÑƒ!"
    
    print(f"""
  ĞšĞĞ§Ğ•Ğ¡Ğ¢Ğ’Ğ:  {quality}
  â†’ {quality_msg}
  
  Ğ‘ĞĞ›ĞĞĞ¡:    {balance}
  â†’ {balance_msg}
  
  Ğ’Ğ•Ğ Ğ”Ğ˜ĞšĞ¢:   {"ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ!" if recall_min > 0.52 and recall_gap < 0.15 else "Ğ¢Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."}
""")
    
    print("=" * 80)
    print("ğŸ“‹ ĞŸĞĞ›ĞĞ«Ğ™ ĞĞ¢Ğ§Ğ•Ğ¢ SKLEARN (Classification Report):")
    print("=" * 80)
    print("""
  Ğ§Ñ‚Ğ¾ Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ°ÑÑ‚ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸:
    â€¢ precision - Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ: Ğ¸Ğ· Ğ²ÑĞµÑ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ ĞºĞ»Ğ°ÑÑĞ°, ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ % Ğ²ĞµÑ€Ğ½Ñ‹Ñ…
    â€¢ recall    - Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğ°: Ğ¸Ğ· Ğ²ÑĞµÑ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² ĞºĞ»Ğ°ÑÑĞ°, ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ % Ğ½Ğ°ÑˆĞ»Ğ¸  
    â€¢ f1-score  - Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑÑ€ĞµĞ´Ğ½ĞµĞµ precision Ğ¸ recall
    â€¢ support   - ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² ÑÑ‚Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ°ÑÑĞ° Ğ² Ñ‚ĞµÑÑ‚Ğµ
""")
    report = classification_report(
        y_test, y_pred, labels=[0, 1],
        target_names=["DOWN", "UP"], zero_division=0
    )
    print(report)
    
    # Confusion matrix
    print("\nConfusion Matrix:")
    plot_confusion_matrix(y_test, y_pred)
    
    # Predictions plot
    print("\nPredictions vs True Labels:")
    
    # Prepare test_df with close prices
    test_df_plot = test_df.copy()
    test_df_plot["date"] = pd.to_datetime(test_df_plot["date"])
    market_close = df_market[["date", "close"]].copy()
    market_close["date"] = pd.to_datetime(market_close["date"])
    test_df_plot = test_df_plot.merge(market_close, on="date", how="left")
    
    plot_predictions(test_df_plot, y_pred, y_true=y_test, price_mode="log")
    
    return {
        "model": model,
        "threshold": best_t,
        "y_pred": y_pred,
        "y_test": y_test,
        "metrics": metrics,
        "feature_cols": feature_cols,
        "params": {
            "orb_mult": orb_mult,
            "gauss_window": gauss_window,
            "gauss_std": gauss_std,
            "exclude_bodies": exclude_bodies,
        },
    }
