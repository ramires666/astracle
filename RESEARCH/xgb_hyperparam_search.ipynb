{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Astro Trading Pipeline - Modular Research Notebook\n",
    "\n",
    "\n",
    "\n",
    " This is the **central orchestrating notebook** that imports and runs all modules.\n",
    "\n",
    "\n",
    "\n",
    " ## Pipeline Steps:\n",
    "\n",
    " 1. Setup & Config\n",
    "\n",
    " 2. Load Market Data (from PostgreSQL)\n",
    "\n",
    " 3. Visualize Price\n",
    "\n",
    " 4. Create Labels (balanced UP/DOWN)\n",
    "\n",
    " 5. Compute Astro Data\n",
    "\n",
    " 6. Build Features\n",
    "\n",
    " 7. Train Model\n",
    "\n",
    " 8. Evaluate & Visualize\n",
    "\n",
    " 9. (Optional) Grid Search\n",
    "\n",
    " 10. Save Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 0. Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All dependencies found\n"
     ]
    }
   ],
   "source": [
    "# Check dependencies\n",
    "import importlib.util as iu\n",
    "\n",
    "required = [\"xgboost\", \"sklearn\", \"matplotlib\", \"seaborn\", \"tqdm\", \"pyarrow\", \"psycopg2\", \"swisseph\"]\n",
    "missing = [pkg for pkg in required if iu.find_spec(pkg) is None]\n",
    "\n",
    "if missing:\n",
    "    print(\"Missing packages:\", \", \".join(missing))\n",
    "    print(\"Install with: conda install -c conda-forge \" + \" \".join(missing))\n",
    "else:\n",
    "    print(\"‚úì All dependencies found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /home/rut/ostrofun\n"
     ]
    }
   ],
   "source": [
    "# Import RESEARCH modules\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Find project root and add to path\n",
    "PROJECT_ROOT = Path.cwd().resolve()\n",
    "if not (PROJECT_ROOT / \"RESEARCH\").exists():\n",
    "    for parent in PROJECT_ROOT.parents:\n",
    "        if (parent / \"RESEARCH\").exists():\n",
    "            PROJECT_ROOT = parent\n",
    "            break\n",
    "\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"PROJECT_ROOT: {PROJECT_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all modules\n",
    "from RESEARCH.config import cfg, PROJECT_ROOT\n",
    "from RESEARCH.data_loader import load_market_data, get_latest_date, get_data_paths\n",
    "from RESEARCH.labeling import create_balanced_labels, gaussian_smooth_centered\n",
    "from RESEARCH.astro_engine import (\n",
    "    init_ephemeris,\n",
    "    calculate_bodies_for_dates,\n",
    "    calculate_bodies_for_dates_multi,  # ‚Üê –î–û–ë–ê–í–õ–ï–ù–û\n",
    "    calculate_aspects_for_dates,\n",
    "    calculate_transits_for_dates,\n",
    "    get_natal_bodies,\n",
    ")\n",
    "from RESEARCH.features import (\n",
    "    build_full_features,\n",
    "    merge_features_with_labels,\n",
    "    get_feature_columns,\n",
    "    get_feature_inventory,\n",
    ")\n",
    "from RESEARCH.model_training import (\n",
    "    split_dataset,\n",
    "    prepare_xy,\n",
    "    train_xgb_model,\n",
    "    tune_threshold,\n",
    "    predict_with_threshold,\n",
    "    evaluate_model,\n",
    "    get_feature_importance,\n",
    "    check_cuda_available,\n",
    ")\n",
    "from RESEARCH.visualization import (\n",
    "    plot_price_distribution,\n",
    "    plot_class_distribution,\n",
    "    plot_price_with_labels,\n",
    "    plot_last_n_days,\n",
    "    plot_confusion_matrix,\n",
    "    plot_feature_importance,\n",
    "    plot_predictions,\n",
    ")\n",
    "\n",
    "print(\"‚úì All RESEARCH modules imported\")\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# üéØ –í–´–ë–ï–†–ò–¢–ï –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Æ –ó–î–ï–°–¨!\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# –≠—Ç–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –í–ï–ó–î–ï –≤ –Ω–æ—É—Ç–±—É–∫–µ:\n",
    "# - –°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç–æ–∫ (gauss_window, gauss_std)\n",
    "# - –†–∞—Å—á—ë—Ç –∞—Å–ø–µ–∫—Ç–æ–≤ (orb_mult)\n",
    "# - –í—ã–±–æ—Ä –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç (coord_mode: geo, helio, both)\n",
    "# - –ò—Å–∫–ª—é—á–µ–Ω–∏–µ —Ç–µ–ª (exclude_bodies)\n",
    "\n",
    "CONFIGS = {\n",
    "    \"CONFIG_1\": {\n",
    "        \"name\": \"Old Baseline (geo -Uranus,Pluto)\",\n",
    "        \"coord_mode\": \"geo\",\n",
    "        \"orb_mult\": 0.25,\n",
    "        \"gauss_window\": 201,\n",
    "        \"gauss_std\": 50.0,\n",
    "        \"exclude_bodies\": [\"Uranus\", \"Pluto\"],\n",
    "        \"baseline_r_min\": 0.578,\n",
    "        \"baseline_mcc\": 0.159,\n",
    "    },\n",
    "    \"CONFIG_2\": {\n",
    "        \"name\": \"New Best (both all bodies)\",\n",
    "        \"coord_mode\": \"both\",\n",
    "        \"orb_mult\": 0.15,\n",
    "        \"gauss_window\": 300,\n",
    "        \"gauss_std\": 70.0,\n",
    "        \"exclude_bodies\": None,  # –í–°–ï –¢–ï–õ–ê –≤–∫–ª—é—á–µ–Ω—ã\n",
    "        \"baseline_r_min\": 0.587,\n",
    "        \"baseline_mcc\": 0.182,\n",
    "    },\n",
    "}\n",
    "\n",
    "ACTIVE_CONFIG = \"CONFIG_2\"  # ‚Üê –ò–ó–ú–ï–ù–ò–¢–¨ –ó–î–ï–°–¨!\n",
    "config = CONFIGS[ACTIVE_CONFIG]\n",
    "\n",
    "print(f\"\\nüéØ ACTIVE CONFIG: {ACTIVE_CONFIG}\")\n",
    "print(f\"   {config['name']}\")\n",
    "print(f\"   coord_mode: {config['coord_mode']}, orb: {config['orb_mult']}\")\n",
    "print(f\"   gauss: {config['gauss_window']}/{config['gauss_std']}\")\n",
    "print(f\"   exclude: {config['exclude_bodies']}\")\n",
    "print(f\"   baseline: R_MIN={config['baseline_r_min']}, MCC={config['baseline_mcc']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show configuration\n",
    "print(f\"\\nActive subject: {cfg.active_subject_id}\")\n",
    "print(f\"Data root: {cfg.data_root}\")\n",
    "print(f\"DB URL configured: {bool(cfg.db_url)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Load Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load market data from database\n",
    "df_market = load_market_data()\n",
    "\n",
    "# Optional: filter by start date\n",
    "DATA_START = \"2017-11-01\"\n",
    "df_market = df_market[df_market[\"date\"] >= DATA_START].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nMarket data: {len(df_market)} rows\")\n",
    "print(f\"Date range: {df_market['date'].min().date()} ‚Üí {df_market['date'].max().date()}\")\n",
    "df_market.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3. Visualize Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price and return distribution\n",
    "plot_price_distribution(df_market, price_mode=\"log\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4. Create Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for labeling - –ë–ï–†–Å–ú –ò–ó config!\n",
    "LABEL_CONFIG = {\n",
    "    \"horizon\": 1,           # Prediction horizon (days)\n",
    "    \"move_share\": 0.5,      # Total share of samples to keep\n",
    "    \"gauss_window\": config[\"gauss_window\"],  # ‚Üê –ò–ó CONFIG\n",
    "    \"gauss_std\": config[\"gauss_std\"],        # ‚Üê –ò–ó CONFIG\n",
    "    \"price_mode\": \"raw\",\n",
    "    \"label_mode\": \"balanced_detrended\",\n",
    "}\n",
    "\n",
    "print(f\"üè∑Ô∏è Label config: window={LABEL_CONFIG['gauss_window']}, std={LABEL_CONFIG['gauss_std']}\")\n",
    "\n",
    "# Create balanced labels\n",
    "df_labels = create_balanced_labels(\n",
    "    df_market,\n",
    "    horizon=LABEL_CONFIG[\"horizon\"],\n",
    "    move_share=LABEL_CONFIG[\"move_share\"],\n",
    "    gauss_window=LABEL_CONFIG[\"gauss_window\"],\n",
    "    gauss_std=LABEL_CONFIG[\"gauss_std\"],\n",
    "    price_mode=LABEL_CONFIG[\"price_mode\"],\n",
    "    label_mode=LABEL_CONFIG[\"label_mode\"],\n",
    ")\n",
    "\n",
    "df_labels.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "plot_class_distribution(df_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price with labels\n",
    "plot_price_with_labels(df_market, df_labels, price_mode=\"raw\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last 30 days\n",
    "plot_last_n_days(df_market, df_labels, n_days=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5. Compute Astro Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ephemeris\n",
    "settings = init_ephemeris()\n",
    "print(f\"Bodies: {[b.name for b in settings.bodies]}\")\n",
    "print(f\"Aspects: {[a.name for a in settings.aspects]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate body positions for all dates - –ò–°–ü–û–õ–¨–ó–£–ï–ú coord_mode –∏–∑ config!\n",
    "print(f\"\\nüìç –†–∞—Å—á—ë—Ç –ø–æ–∑–∏—Ü–∏–π (coord_mode={config['coord_mode']})...\")\n",
    "\n",
    "if config[\"coord_mode\"] == \"geo\":\n",
    "    df_bodies, bodies_by_date = calculate_bodies_for_dates(\n",
    "        df_market[\"date\"], settings, progress=True\n",
    "    )\n",
    "else:\n",
    "    # both –∏–ª–∏ helio - –∏—Å–ø–æ–ª—å–∑—É–µ–º multi –≤–µ—Ä—Å–∏—é\n",
    "    df_bodies, geo_by_date, helio_by_date = calculate_bodies_for_dates_multi(\n",
    "        df_market[\"date\"], settings, coord_mode=config[\"coord_mode\"], progress=True\n",
    "    )\n",
    "    bodies_by_date = geo_by_date if geo_by_date else helio_by_date\n",
    "\n",
    "print(f\"‚úì Bodies calculated: {len(df_bodies)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate aspects - –ò–°–ü–û–õ–¨–ó–£–ï–ú orb_mult –∏–∑ config!\n",
    "print(f\"\\nüìê –†–∞—Å—á—ë—Ç –∞—Å–ø–µ–∫—Ç–æ–≤ (orb={config['orb_mult']})...\")\n",
    "\n",
    "df_aspects = calculate_aspects_for_dates(\n",
    "    bodies_by_date,\n",
    "    settings,\n",
    "    orb_mult=config[\"orb_mult\"],  # ‚Üê –ò–ó CONFIG\n",
    "    progress=True,\n",
    ")\n",
    "print(f\"‚úì Aspects: {len(df_aspects)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate phases - —Ñ–∞–∑—ã –õ—É–Ω—ã –∏ —ç–ª–æ–Ω–≥–∞—Ü–∏–∏ –ø–ª–∞–Ω–µ—Ç\n",
    "print(\"\\nüåô –†–∞—Å—á—ë—Ç —Ñ–∞–∑ –õ—É–Ω—ã –∏ —ç–ª–æ–Ω–≥–∞—Ü–∏–π...\")\n",
    "from RESEARCH.astro_engine import calculate_phases_for_dates\n",
    "df_phases = calculate_phases_for_dates(bodies_by_date, progress=True)\n",
    "print(f\"‚úì Phases: {len(df_phases)} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 6. Build Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build feature matrix - –ò–°–ü–û–õ–¨–ó–£–ï–ú exclude_bodies –∏ df_phases –∏–∑ config!\n",
    "print(f\"\\nüîß Building features (exclude={config['exclude_bodies']})...\")\n",
    "\n",
    "df_features = build_full_features(\n",
    "    df_bodies,\n",
    "    df_aspects,\n",
    "    df_phases=df_phases,  # ‚Üê –î–û–ë–ê–í–õ–Ø–ï–ú –§–ê–ó–´!\n",
    "    exclude_bodies=config[\"exclude_bodies\"],  # ‚Üê –ò–ó CONFIG\n",
    ")\n",
    "\n",
    "print(f\"‚úì Features shape: {df_features.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with labels\n",
    "df_dataset = merge_features_with_labels(df_features, df_labels)\n",
    "print(f\"\\nDataset shape: {df_dataset.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature inventory\n",
    "feature_inventory = get_feature_inventory(df_dataset)\n",
    "print(\"\\nFeature groups:\")\n",
    "print(feature_inventory.groupby(\"group\").size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 7. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA availability\n",
    "use_cuda, device = check_cuda_available()\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset (time-based)\n",
    "train_df, val_df, test_df = split_dataset(df_dataset, train_ratio=0.7, val_ratio=0.15)\n",
    "\n",
    "print(f\"Train: {train_df['date'].min().date()} ‚Üí {train_df['date'].max().date()}\")\n",
    "print(f\"Val:   {val_df['date'].min().date()} ‚Üí {val_df['date'].max().date()}\")\n",
    "print(f\"Test:  {test_df['date'].min().date()} ‚Üí {test_df['date'].max().date()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X, y\n",
    "feature_cols = get_feature_columns(df_dataset)\n",
    "X_train, y_train = prepare_xy(train_df, feature_cols)\n",
    "X_val, y_val = prepare_xy(val_df, feature_cols)\n",
    "X_test, y_test = prepare_xy(test_df, feature_cols)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_val:   {X_val.shape}, y_val:   {y_val.shape}\")\n",
    "print(f\"X_test:  {X_test.shape}, y_test:  {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost model with BASELINE params (–ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –≤ single_body_search)\n",
    "MODEL_PARAMS = {\n",
    "    \"n_estimators\": 300,   # ‚Üê Baseline\n",
    "    \"max_depth\": 3,        # ‚Üê Baseline (–º–µ–Ω—å—à–µ = –º–µ–Ω—å—à–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è)\n",
    "    \"learning_rate\": 0.03, # ‚Üê Baseline\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "}\n",
    "\n",
    "print(f\"üå≥ Model params: {MODEL_PARAMS}\")\n",
    "\n",
    "model = train_xgb_model(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    feature_cols,\n",
    "    n_classes=2,\n",
    "    device=device,\n",
    "    **MODEL_PARAMS,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune threshold on validation set\n",
    "# –¢–ï–ü–ï–†–¨ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç recall_min (–∫–∞—á–µ—Å—Ç–≤–æ —Ö—É–¥—à–µ–≥–æ –∫–ª–∞—Å—Å–∞)\n",
    "best_threshold, best_score = tune_threshold(model, X_val, y_val)  # metric=\"recall_min\" –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 8. Evaluate & Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "y_pred = predict_with_threshold(model, X_test, threshold=best_threshold)\n",
    "\n",
    "# Evaluate\n",
    "results = evaluate_model(y_test, y_pred, label_names=[\"DOWN\", \"UP\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "imp_df = get_feature_importance(model, feature_cols, top_n=20)\n",
    "plot_feature_importance(imp_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on test set\n",
    "# –¢–µ–ø–µ—Ä—å df_dataset —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ –¥–Ω–∏ —Å forward-filled –º–µ—Ç–∫–∞–º–∏\n",
    "# –ù—É–∂–Ω–æ —Ç–æ–ª—å–∫–æ –¥–æ–±–∞–≤–∏—Ç—å close –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "import pandas as pd  # –ù–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ\n",
    "\n",
    "test_df_plot = test_df.copy()\n",
    "test_df_plot[\"date\"] = pd.to_datetime(test_df_plot[\"date\"])\n",
    "test_df_plot = test_df_plot.merge(\n",
    "    df_market[[\"date\", \"close\"]].assign(date=lambda x: pd.to_datetime(x[\"date\"])), \n",
    "    on=\"date\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "plot_predictions(test_df_plot, y_pred, y_true=y_test, price_mode=\"log\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 9. (Optional) Grid Search\n",
    "\n",
    "\n",
    "\n",
    " –í—ã–±–µ—Ä–∏—Ç–µ –æ–¥–∏–Ω –∏–∑ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –Ω–∏–∂–µ:\n",
    "\n",
    " - **run_grid_search** ‚Äî –æ—Å–Ω–æ–≤–Ω–æ–π –ø–æ–∏—Å–∫ (coord + gauss + orb)\n",
    "\n",
    " - **run_full_grid_search** ‚Äî –ø–æ–ª–Ω—ã–π –ø–æ–∏—Å–∫ —Å body ablation\n",
    "\n",
    " - **run_body_ablation_search** ‚Äî —Ç–æ–ª—å–∫–æ ablation —Ç–µ–ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# üå≥ XGBOOST HYPERPARAMETER SEARCH\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "#\n",
    "# –≠—Ç–æ—Ç –Ω–æ—É—Ç–±—É–∫ –ø–µ—Ä–µ–±–∏—Ä–∞–µ—Ç –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã XGBoost –¥–ª—è –¥–≤—É—Ö –ª—É—á—à–∏—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π:\n",
    "#\n",
    "# CONFIG 1 (—Å—Ç–∞—Ä—ã–π baseline):\n",
    "#   geo O=0.25 W=201 S=50.0 -[Uranus,Pluto]\n",
    "#   R_MIN=0.578, MCC=0.159\n",
    "#\n",
    "# CONFIG 2 (–Ω–æ–≤—ã–π –ª—É—á—à–∏–π):\n",
    "#   both O=0.15 W=300 S=70\n",
    "#   R_MIN=0.587, MCC=0.182\n",
    "#\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "from RESEARCH.grid_search import evaluate_combo\n",
    "from RESEARCH.astro_engine import init_ephemeris, calculate_bodies_for_dates_multi, precompute_angles_for_dates\n",
    "from RESEARCH.model_training import check_cuda_available\n",
    "from itertools import product\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "settings = init_ephemeris()\n",
    "_, device = check_cuda_available()\n",
    "print(f\"üñ•Ô∏è –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "#  XGBOOST HYPERPARAMETERS ‚Äî –ß–¢–û –ü–ï–†–ï–ë–ò–†–ê–ï–ú\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# CONFIGS –∏ ACTIVE_CONFIG —É–∂–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –≤ –Ω–∞—á–∞–ª–µ —Ñ–∞–π–ª–∞!\n",
    "\n",
    "# n_estimators ‚Äî –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤\n",
    "N_ESTIMATORS = [100, 200, 300, 500, 700]\n",
    "\n",
    "# max_depth ‚Äî –≥–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤–∞ (–º–µ–Ω—å—à–µ = –º–µ–Ω—å—à–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è)\n",
    "MAX_DEPTHS = [2, 3, 4, 5, 6]\n",
    "\n",
    "# learning_rate ‚Äî —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è\n",
    "LEARNING_RATES = [0.01, 0.02, 0.03, 0.05, 0.1]\n",
    "\n",
    "# subsample ‚Äî –¥–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –∫–∞–∂–¥–æ–µ –¥–µ—Ä–µ–≤–æ\n",
    "SUBSAMPLES = [0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "# colsample_bytree ‚Äî –¥–æ–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –∫–∞–∂–¥–æ–µ –¥–µ—Ä–µ–≤–æ\n",
    "COLSAMPLES = [0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "# early_stopping_rounds ‚Äî –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –µ—Å–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –Ω–µ —É–ª—É—á—à–∞–µ—Ç—Å—è\n",
    "EARLY_STOPPING = 50\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# –†–µ–∂–∏–º –ø–µ—Ä–µ–±–æ—Ä–∞: \"full\" –∏–ª–∏ \"fast\"\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# \"full\" = –≤—Å–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ (5^5 = 3125) ‚Äî –î–û–õ–ì–û!\n",
    "# \"fast\" = —Ç–æ–ª—å–∫–æ –≤–∞–∂–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (n_estimators, max_depth, learning_rate)\n",
    "SEARCH_MODE = \"fast\"  # ‚Üê –ò–∑–º–µ–Ω–∏—Ç—å –Ω–∞ \"full\" –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –ø–µ—Ä–µ–±–æ—Ä–∞\n",
    "\n",
    "# –ü–æ–¥—Å—á—ë—Ç –∫–æ–º–±–∏–Ω–∞—Ü–∏–π\n",
    "if SEARCH_MODE == \"full\":\n",
    "    total_combos = len(N_ESTIMATORS) * len(MAX_DEPTHS) * len(LEARNING_RATES) * len(SUBSAMPLES) * len(COLSAMPLES)\n",
    "    param_combos = list(product(N_ESTIMATORS, MAX_DEPTHS, LEARNING_RATES, SUBSAMPLES, COLSAMPLES))\n",
    "else:\n",
    "    total_combos = len(N_ESTIMATORS) * len(MAX_DEPTHS) * len(LEARNING_RATES)\n",
    "    param_combos = list(product(N_ESTIMATORS, MAX_DEPTHS, LEARNING_RATES, [0.8], [0.8]))\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üå≥ XGBOOST HYPERPARAMETER SEARCH\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\"\"\n",
    "üìä –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é: {config['name']}\n",
    "   Coord:    {config['coord_mode']}\n",
    "   Orb:      {config['orb_mult']}\n",
    "   Window:   {config['gauss_window']}\n",
    "   Std:      {config['gauss_std']}\n",
    "   Exclude:  {config['exclude_bodies']}\n",
    "   \n",
    "   üìå BASELINE:\n",
    "      R_MIN = {config['baseline_r_min']:.3f}\n",
    "      MCC   = {config['baseline_mcc']:.3f}\n",
    "\n",
    "üéõÔ∏è XGBoost –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø–µ—Ä–µ–±–æ—Ä–∞:\n",
    "   ‚Ä¢ n_estimators:    {N_ESTIMATORS}\n",
    "   ‚Ä¢ max_depth:       {MAX_DEPTHS}\n",
    "   ‚Ä¢ learning_rate:   {LEARNING_RATES}\n",
    "   ‚Ä¢ subsample:       {SUBSAMPLES if SEARCH_MODE == 'full' else '[0.8]'}\n",
    "   ‚Ä¢ colsample:       {COLSAMPLES if SEARCH_MODE == 'full' else '[0.8]'}\n",
    "\n",
    "üìà –†–µ–∂–∏–º: {SEARCH_MODE.upper()}\n",
    "üìà –í—Å–µ–≥–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏–π: {total_combos}\n",
    "   –ü—Ä–∏ ~51 —Ä–∞—Å—á/–º–∏–Ω —ç—Ç–æ –∑–∞–π–º—ë—Ç ~{total_combos/51:.1f} –º–∏–Ω—É—Ç\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# üìç –ü–†–ï–î–í–ê–†–ò–¢–ï–õ–¨–ù–´–ô –†–ê–°–ß–Å–¢ –ê–°–¢–†–û-–î–ê–ù–ù–´–•\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "print(\"\\nüìç –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞—Å—á—ë—Ç –ø–æ–∑–∏—Ü–∏–π...\")\n",
    "df_bodies, geo_by_date, helio_by_date = calculate_bodies_for_dates_multi(\n",
    "    df_market[\"date\"], settings, coord_mode=config[\"coord_mode\"], progress=True\n",
    ")\n",
    "bodies_by_date = geo_by_date if geo_by_date else helio_by_date\n",
    "\n",
    "print(\"\\nüìê –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞—Å—á—ë—Ç —É–≥–ª–æ–≤...\")\n",
    "angles_cache = precompute_angles_for_dates(bodies_by_date, progress=True)\n",
    "\n",
    "print(\"‚úì –ö—ç—à –≥–æ—Ç–æ–≤!\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# üöÄ –ó–ê–ü–£–°–ö HYPERPARAMETER SEARCH\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "results = []\n",
    "best_so_far = {\"R_MIN\": 0, \"combo\": None}\n",
    "\n",
    "# Checkpoint settings\n",
    "CHECKPOINT_EVERY = 100\n",
    "checkpoint_path = PROJECT_ROOT / \"data\" / \"market\" / \"reports\" / f\"xgb_hyperparam_{ACTIVE_CONFIG}_checkpoint.parquet\"\n",
    "\n",
    "print(f\"üî¢ –í—Å–µ–≥–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏–π: {len(param_combos)}\")\n",
    "print(f\"üíæ Checkpoint –∫–∞–∂–¥—ã–µ {CHECKPOINT_EVERY} –∏—Ç–µ—Ä–∞—Ü–∏–π ‚Üí {checkpoint_path}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, params in enumerate(param_combos):\n",
    "    if SEARCH_MODE == \"full\":\n",
    "        n_est, max_d, lr, subsample, colsample = params\n",
    "    else:\n",
    "        n_est, max_d, lr, subsample, colsample = params\n",
    "    \n",
    "    model_params = {\n",
    "        \"n_estimators\": n_est,\n",
    "        \"max_depth\": max_d,\n",
    "        \"learning_rate\": lr,\n",
    "        \"subsample\": subsample,\n",
    "        \"colsample_bytree\": colsample,\n",
    "        \"early_stopping_rounds\": EARLY_STOPPING,\n",
    "    }\n",
    "    \n",
    "    params_str = f\"[{i+1}/{len(param_combos)}] n={n_est} d={max_d} lr={lr}\"\n",
    "    if SEARCH_MODE == \"full\":\n",
    "        params_str += f\" sub={subsample} col={colsample}\"\n",
    "    \n",
    "    try:\n",
    "        res = evaluate_combo(\n",
    "            df_market, df_bodies, bodies_by_date, settings,\n",
    "            config[\"orb_mult\"], config[\"gauss_window\"], config[\"gauss_std\"],\n",
    "            exclude_bodies=config[\"exclude_bodies\"],\n",
    "            angles_cache=angles_cache,\n",
    "            device=device,\n",
    "            model_params=model_params,\n",
    "        )\n",
    "        res[\"n_estimators\"] = n_est\n",
    "        res[\"max_depth\"] = max_d\n",
    "        res[\"learning_rate\"] = lr\n",
    "        res[\"subsample\"] = subsample\n",
    "        res[\"colsample\"] = colsample\n",
    "        results.append(res)\n",
    "        \n",
    "        if \"error\" not in res:\n",
    "            r_up = res['recall_up']\n",
    "            r_down = res['recall_down']\n",
    "            r_min = res['recall_min']\n",
    "            f1 = res.get('f1_macro', 0)\n",
    "            acc = res.get('bal_acc', 0)\n",
    "            mcc = res['mcc']\n",
    "            \n",
    "            # Update best\n",
    "            if r_min > best_so_far[\"R_MIN\"]:\n",
    "                best_so_far[\"R_MIN\"] = r_min\n",
    "                best_so_far[\"MCC\"] = mcc\n",
    "                best_so_far[\"combo\"] = f\"n={n_est} d={max_d} lr={lr}\"\n",
    "                best_so_far[\"params\"] = model_params.copy()\n",
    "            \n",
    "            # –í—ã–≤–æ–¥ –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫ –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É\n",
    "            metrics_str = f\"R‚Üë={r_up:.2f} R‚Üì={r_down:.2f} R_MIN={r_min:.3f} F1={f1:.3f} ACC={acc:.3f} MCC={mcc:.3f}\"\n",
    "            print(f\"{params_str:<45} ‚Üí {metrics_str}\")\n",
    "            print(f\"   üèÜ BEST: R_MIN={best_so_far['R_MIN']:.3f} MCC={best_so_far.get('MCC', 0):.3f} ({best_so_far['combo']})\")\n",
    "        else:\n",
    "            print(f\"{params_str:<50} ‚Üí ERROR: {res.get('error')}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"{params_str:<50} ‚Üí CRASH: {e}\")\n",
    "        results.append({\n",
    "            \"n_estimators\": n_est, \"max_depth\": max_d,\n",
    "            \"learning_rate\": lr, \"error\": str(e)\n",
    "        })\n",
    "    \n",
    "    # üíæ Checkpoint every N iterations\n",
    "    if (i + 1) % CHECKPOINT_EVERY == 0:\n",
    "        checkpoint_df = pd.DataFrame(results)\n",
    "        checkpoint_df.to_parquet(checkpoint_path, index=False)\n",
    "        print(f\"\\nüíæ CHECKPOINT saved: {len(results)} results ‚Üí {checkpoint_path.name}\\n\")\n",
    "\n",
    "# Final save\n",
    "if results:\n",
    "    final_df = pd.DataFrame(results)\n",
    "    final_df.to_parquet(checkpoint_path, index=False)\n",
    "    print(f\"\\nüíæ FINAL saved: {len(results)} results ‚Üí {checkpoint_path.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# üìä RESULTS\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä RESULTS: XGBOOST HYPERPARAMETER SEARCH\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "üìå Config: {config['name']}\n",
    "üìå BASELINE:\n",
    "   R_MIN = {config['baseline_r_min']:.3f}\n",
    "   MCC   = {config['baseline_mcc']:.3f}\n",
    "\"\"\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "if \"recall_min\" in results_df.columns:\n",
    "    # –î–æ–±–∞–≤–ª—è–µ–º delta –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ baseline\n",
    "    results_df[\"delta_R_MIN\"] = results_df[\"recall_min\"] - config[\"baseline_r_min\"]\n",
    "    results_df[\"delta_MCC\"] = results_df[\"mcc\"] - config[\"baseline_mcc\"]\n",
    "    \n",
    "    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ recall_min\n",
    "    results_df = results_df.sort_values(\"recall_min\", ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # –¢–æ–ø-20 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "    print(\"\\nüèÜ TOP 20 BEST HYPERPARAMETER COMBINATIONS:\")\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"{'#':<3} {'n_est':<6} {'depth':<6} {'lr':<6} {'sub':<5} {'col':<5} {'R_MIN':<7} {'Œî R_MIN':<9} {'MCC':<7} {'Status':<10}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for i, row in results_df.head(20).iterrows():\n",
    "        if \"error\" in row and pd.notna(row.get(\"error\")):\n",
    "            continue\n",
    "            \n",
    "        delta_r = row.get(\"delta_R_MIN\", 0)\n",
    "        \n",
    "        if delta_r > 0:\n",
    "            status = \"‚úÖ BETTER\"\n",
    "        elif delta_r > -0.02:\n",
    "            status = \"üü° ~SAME\"\n",
    "        else:\n",
    "            status = \"‚ùå WORSE\"\n",
    "        \n",
    "        print(f\"{i+1:<3} {row['n_estimators']:<6} {row['max_depth']:<6} {row['learning_rate']:<6} \"\n",
    "              f\"{row['subsample']:<5} {row['colsample']:<5} \"\n",
    "              f\"{row['recall_min']:<7.3f} {delta_r:<+9.3f} {row['mcc']:<7.3f} {status:<10}\")\n",
    "    \n",
    "    print(\"-\" * 100)\n",
    "    print(f\"{'---':<3} {'300':<6} {'3':<6} {'0.03':<6} {'0.8':<5} {'0.8':<5} \"\n",
    "          f\"{config['baseline_r_min']:<7.3f} {'---':<9} {config['baseline_mcc']:<7.3f} BASELINE\")\n",
    "    \n",
    "    # –ê–Ω–∞–ª–∏–∑ –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º\n",
    "    print(\"\\nüìä ANALYSIS BY PARAMETER:\")\n",
    "    \n",
    "    print(\"\\n   By n_estimators:\")\n",
    "    for n in N_ESTIMATORS:\n",
    "        subset = results_df[results_df[\"n_estimators\"] == n]\n",
    "        if not subset.empty:\n",
    "            best_r = subset[\"recall_min\"].max()\n",
    "            avg_r = subset[\"recall_min\"].mean()\n",
    "            print(f\"      n={n:<4}: best R_MIN={best_r:.3f}, avg={avg_r:.3f}\")\n",
    "    \n",
    "    print(\"\\n   By max_depth:\")\n",
    "    for d in MAX_DEPTHS:\n",
    "        subset = results_df[results_df[\"max_depth\"] == d]\n",
    "        if not subset.empty:\n",
    "            best_r = subset[\"recall_min\"].max()\n",
    "            avg_r = subset[\"recall_min\"].mean()\n",
    "            print(f\"      d={d}: best R_MIN={best_r:.3f}, avg={avg_r:.3f}\")\n",
    "    \n",
    "    print(\"\\n   By learning_rate:\")\n",
    "    for lr in LEARNING_RATES:\n",
    "        subset = results_df[results_df[\"learning_rate\"] == lr]\n",
    "        if not subset.empty:\n",
    "            best_r = subset[\"recall_min\"].max()\n",
    "            avg_r = subset[\"recall_min\"].mean()\n",
    "            print(f\"      lr={lr:<5}: best R_MIN={best_r:.3f}, avg={avg_r:.3f}\")\n",
    "\n",
    "# Save\n",
    "out_path = PROJECT_ROOT / \"data\" / \"market\" / \"reports\" / f\"xgb_hyperparam_{ACTIVE_CONFIG}.csv\"\n",
    "results_df.to_csv(out_path, index=False)\n",
    "print(f\"\\nüíæ Results saved: {out_path}\")\n",
    "\n",
    "# Best overall\n",
    "if not results_df.empty and \"recall_min\" in results_df.columns:\n",
    "    best = results_df.iloc[0]\n",
    "    delta = best['recall_min'] - config['baseline_r_min']\n",
    "    print(f\"\\nüèÜ BEST HYPERPARAMETERS:\")\n",
    "    print(f\"   n_estimators:    {best['n_estimators']}\")\n",
    "    print(f\"   max_depth:       {best['max_depth']}\")\n",
    "    print(f\"   learning_rate:   {best['learning_rate']}\")\n",
    "    print(f\"   subsample:       {best['subsample']}\")\n",
    "    print(f\"   colsample:       {best['colsample']}\")\n",
    "    print(f\"   R_MIN = {best['recall_min']:.3f} (Œî {delta:+.3f} vs baseline)\")\n",
    "    print(f\"   MCC   = {best['mcc']:.3f}\")\n",
    "    \n",
    "    if delta > 0:\n",
    "        print(f\"\\n   üéØ NEW BEST found! Beats baseline by {delta:+.3f}!\")\n",
    "    else:\n",
    "        print(f\"\\n   ‚ö†Ô∏è No combination beats the baseline. Best params are already optimal.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# üìä FULL ANALYSIS ‚Äî CONFUSION MATRIX & VISUALIZATIONS\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "#\n",
    "# –û–±—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å —Å –ª—É—á—à–∏–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º:\n",
    "# - Classification Report\n",
    "# - Confusion Matrix\n",
    "# - Predictions vs True (–≥—Ä–∞—Ñ–∏–∫)\n",
    "# - Feature Importance\n",
    "#\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üîç FULL ANALYSIS WITH BEST HYPERPARAMETERS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
    "# –û–ø—Ü–∏—è: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å baseline –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –≤ single_body_search)\n",
    "FORCE_BASELINE_PARAMS = True  # ‚Üê –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ False —á—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã hyperparameter search\n",
    "\n",
    "BASELINE_XGB_PARAMS = {\n",
    "    \"n_estimators\": 300,\n",
    "    \"max_depth\": 3,\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"early_stopping_rounds\": 50,\n",
    "}\n",
    "\n",
    "if FORCE_BASELINE_PARAMS:\n",
    "    BEST_PARAMS = BASELINE_XGB_PARAMS.copy()\n",
    "    print(\"‚ö†Ô∏è Using BASELINE XGB params (from single_body_search)\")\n",
    "elif best_so_far.get(\"params\"):\n",
    "    BEST_PARAMS = best_so_far[\"params\"]\n",
    "    print(\"‚úì Using params from hyperparameter search\")\n",
    "else:\n",
    "    BEST_PARAMS = BASELINE_XGB_PARAMS.copy()\n",
    "    print(\"‚ö†Ô∏è Fallback to BASELINE XGB params\")\n",
    "\n",
    "print(f\"\\nüå≥ Using hyperparameters: {BEST_PARAMS}\")\n",
    "print(f\"üåç Config: {config['name']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞\n",
    "from RESEARCH.labeling import create_balanced_labels\n",
    "from RESEARCH.features import build_full_features, get_feature_inventory, get_feature_columns\n",
    "from RESEARCH.astro_engine import calculate_aspects_for_dates\n",
    "from RESEARCH.model_training import (\n",
    "    train_xgb_model, \n",
    "    split_dataset,\n",
    "    prepare_xy,\n",
    "    tune_threshold,\n",
    ")\n",
    "from RESEARCH.visualization import (\n",
    "    plot_confusion_matrix,\n",
    "    plot_predictions,\n",
    "    plot_feature_importance,\n",
    ")\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìç –ü–µ—Ä–µ—Å—á—ë—Ç –∞—Å—Ç—Ä–æ-–¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ (—Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º config)\n",
    "print(f\"\\nüìç Recalculating astro data for config: {config['name']}\")\n",
    "print(f\"   coord_mode: {config['coord_mode']}\")\n",
    "print(f\"   exclude_bodies: {config['exclude_bodies']}\")\n",
    "\n",
    "df_bodies_final, geo_by_date, helio_by_date = calculate_bodies_for_dates_multi(\n",
    "    df_market[\"date\"], settings, coord_mode=config[\"coord_mode\"], progress=True\n",
    ")\n",
    "bodies_by_date_final = geo_by_date if geo_by_date else helio_by_date\n",
    "\n",
    "print(\"\\nüìê Calculating angles...\")\n",
    "angles_cache_final = precompute_angles_for_dates(bodies_by_date_final, progress=True)\n",
    "print(\"‚úì Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞—ë–º –º–µ—Ç–∫–∏ —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ì–∞—É—Å—Å–∞\n",
    "print(\"\\nüìç Creating labels...\")\n",
    "df_labels = create_balanced_labels(\n",
    "    df_market,\n",
    "    horizon=1,\n",
    "    move_share=0.5,\n",
    "    gauss_window=config[\"gauss_window\"],\n",
    "    gauss_std=config[\"gauss_std\"],\n",
    "    price_mode=\"raw\",\n",
    "    label_mode=\"balanced_detrended\",\n",
    ")\n",
    "print(f\"   Labels created: {len(df_labels)} rows\")\n",
    "print(f\"   Distribution: {df_labels['target'].value_counts().to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í—ã—á–∏—Å–ª—è–µ–º –∞—Å–ø–µ–∫—Ç—ã\n",
    "print(\"\\nüìê Calculating aspects...\")\n",
    "df_aspects = calculate_aspects_for_dates(\n",
    "    bodies_by_date_final,  # ‚Üê –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ—Å—á–∏—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "    settings,\n",
    "    orb_mult=config[\"orb_mult\"],\n",
    "    progress=True,\n",
    ")\n",
    "print(f\"   Aspects: {len(df_aspects)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í—ã—á–∏—Å–ª—è–µ–º —Ñ–∞–∑—ã –õ—É–Ω—ã –∏ —ç–ª–æ–Ω–≥–∞—Ü–∏–∏ –ø–ª–∞–Ω–µ—Ç (–∫–∞–∫ –≤ grid search!)\n",
    "print(\"\\nüåô Calculating moon phases...\")\n",
    "from RESEARCH.astro_engine import calculate_phases_for_dates\n",
    "df_phases = calculate_phases_for_dates(bodies_by_date_final, progress=True)\n",
    "print(f\"   Phases: {len(df_phases)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°—Ç—Ä–æ–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∏ (—Å —Ñ–∞–∑–∞–º–∏!)\n",
    "print(\"\\nüîß Building features...\")\n",
    "df_features = build_full_features(\n",
    "    df_bodies_final,  # ‚Üê –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ—Å—á–∏—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "    df_aspects,\n",
    "    df_phases=df_phases,  # ‚Üê –í–ê–ñ–ù–û: –¥–æ–±–∞–≤–ª—è–µ–º —Ñ–∞–∑—ã –∫–∞–∫ –≤ grid search!\n",
    "    exclude_bodies=config[\"exclude_bodies\"],\n",
    ")\n",
    "print(f\"   Features shape: {df_features.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –º–µ—Ç–∫–∞–º–∏\n",
    "from RESEARCH.features import merge_features_with_labels\n",
    "df_dataset = merge_features_with_labels(df_features, df_labels)\n",
    "print(f\"   Dataset shape: {df_dataset.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Val/Test split (time-based)\n",
    "print(\"\\nüìä Splitting data...\")\n",
    "train_df, val_df, test_df = split_dataset(df_dataset, train_ratio=0.7, val_ratio=0.15)\n",
    "print(f\"   Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "feature_cols = get_feature_columns(df_dataset)\n",
    "X_train, y_train = prepare_xy(train_df, feature_cols)\n",
    "X_val, y_val = prepare_xy(val_df, feature_cols)\n",
    "X_test, y_test = prepare_xy(test_df, feature_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üîç DIAGNOSTICS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"   Features count: {len(feature_cols)}\")\n",
    "print(f\"   Sample features: {feature_cols[:10]}...\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —Ñ–∞–∑—ã\n",
    "phase_features = [c for c in feature_cols if 'phase' in c.lower() or 'elongation' in c.lower() or 'lunar' in c.lower()]\n",
    "print(f\"   Phase/elongation features: {len(phase_features)}\")\n",
    "if phase_features:\n",
    "    print(f\"   ‚Üí {phase_features[:5]}...\")\n",
    "\n",
    "print(f\"\\n   BEST_PARAMS being used: {BEST_PARAMS}\")\n",
    "print(f\"   Config: {config['name']}\")\n",
    "print(f\"   exclude_bodies: {config['exclude_bodies']}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n",
    "print(\"\\nüå≥ Training XGBoost with best hyperparameters...\")\n",
    "model = train_xgb_model(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    feature_names=feature_cols,\n",
    "    n_classes=2,\n",
    "    device=device,\n",
    "    **BEST_PARAMS,\n",
    ")\n",
    "print(\"   ‚úì Model trained!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¢—é–Ω–∏–º –ø–æ—Ä–æ–≥\n",
    "print(\"\\nüéØ Tuning threshold...\")\n",
    "best_threshold, best_metric = tune_threshold(model, X_val, y_val, metric=\"recall_min\")\n",
    "print(f\"   Best threshold: {best_threshold:.3f}\")\n",
    "print(f\"   Best recall_min: {best_metric:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "# XGBBaseline uses internal model.model for probabilities\n",
    "X_test_scaled = model.scaler.transform(X_test)\n",
    "y_pred_proba = model.model.predict_proba(X_test_scaled)[:, 1]\n",
    "y_pred = (y_pred_proba >= best_threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 80)\n",
    "print(classification_report(y_test, y_pred, target_names=[\"DOWN\", \"UP\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üî¢ CONFUSION MATRIX\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"\\n         Predicted\")\n",
    "print(f\"         DOWN   UP\")\n",
    "print(f\"Actual DOWN  {cm[0,0]:4d}  {cm[0,1]:4d}\")\n",
    "print(f\"       UP    {cm[1,0]:4d}  {cm[1,1]:4d}\")\n",
    "\n",
    "# Recall –ø–æ –∫–ª–∞—Å—Å–∞–º\n",
    "recall_down = cm[0,0] / (cm[0,0] + cm[0,1]) if (cm[0,0] + cm[0,1]) > 0 else 0\n",
    "recall_up = cm[1,1] / (cm[1,0] + cm[1,1]) if (cm[1,0] + cm[1,1]) > 0 else 0\n",
    "recall_min = min(recall_down, recall_up)\n",
    "recall_gap = abs(recall_up - recall_down)\n",
    "\n",
    "# MCC\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nüìà Recall DOWN: {recall_down:.3f}\")\n",
    "print(f\"üìà Recall UP:   {recall_up:.3f}\")\n",
    "print(f\"üìà R_MIN:       {recall_min:.3f}\")\n",
    "print(f\"üìà Gap:         {recall_gap:.3f}\")\n",
    "print(f\"üìà MCC:         {mcc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confusion Matrix\n",
    "print(\"\\nüìä Plotting confusion matrix...\")\n",
    "plot_confusion_matrix(y_test, y_pred, label_names=[\"DOWN\", \"UP\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Predictions vs Actual\n",
    "print(\"\\nüìà Plotting predictions...\")\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º —Ü–µ–Ω—ã –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "test_df_plot = test_df.copy()\n",
    "if \"date\" in test_df_plot.columns:\n",
    "    test_df_plot[\"date\"] = pd.to_datetime(test_df_plot[\"date\"])\n",
    "else:\n",
    "    test_df_plot = test_df_plot.reset_index()\n",
    "test_df_plot = test_df_plot.merge(\n",
    "    df_market[[\"date\", \"close\"]].assign(date=lambda x: pd.to_datetime(x[\"date\"])), \n",
    "    on=\"date\", \n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_market\")\n",
    ")\n",
    "# Use market close if exists\n",
    "if \"close_market\" in test_df_plot.columns:\n",
    "    test_df_plot[\"close\"] = test_df_plot[\"close_market\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Price with predictions\n",
    "ax1 = axes[0]\n",
    "ax1.plot(test_df_plot[\"date\"], test_df_plot[\"close\"], 'k-', alpha=0.7, label=\"Price\")\n",
    "\n",
    "# Highlight predictions\n",
    "up_mask = y_pred == 1\n",
    "down_mask = y_pred == 0\n",
    "ax1.scatter(test_df_plot[\"date\"].values[up_mask], test_df_plot[\"close\"].values[up_mask], \n",
    "            c='green', alpha=0.5, s=20, label=\"Pred UP\")\n",
    "ax1.scatter(test_df_plot[\"date\"].values[down_mask], test_df_plot[\"close\"].values[down_mask], \n",
    "            c='red', alpha=0.5, s=20, label=\"Pred DOWN\")\n",
    "ax1.set_ylabel(\"Price\")\n",
    "ax1.set_title(\"Predictions on Test Data\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Accuracy over time (rolling)\n",
    "ax2 = axes[1]\n",
    "correct = (y_pred == y_test).astype(int)\n",
    "rolling_acc = pd.Series(correct).rolling(50, min_periods=10).mean()\n",
    "ax2.plot(test_df_plot[\"date\"], rolling_acc, 'b-', linewidth=2)\n",
    "ax2.axhline(y=0.5, color='r', linestyle='--', label=\"Random (50%)\")\n",
    "ax2.axhline(y=recall_min, color='g', linestyle='--', label=f\"Avg R_MIN ({recall_min:.1%})\")\n",
    "ax2.set_ylabel(\"Rolling Accuracy (50-day window)\")\n",
    "ax2.set_xlabel(\"Date\")\n",
    "ax2.set_title(\"Model Accuracy Over Time\")\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫: Predicted vs True (—Å —Ü–≤–µ—Ç–Ω–æ–π –∑–∞–ª–∏–≤–∫–æ–π)\n",
    "print(\"\\nüìä Predicted vs True Labels (shaded)...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "\n",
    "dates = test_df_plot[\"date\"].values\n",
    "close = test_df_plot[\"close\"].values\n",
    "\n",
    "# Helper function for continuous shading\n",
    "def shade_labels(ax, dates, labels, close, title):\n",
    "    ax.plot(dates, close, color=\"black\", linewidth=1.2, label=\"Price\")\n",
    "    \n",
    "    up_added = False\n",
    "    down_added = False\n",
    "    \n",
    "    for i in range(len(dates) - 1):\n",
    "        label = labels[i]\n",
    "        if label == 1:\n",
    "            ax.axvspan(dates[i], dates[i+1], color=\"green\", alpha=0.2, \n",
    "                      label=\"UP\" if not up_added else None)\n",
    "            up_added = True\n",
    "        else:\n",
    "            ax.axvspan(dates[i], dates[i+1], color=\"red\", alpha=0.2,\n",
    "                      label=\"DOWN\" if not down_added else None)\n",
    "            down_added = True\n",
    "    \n",
    "    ax.set_ylabel(\"Price\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 1: Predictions\n",
    "shade_labels(axes[0], dates, y_pred, close, \"PREDICTED Labels\")\n",
    "\n",
    "# Plot 2: True labels  \n",
    "shade_labels(axes[1], dates, y_test, close, \"TRUE Labels\")\n",
    "axes[1].set_xlabel(\"Date\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π\n",
    "accuracy = (y_pred == y_test).mean()\n",
    "print(f\"\\nüìà Test Accuracy: {accuracy:.1%}\")\n",
    "print(f\"   Correct: {(y_pred == y_test).sum()} / {len(y_test)}\")\n",
    "print(f\"   R_MIN: {recall_min:.3f}\")\n",
    "print(f\"   MCC:   {mcc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "print(\"\\nüìä Feature Importance (Top 20)...\")\n",
    "from RESEARCH.model_training import get_feature_importance\n",
    "importance_df = get_feature_importance(model, feature_cols, top_n=20)\n",
    "plot_feature_importance(importance_df, title=\"Top 20 Features by Importance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# üíæ SAVE FINAL MODEL\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "from joblib import dump\n",
    "\n",
    "artifact_dir = PROJECT_ROOT / \"models_artifacts\"\n",
    "artifact_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "artifact = {\n",
    "    \"model\": model.model,\n",
    "    \"scaler\": model.scaler,\n",
    "    \"feature_names\": feature_cols,\n",
    "    \"threshold\": best_threshold,\n",
    "    \"config\": {\n",
    "        \"astro_config\": config,\n",
    "        \"xgb_params\": BEST_PARAMS,\n",
    "    },\n",
    "    \"metrics\": {\n",
    "        \"recall_up\": recall_up,\n",
    "        \"recall_down\": recall_down,\n",
    "        \"recall_min\": recall_min,\n",
    "        \"recall_gap\": recall_gap,\n",
    "        \"mcc\": mcc,\n",
    "    }\n",
    "}\n",
    "\n",
    "model_name = f\"xgb_astro_{ACTIVE_CONFIG.lower()}.joblib\"\n",
    "out_path = artifact_dir / model_name\n",
    "dump(artifact, out_path)\n",
    "print(f\"\\nüíæ Model saved: {out_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ ANALYSIS COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\"\"\n",
    "üìä Final Results:\n",
    "   Config:     {config['name']}\n",
    "   R_MIN:      {recall_min:.3f}\n",
    "   R_UP:       {recall_up:.3f}\n",
    "   R_DOWN:     {recall_down:.3f}\n",
    "   Gap:        {recall_gap:.3f}\n",
    "   MCC:        {mcc:.3f}\n",
    "   Threshold:  {best_threshold:.3f}\n",
    "   \n",
    "üå≥ Best XGBoost Params:\n",
    "   {BEST_PARAMS}\n",
    "   \n",
    "üíæ Model saved to: {out_path}\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
