{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0f92afc",
   "metadata": {},
   "source": [
    "# XGBoost Hyperparameter Tuning (Phase 2)\n",
    "\n",
    "Takes **Top 30** configs from Phase 1 and tunes XGBoost hyperparams.\n",
    "\n",
    "**OPTIMIZED**: Pre-computes bodies, aspects, phases, labels ONCE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d186b02",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3d7a082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV, PredefinedSplit\n",
    "from sklearn.metrics import make_scorer, recall_score\n",
    "from pathlib import Path\n",
    "import ast\n",
    "import warnings\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d19bd1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', category=UserWarning, module='xgboost')\n",
    "warnings.filterwarnings('ignore', message='Falling back to prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6903cacd",
   "metadata": {},
   "source": [
    "## Project Root & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "022d3752",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = Path(os.getcwd())\n",
    "if (current_dir / \"RESEARCH\").exists():\n",
    "    PROJECT_ROOT = current_dir\n",
    "elif (current_dir.parent / \"RESEARCH\").exists():\n",
    "    PROJECT_ROOT = current_dir.parent\n",
    "else:\n",
    "    PROJECT_ROOT = current_dir\n",
    "\n",
    "sys.path.append(str(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "48060ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RESEARCH.data_loader import load_market_data\n",
    "from RESEARCH.labeling import create_balanced_labels\n",
    "from RESEARCH.astro_engine import (\n",
    "    init_ephemeris,\n",
    "    calculate_bodies_for_dates_multi,\n",
    "    calculate_phases_for_dates,\n",
    ")\n",
    "from RESEARCH.astro.aspects import (\n",
    "    precompute_angles_for_dates,\n",
    "    calculate_aspects_from_cache,\n",
    ")\n",
    "from RESEARCH.numba_utils import warmup_jit, check_numba_available\n",
    "from RESEARCH.features import build_full_features, merge_features_with_labels, get_feature_columns\n",
    "from RESEARCH.model_training import split_dataset, prepare_xy, check_cuda_available, calc_metrics\n",
    "from sklearn.utils.class_weight import compute_sample_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dc7585",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1a176f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPORTS_DIR = PROJECT_ROOT / \"RESEARCH\" / \"reports\"\n",
    "INPUT_CSV = REPORTS_DIR / \"grid_search_partial.csv\"\n",
    "OUTPUT_CSV = REPORTS_DIR / \"xgb_tuning_results.csv\"\n",
    "\n",
    "if not INPUT_CSV.exists():\n",
    "    csv_files = sorted(REPORTS_DIR.glob(\"grid_search_*.csv\"))\n",
    "    if csv_files:\n",
    "        INPUT_CSV = csv_files[-1]\n",
    "        print(f\"‚úÖ Using: {INPUT_CSV.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "64c7fe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_MODE = True  # True = 2 candidates, 2 iter | False = 30 candidates, 50 iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cac93eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM_DIST = {\n",
    "    'n_estimators': [100, 200, 300, 500, 700, 1000],\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8, 10],\n",
    "    'learning_rate': [0.005, 0.01, 0.03, 0.05, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2, 0.5],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'scale_pos_weight': [1], \n",
    "}\n",
    "N_ITER = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d6156e81",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "use_cuda_check, _ = check_cuda_available()\n",
    "N_JOBS = 1 if use_cuda_check else 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4a92dfd7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def parse_list_string(s):\n",
    "    if pd.isna(s) or s == 'None':\n",
    "        return []\n",
    "    try:\n",
    "        return ast.literal_eval(s)\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "921882da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom scorer: min(recall_up, recall_down)\n",
    "# We want BOTH classes to be predicted well, not just one.\n",
    "def recall_min_score(y_true, y_pred):\n",
    "    recalls = recall_score(y_true, y_pred, labels=[0, 1], average=None, zero_division=0)\n",
    "    return min(recalls)  # Return the WORST recall\n",
    "\n",
    "RECALL_MIN_SCORER = make_scorer(recall_min_score, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62821869",
   "metadata": {},
   "source": [
    "## Load Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8a02c4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÜ Loaded 2 candidates\n"
     ]
    }
   ],
   "source": [
    "top_candidates = pd.DataFrame()\n",
    "\n",
    "if not os.path.exists(INPUT_CSV):\n",
    "    print(f\"‚ùå File not found: {INPUT_CSV}\")\n",
    "else:\n",
    "    df_results = pd.read_csv(INPUT_CSV)\n",
    "    df_results = df_results.sort_values('recall_min', ascending=False)\n",
    "    limit = 30 if not TEST_MODE else 2\n",
    "    top_candidates = df_results.head(limit).copy()\n",
    "    print(f\"üèÜ Loaded {len(top_candidates)} candidates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681fd301",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8a534377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è Device: cuda\n"
     ]
    }
   ],
   "source": [
    "settings = init_ephemeris()\n",
    "use_cuda, device = check_cuda_available()\n",
    "print(f\"üñ•Ô∏è Device: {device}\")\n",
    "\n",
    "if check_numba_available():\n",
    "    warmup_jit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6c3255df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5677 rows from DB for subject=btc\n",
      "Date range: 2010-07-18 -> 2026-01-31\n",
      "üìà Market: 3014 days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rut/ostrofun/RESEARCH/data_loader.py:55: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn, params=params)\n"
     ]
    }
   ],
   "source": [
    "df_market = load_market_data()\n",
    "df_market = df_market[df_market['date'] >= '2017-11-01'].reset_index(drop=True)\n",
    "print(f\"üìà Market: {len(df_market)} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8e7e03",
   "metadata": {},
   "source": [
    "## üöÄ PRE-COMPUTE EVERYTHING\n",
    "\n",
    "Calculate ONCE, reuse many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1d486572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Unique coord modes: ['both', 'geo']\n",
      "üì¶ Unique orb mults: [0.25]\n",
      "üì¶ Unique gauss params: 2 combinations\n"
     ]
    }
   ],
   "source": [
    "# Extract unique values from candidates\n",
    "unique_coords = top_candidates['coord_mode'].unique().tolist()\n",
    "unique_orbs = top_candidates['orb_mult'].unique().tolist()\n",
    "unique_gauss = top_candidates[['gauss_window', 'gauss_std']].drop_duplicates().values.tolist()\n",
    "\n",
    "print(f\"üì¶ Unique coord modes: {unique_coords}\")\n",
    "print(f\"üì¶ Unique orb mults: {unique_orbs}\")\n",
    "print(f\"üì¶ Unique gauss params: {len(unique_gauss)} combinations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b81b195",
   "metadata": {},
   "source": [
    "### 1. Pre-compute BODIES + ANGLES + PHASES (per coord_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8de497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Pre-computing bodies, angles, phases...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Coords:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "body_cache = {}  # coord_mode -> (df_bodies, geo_by_date, helio_by_date, angles_cache)\n",
    "phase_cache = {}  # coord_mode -> df_phases\n",
    "\n",
    "print(\"‚è≥ Pre-computing bodies, angles, phases...\")\n",
    "\n",
    "for coord in tqdm(unique_coords, desc=\"Coords\"):\n",
    "    # Bodies & Angles\n",
    "    df_bodies, geo_by_date, helio_by_date = calculate_bodies_for_dates_multi(\n",
    "        df_market['date'], settings, coord_mode=coord, progress=False\n",
    "    )\n",
    "    # Filter Chiron\n",
    "    if 'Chiron' in df_bodies['body'].values:\n",
    "        df_bodies = df_bodies[df_bodies['body'] != 'Chiron']\n",
    "    for d in geo_by_date:\n",
    "        geo_by_date[d] = [b for b in geo_by_date[d] if b.body != 'Chiron']\n",
    "    if helio_by_date:\n",
    "        for d in helio_by_date:\n",
    "            helio_by_date[d] = [b for b in helio_by_date[d] if b.body != 'Chiron']\n",
    "    \n",
    "    angles_cache = precompute_angles_for_dates(geo_by_date, progress=False)\n",
    "    body_cache[coord] = (df_bodies, geo_by_date, helio_by_date, angles_cache)\n",
    "    \n",
    "    # Phases (only depends on geo_by_date)\n",
    "    df_phases = calculate_phases_for_dates(geo_by_date, progress=False)\n",
    "    phase_cache[coord] = df_phases\n",
    "\n",
    "print(f\"‚úÖ Bodies/Angles/Phases cached for {len(unique_coords)} coord modes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49092e0",
   "metadata": {},
   "source": [
    "### 2. Pre-compute ASPECTS (per coord_mode + orb_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88a78b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_cache = {}  # (coord_mode, orb_mult) -> df_aspects\n",
    "\n",
    "print(\"‚è≥ Pre-computing aspects...\")\n",
    "\n",
    "for coord in unique_coords:\n",
    "    _, geo_by_date, _, angles_cache = body_cache[coord]\n",
    "    for orb in tqdm(unique_orbs, desc=f\"Orbs ({coord})\", leave=False):\n",
    "        key = (coord, orb)\n",
    "        df_aspects = calculate_aspects_from_cache(angles_cache, settings, orb_mult=orb, progress=False)\n",
    "        aspect_cache[key] = df_aspects\n",
    "\n",
    "print(f\"‚úÖ Aspects cached: {len(aspect_cache)} combinations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6df402d",
   "metadata": {},
   "source": [
    "### 3. Pre-compute LABELS (per gauss_window + gauss_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a0edb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cache = {}  # (gauss_window, gauss_std) -> df_labels\n",
    "\n",
    "print(\"‚è≥ Pre-computing labels...\")\n",
    "\n",
    "for gw, gs in tqdm(unique_gauss, desc=\"Gauss params\"):\n",
    "    key = (int(gw), float(gs))\n",
    "    df_labels = create_balanced_labels(\n",
    "        df_market, horizon=1, move_share=0.5, \n",
    "        gauss_window=key[0], gauss_std=key[1], \n",
    "        price_mode='raw', label_mode='balanced_detrended',\n",
    "        verbose=False\n",
    "    )\n",
    "    label_cache[key] = df_labels\n",
    "\n",
    "print(f\"‚úÖ Labels cached: {len(label_cache)} combinations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f09f1ec",
   "metadata": {},
   "source": [
    "## Tuning Loop (FAST - only assembly + training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39528dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = []\n",
    "\n",
    "print(\"\\nüöÄ STARTING TUNING LOOP (pre-computed data)...\")\n",
    "\n",
    "for i, (_, row) in enumerate(top_candidates.iterrows()):\n",
    "    coord = row['coord_mode']\n",
    "    gw = int(row['gauss_window'])\n",
    "    gs = float(row['gauss_std'])\n",
    "    orb = row['orb_mult']\n",
    "    excl_list = parse_list_string(row['exclude_bodies'])\n",
    "    \n",
    "    print(f\"\\n‚ö° [{i+1}/{len(top_candidates)}] Coord={coord} GW={gw} Orb={orb}\")\n",
    "    \n",
    "    # --- FAST: Get from cache ---\n",
    "    df_bodies, geo_by_date, helio_by_date, _ = body_cache[coord]\n",
    "    df_phases = phase_cache[coord]\n",
    "    df_aspects = aspect_cache[(coord, orb)]\n",
    "    df_labels = label_cache[(gw, gs)]\n",
    "    \n",
    "    # --- Build Features (fast merge) ---\n",
    "    df_features = build_full_features(df_bodies, df_aspects, df_phases, exclude_bodies=excl_list)\n",
    "    df_dataset = merge_features_with_labels(df_features, df_labels, verbose=False)\n",
    "    \n",
    "    # --- Split ---\n",
    "    train_df, val_df, test_df = split_dataset(df_dataset, train_ratio=0.7, val_ratio=0.15)\n",
    "    feature_cols = get_feature_columns(df_dataset)\n",
    "    X_train, y_train = prepare_xy(train_df, feature_cols)\n",
    "    X_val, y_val = prepare_xy(val_df, feature_cols)\n",
    "    \n",
    "    # --- PredefinedSplit ---\n",
    "    X_full = np.concatenate([X_train, X_val], axis=0)\n",
    "    y_full = np.concatenate([y_train, y_val], axis=0)\n",
    "    test_fold = np.concatenate([np.full(len(X_train), -1), np.full(len(X_val), 0)])\n",
    "    ps = PredefinedSplit(test_fold)\n",
    "    \n",
    "    # --- RandomizedSearchCV ---\n",
    "    iter_limit = N_ITER if not TEST_MODE else 2\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        objective='binary:logistic', eval_metric='logloss',\n",
    "        device=device if device == 'cuda' else 'cpu', \n",
    "        tree_method='hist' if device == 'cuda' else 'auto'\n",
    "    )\n",
    "    \n",
    "    search = RandomizedSearchCV(\n",
    "        xgb_model, PARAM_DIST, n_iter=iter_limit, \n",
    "        scoring=RECALL_MIN_SCORER,  # Custom: min(recall_up, recall_down)\n",
    "        cv=ps, n_jobs=N_JOBS, verbose=0, \n",
    "        random_state=42 + i,  # Different seed for each candidate = explore different HP combos\n",
    "        refit=False\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        search.fit(X_full, y_full)\n",
    "        best_params = search.best_params_\n",
    "        \n",
    "        # Retrain on X_train only\n",
    "        final_model = xgb.XGBClassifier(\n",
    "            objective='binary:logistic', eval_metric='logloss',\n",
    "            device=device if device == 'cuda' else 'cpu', \n",
    "            tree_method='hist' if device == 'cuda' else 'auto',\n",
    "            **best_params\n",
    "        )\n",
    "        final_model.fit(X_train, y_train, sample_weight=compute_sample_weight('balanced', y_train))\n",
    "        y_pred_val = final_model.predict(X_val)\n",
    "        metrics = calc_metrics(y_val, y_pred_val, labels=[0, 1])\n",
    "        \n",
    "        print(f\"   üìâ BASELINE: R_MIN={row['recall_min']:.3f} MCC={row['mcc']:.3f}\")\n",
    "        print(f\"   üìà TUNED:    R_MIN={metrics['recall_min']:.3f} MCC={metrics['mcc']:.3f}\")\n",
    "        \n",
    "        record = {\n",
    "            'rank': i + 1, 'coord_mode': coord, 'gauss_window': gw, 'gauss_std': gs,\n",
    "            'orb_mult': orb, 'exclude_bodies': row['exclude_bodies'],\n",
    "            'baseline_recall_min': row['recall_min'], 'baseline_mcc': row['mcc'],\n",
    "            'tuned_recall_min': metrics['recall_min'], 'tuned_mcc': metrics['mcc'],\n",
    "            'best_params': str(best_params),\n",
    "        }\n",
    "        final_results.append(record)\n",
    "        pd.DataFrame(final_results).to_csv(OUTPUT_CSV, index=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177ee82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(f\"üíæ DONE. Results: {OUTPUT_CSV}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
