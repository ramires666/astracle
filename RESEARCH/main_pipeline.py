# %% [markdown]
# # Astro Trading Pipeline - Modular Research Notebook
# 
# This is the **central orchestrating notebook** that imports and runs all modules.
# 
# ## Pipeline Steps:
# 1. Setup & Config
# 2. Load Market Data (from PostgreSQL)
# 3. Visualize Price
# 4. Create Labels (balanced UP/DOWN)
# 5. Compute Astro Data
# 6. Build Features
# 7. Train Model
# 8. Evaluate & Visualize
# 9. (Optional) Grid Search
# 10. Save Model

# %% [markdown]
# ## 0. Environment Check

# %%
# Check dependencies
import importlib.util as iu

required = ["xgboost", "sklearn", "matplotlib", "seaborn", "tqdm", "pyarrow", "psycopg2", "swisseph"]
missing = [pkg for pkg in required if iu.find_spec(pkg) is None]

if missing:
    print("Missing packages:", ", ".join(missing))
    print("Install with: conda install -c conda-forge " + " ".join(missing))
else:
    print("âœ“ All dependencies found")

# %% [markdown]
# ## 1. Setup & Configuration

# %%
# Import RESEARCH modules
import sys
from pathlib import Path

# Find project root and add to path
PROJECT_ROOT = Path.cwd().resolve()
if not (PROJECT_ROOT / "RESEARCH").exists():
    for parent in PROJECT_ROOT.parents:
        if (parent / "RESEARCH").exists():
            PROJECT_ROOT = parent
            break

if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

print(f"PROJECT_ROOT: {PROJECT_ROOT}")

# %%
# Import all modules
from RESEARCH.config import cfg, PROJECT_ROOT
from RESEARCH.data_loader import load_market_data, get_latest_date, get_data_paths
from RESEARCH.labeling import create_balanced_labels, gaussian_smooth_centered
from RESEARCH.astro_engine import (
    init_ephemeris,
    calculate_bodies_for_dates,
    calculate_aspects_for_dates,
    calculate_transits_for_dates,
    get_natal_bodies,
)
from RESEARCH.features import (
    build_full_features,
    merge_features_with_labels,
    get_feature_columns,
    get_feature_inventory,
)
from RESEARCH.model_training import (
    split_dataset,
    prepare_xy,
    train_xgb_model,
    tune_threshold,
    predict_with_threshold,
    evaluate_model,
    get_feature_importance,
    check_cuda_available,
)
from RESEARCH.visualization import (
    plot_price_distribution,
    plot_class_distribution,
    plot_price_with_labels,
    plot_last_n_days,
    plot_confusion_matrix,
    plot_feature_importance,
    plot_predictions,
)

print("âœ“ All RESEARCH modules imported")

# %%
# Show configuration
print(f"Active subject: {cfg.active_subject_id}")
print(f"Data root: {cfg.data_root}")
print(f"DB URL configured: {bool(cfg.db_url)}")
print(f"Label config: {cfg.get_label_config()}")

# %% [markdown]
# ## 2. Load Market Data

# %%
# Load market data from database
df_market = load_market_data()

# Optional: filter by start date
DATA_START = "2017-11-01"
df_market = df_market[df_market["date"] >= DATA_START].reset_index(drop=True)

print(f"\nMarket data: {len(df_market)} rows")
print(f"Date range: {df_market['date'].min().date()} â†’ {df_market['date'].max().date()}")
df_market.head()

# %% [markdown]
# ## 3. Visualize Price

# %%
# Price and return distribution
plot_price_distribution(df_market, price_mode="log")

# %% [markdown]
# ## 4. Create Labels

# %%
# Configuration for labeling
LABEL_CONFIG = {
    "horizon": 1,           # Prediction horizon (days)
    "move_share": 0.5,      # Total share of samples to keep
    "gauss_window": 201,    # Gaussian window for detrending (odd)
    "gauss_std": 50.0,      # Gaussian std
    "price_mode": "raw",    # 'raw' or 'log'
    "label_mode": "balanced_detrended",
}

# Create balanced labels
df_labels = create_balanced_labels(
    df_market,
    horizon=LABEL_CONFIG["horizon"],
    move_share=LABEL_CONFIG["move_share"],
    gauss_window=LABEL_CONFIG["gauss_window"],
    gauss_std=LABEL_CONFIG["gauss_std"],
    price_mode=LABEL_CONFIG["price_mode"],
    label_mode=LABEL_CONFIG["label_mode"],
)

df_labels.head()

# %%
# Class distribution
plot_class_distribution(df_labels)

# %%
# Price with labels
plot_price_with_labels(df_market, df_labels, price_mode="raw")

# %%
# Last 30 days
plot_last_n_days(df_market, df_labels, n_days=30)

# %% [markdown]
# ## 5. Compute Astro Data

# %%
# Initialize ephemeris
settings = init_ephemeris()
print(f"Bodies: {[b.name for b in settings.bodies]}")
print(f"Aspects: {[a.name for a in settings.aspects]}")

# %%
# Calculate body positions for all dates
# (This is fast - no need to cache)
df_bodies, bodies_by_date = calculate_bodies_for_dates(
    df_market["date"],
    settings,
    progress=True,
)

print(f"\nBodies calculated: {len(df_bodies)} rows")
df_bodies.head()

# %%
# Calculate aspects
ORB_MULTIPLIER = 1.0  # Adjust orbs (1.0 = default)

df_aspects = calculate_aspects_for_dates(
    bodies_by_date,
    settings,
    orb_mult=ORB_MULTIPLIER,
    progress=True,
)

print(f"\nAspects: {len(df_aspects)} rows")
df_aspects.head()

# %% [markdown]
# ## 6. Build Features

# %%
# Build feature matrix
df_features = build_full_features(
    df_bodies,
    df_aspects,
    df_transits=None,  # Add transit aspects if needed
    include_pair_aspects=True,
    include_transit_aspects=False,
    exclude_bodies=None,  # List of body names to exclude
)

print(f"Features shape: {df_features.shape}")
df_features.head()

# %%
# Merge with labels
df_dataset = merge_features_with_labels(df_features, df_labels)
print(f"\nDataset shape: {df_dataset.shape}")

# %%
# Feature inventory
feature_inventory = get_feature_inventory(df_dataset)
print("\nFeature groups:")
print(feature_inventory.groupby("group").size())

# %% [markdown]
# ## 7. Train Model

# %%
# Check CUDA availability
use_cuda, device = check_cuda_available()
print(f"Using device: {device}")

# %%
# Split dataset (time-based)
train_df, val_df, test_df = split_dataset(df_dataset, train_ratio=0.7, val_ratio=0.15)

print(f"Train: {train_df['date'].min().date()} â†’ {train_df['date'].max().date()}")
print(f"Val:   {val_df['date'].min().date()} â†’ {val_df['date'].max().date()}")
print(f"Test:  {test_df['date'].min().date()} â†’ {test_df['date'].max().date()}")

# %%
# Prepare X, y
feature_cols = get_feature_columns(df_dataset)
X_train, y_train = prepare_xy(train_df, feature_cols)
X_val, y_val = prepare_xy(val_df, feature_cols)
X_test, y_test = prepare_xy(test_df, feature_cols)

print(f"X_train: {X_train.shape}, y_train: {y_train.shape}")
print(f"X_val:   {X_val.shape}, y_val:   {y_val.shape}")
print(f"X_test:  {X_test.shape}, y_test:  {y_test.shape}")

# %%
# Train XGBoost model
MODEL_PARAMS = {
    "n_estimators": 500,
    "max_depth": 6,
    "learning_rate": 0.03,
    "subsample": 0.8,
    "colsample_bytree": 0.8,
}

model = train_xgb_model(
    X_train, y_train,
    X_val, y_val,
    feature_cols,
    n_classes=2,
    device=device,
    **MODEL_PARAMS,
)

# %%
# Tune threshold on validation set
# Ğ¢Ğ•ĞŸĞ•Ğ Ğ¬ Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ recall_min (ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ…ÑƒĞ´ÑˆĞµĞ³Ğ¾ ĞºĞ»Ğ°ÑÑĞ°)
best_threshold, best_score = tune_threshold(model, X_val, y_val)  # metric="recall_min" Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ

# %% [markdown]
# ## 8. Evaluate & Visualize

# %%
# Predict on test set
y_pred = predict_with_threshold(model, X_test, threshold=best_threshold)

# Evaluate
results = evaluate_model(y_test, y_pred, label_names=["DOWN", "UP"])

# %%
# Confusion matrix
plot_confusion_matrix(y_test, y_pred)

# %%
# Feature importance
imp_df = get_feature_importance(model, feature_cols, top_n=20)
plot_feature_importance(imp_df)

# %%
# Predictions on test set
# Ğ¢ĞµĞ¿ĞµÑ€ÑŒ df_dataset ÑƒĞ¶Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ²ÑĞµ Ğ´Ğ½Ğ¸ Ñ forward-filled Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸
# ĞÑƒĞ¶Ğ½Ğ¾ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ close Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
import pandas as pd  # ĞĞ° ÑĞ»ÑƒÑ‡Ğ°Ğ¹ ĞµÑĞ»Ğ¸ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ÑÑ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾

test_df_plot = test_df.copy()
test_df_plot["date"] = pd.to_datetime(test_df_plot["date"])
test_df_plot = test_df_plot.merge(
    df_market[["date", "close"]].assign(date=lambda x: pd.to_datetime(x["date"])), 
    on="date", 
    how="left"
)

plot_predictions(test_df_plot, y_pred, y_true=y_test, price_mode="log")

# %% [markdown]
# ## 9. (Optional) Grid Search
# 
# Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ğ¾Ğ´Ğ¸Ğ½ Ğ¸Ğ· Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² Ğ½Ğ¸Ğ¶Ğµ:
# - **run_grid_search** â€” Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº (coord + gauss + orb)
# - **run_full_grid_search** â€” Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ body ablation
# - **run_body_ablation_search** â€” Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ablation Ñ‚ĞµĞ»

# %%
# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ DataFrame (Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ²Ğ¸Ğ´ĞµÑ‚ÑŒ Ğ²ÑĞµ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸)
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.expand_frame_repr', False)
pd.set_option('display.max_colwidth', None)

# %%
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”§ ĞĞĞ¡Ğ¢Ğ ĞĞ™ĞšĞ˜ GRID SEARCH â€” Ğ’Ğ¡Ğ• ĞŸĞĞ ĞĞœĞ•Ğ¢Ğ Ğ« Ğ’ ĞĞ”ĞĞĞœ ĞœĞ•Ğ¡Ğ¢Ğ•
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# ĞŸÑ€Ğ¾ÑÑ‚Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ¸Ğ¶Ğµ Ğ¸ Ğ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚Ğµ Grid Search!
# Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ±ĞµÑ€Ñ‘Ñ‚ Ğ’Ğ¡Ğ• ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².
#
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from RESEARCH.grid_search import GridSearchConfig, run_grid_search

GRID_CONFIG = GridSearchConfig(
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸŒ Ğ¡Ğ˜Ğ¡Ğ¢Ğ•ĞœĞ ĞšĞĞĞ Ğ”Ğ˜ĞĞĞ¢ (Ğ¾Ñ‚ĞºÑƒĞ´Ğ° ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¸Ğ¼ Ğ½Ğ° Ğ¿Ğ»Ğ°Ğ½ĞµÑ‚Ñ‹)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 
    # "geo"   = Ğ—ĞµĞ¼Ğ»Ñ Ğ² Ñ†ĞµĞ½Ñ‚Ñ€Ğµ (ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ°ÑÑ‚Ñ€Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ´Ğ¸Ğ¼ Ñ Ğ—ĞµĞ¼Ğ»Ğ¸)
    # "helio" = Ğ¡Ğ¾Ğ»Ğ½Ñ†Ğµ Ğ² Ñ†ĞµĞ½Ñ‚Ñ€Ğµ (Ğ½Ğ°ÑƒÑ‡Ğ½Ğ°Ñ Ñ‚Ğ¾Ñ‡ĞºĞ° Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ€Ğ±Ğ¸Ñ‚Ñ‹)
    # "both"  = ĞĞ‘Ğ• Ğ¡Ğ˜Ğ¡Ğ¢Ğ•ĞœĞ« Ğ¡Ğ ĞĞ—Ğ£ (ÑƒĞ´Ğ²Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²!)
    #
    # Ğ¡Ğ¾Ğ²ĞµÑ‚: Ğ½Ğ°Ñ‡Ğ½Ğ¸Ñ‚Ğµ Ñ ["geo"], Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ ["geo", "helio", "both"]
    #        Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ğ»ÑƒÑ‡ÑˆÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ
    #
    coord_modes=["geo"],  # â† Ğ˜Ğ—ĞœĞ•ĞĞ˜Ğ¢Ğ•: ["geo"], ["helio"], ["both"], Ğ¸Ğ»Ğ¸ Ğ²ÑĞµ ÑÑ€Ğ°Ğ·Ñƒ
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸ”® ĞĞ Ğ‘Ğ˜Ğ¡Ğ« ĞĞ¡ĞŸĞ•ĞšĞ¢ĞĞ’ (Ğ½Ğ°ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¿Ğ»Ğ°Ğ½ĞµÑ‚Ñ‹ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ±Ñ‹Ñ‚ÑŒ Ğ² Ğ°ÑĞ¿ĞµĞºÑ‚Ğµ)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    #
    # ĞÑ€Ğ±Ğ¸Ñ = Ğ´Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ¼Ğ¾Ğµ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ³Ğ»Ğ° Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°.
    # ĞĞ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ = 0Â°, Ğ½Ğ¾ ĞµÑĞ»Ğ¸ Ğ¾Ñ€Ğ±Ğ¸Ñ 8Â°, Ñ‚Ğ¾ 0Â°Â±8Â° ÑÑ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ÑÑ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸ĞµĞ¼.
    #
    # 0.5 = Ğ£Ğ—ĞšĞ˜Ğ• Ğ¾Ñ€Ğ±Ğ¸ÑÑ‹ (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ñ‡ĞµĞ½ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹, Ğ¼ĞµĞ½ÑŒÑˆĞµ ÑˆÑƒĞ¼Ğ°)
    # 1.0 = Ğ¡Ğ¢ĞĞĞ”ĞĞ Ğ¢ĞĞ«Ğ• Ğ¾Ñ€Ğ±Ğ¸ÑÑ‹ (ĞºĞ°Ğº Ğ² ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ°ÑÑ‚Ñ€Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸)
    # 1.5 = Ğ¨Ğ˜Ğ ĞĞšĞ˜Ğ• Ğ¾Ñ€Ğ±Ğ¸ÑÑ‹ (Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ°Ğ¹Ğ´Ñ‘Ñ‚ÑÑ, Ğ½Ğ¾ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ÑˆÑƒĞ¼Ğ°)
    #
    orb_multipliers=[0.25,0.75, 1.5],  # â† Ğ˜Ğ—ĞœĞ•ĞĞ˜Ğ¢Ğ•: Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ [0.5], [1.0], [0.5, 1.0, 1.5]
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸ“ˆ ĞĞšĞĞ Ğ¡Ğ“Ğ›ĞĞ–Ğ˜Ğ’ĞĞĞ˜Ğ¯ Ğ“ĞĞ£Ğ¡Ğ¡Ğ (ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ½ĞµĞ¹ "Ğ²Ğ¸Ğ´Ğ¸Ñ‚" Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    #
    # Ğ­Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¾ĞºĞ½Ğ° Ğ² Ğ”ĞĞ¯Ğ¥ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ "Ñ‚Ñ€ĞµĞ½Ğ´ UP Ğ¸Ğ»Ğ¸ DOWN".
    # Ğ§ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ¾ĞºĞ½Ğ¾ â€” Ñ‚ĞµĞ¼ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞ½Ğ´Ñ‹ Ğ»Ğ¾Ğ²Ğ¸Ğ¼.
    #
    #  51 = ~2.5 Ğ¼ĞµÑÑÑ†Ğ° (Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğº ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼)
    # 101 = ~5 Ğ¼ĞµÑÑÑ†ĞµĞ² 
    # 151 = ~7.5 Ğ¼ĞµÑÑÑ†ĞµĞ²
    # 201 = ~10 Ğ¼ĞµÑÑÑ†ĞµĞ² (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ñ‚Ñ€ĞµĞ½Ğ´Ñ‹, Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµĞ»ĞºĞ¸Ğµ)
    #
    gauss_windows=[51,101, 151, 201],  # â† Ğ˜Ğ—ĞœĞ•ĞĞ˜Ğ¢Ğ•: Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ [101], [51, 101, 201]
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸ”” Ğ¨Ğ˜Ğ Ğ˜ĞĞ ĞšĞĞ›ĞĞšĞĞ›Ğ Ğ“ĞĞ£Ğ¡Ğ¡Ğ (Ğ½Ğ°ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ "Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ°" Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° UP/DOWN)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    #
    # Ğ­Ñ‚Ğ¾ "ÑˆĞ¸Ñ€Ğ¸Ğ½Ğ°" ÑĞ³Ğ»Ğ°Ğ¶Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞ³Ğ¾ ĞºĞ¾Ğ»Ğ¾ĞºĞ¾Ğ»Ğ°.
    # Ğ§ĞµĞ¼ Ğ¼ĞµĞ½ÑŒÑˆĞµ â€” Ñ‚ĞµĞ¼ Ñ€ĞµĞ·Ñ‡Ğµ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ UP Ğ¸ DOWN.
    # Ğ§ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ â€” Ñ‚ĞµĞ¼ Ğ¿Ğ»Ğ°Ğ²Ğ½ĞµĞµ, Ğ½Ğ¾ Ğ¼ĞµĞ½ĞµĞµ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğº Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼.
    #
    # 20.0 = Ğ£Ğ—ĞšĞ˜Ğ™ ĞºĞ¾Ğ»Ğ¾ĞºĞ¾Ğ» (Ñ€ĞµĞ·ĞºĞ¸Ğµ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ñ‹, Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ĞµĞ½ Ğº ÑˆÑƒĞ¼Ñƒ)
    # 50.0 = Ğ¡Ğ Ğ•Ğ”ĞĞ˜Ğ™ ĞºĞ¾Ğ»Ğ¾ĞºĞ¾Ğ» (Ğ±Ğ°Ğ»Ğ°Ğ½Ñ)
    # 80.0 = Ğ¨Ğ˜Ğ ĞĞšĞ˜Ğ™ ĞºĞ¾Ğ»Ğ¾ĞºĞ¾Ğ» (Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ñ‹, Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾ĞºĞ°)
    #
    gauss_stds=[30.0, 50.0, 90.0],  # â† Ğ˜Ğ—ĞœĞ•ĞĞ˜Ğ¢Ğ•: Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ [50.0], [20.0, 50.0, 80.0]
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸ”¬ ABLATION: Ğ˜Ğ¡ĞšĞ›Ğ®Ğ§Ğ•ĞĞ˜Ğ• ĞĞ¡Ğ¢Ğ Ğ-Ğ¢Ğ•Ğ› (ĞºĞ°ĞºĞ¸Ğµ Ğ¿Ğ»Ğ°Ğ½ĞµÑ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ±Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑƒĞ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    #
    # Ablation study â€” Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, ĞºĞ°ĞºĞ¸Ğµ Ğ¿Ğ»Ğ°Ğ½ĞµÑ‚Ñ‹ Ğ Ğ•ĞĞ›Ğ¬ĞĞ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚.
    # ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ¸ÑĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞ» Ğ¸ ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¸Ğ¼, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ÑÑ Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ.
    #
    # 0 = ĞĞ• Ğ¸ÑĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒ Ñ‚ĞµĞ»Ğ° (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ orb/gauss/coord Ğ¿Ğ¾Ğ¸ÑĞº)
    # 1 = ĞŸÑ€Ğ¾Ğ±Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ 1 Ğ¿Ğ»Ğ°Ğ½ĞµÑ‚Ğµ
    # 2 = ĞŸÑ€Ğ¾Ğ±Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒ Ğ´Ğ¾ 2 Ğ¿Ğ»Ğ°Ğ½ĞµÑ‚
    # 4 = ĞŸÑ€Ğ¾Ğ±Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒ Ğ´Ğ¾ 4 Ğ¿Ğ»Ğ°Ğ½ĞµÑ‚ (Ğ Ğ•ĞšĞĞœĞ•ĞĞ”Ğ£Ğ•Ğ¢Ğ¡Ğ¯)
    #
    # âš ï¸ Ğ’ĞĞ˜ĞœĞĞĞ˜Ğ•: ÑÑ‚Ğ¾ ĞšĞĞœĞ‘Ğ˜ĞĞĞ¢ĞĞ ĞĞ«Ğ™ Ğ²Ğ·Ñ€Ñ‹Ğ²! Ğ¡ 11 Ñ‚ĞµĞ»Ğ°Ğ¼Ğ¸:
    #   max_exclude=1: 11 Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ²
    #   max_exclude=2: 66 Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ²
    #   max_exclude=3: 231 Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚
    #   max_exclude=4: 561 Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚
    #
    max_exclude=2,  # â† Ğ˜Ğ—ĞœĞ•ĞĞ˜Ğ¢Ğ•: 0 (Ğ±ĞµĞ· ablation), 1, 2, 3, 4
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸ›‘ Ğ›Ğ˜ĞœĞ˜Ğ¢ ĞšĞĞœĞ‘Ğ˜ĞĞĞ¦Ğ˜Ğ™ (Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿ÑƒÑĞºĞ¾Ğ²)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    #
    # Grid Search Ğ¿ĞµÑ€ĞµĞ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ’Ğ¡Ğ• ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸. ĞĞ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€:
    #   3 coord_modes Ã— 3 orb Ã— 3 window Ã— 3 std = 81 ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ!
    #   Ğ Ñ ablation: 81 Ã— 561 = 45,441 ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹!
    #
    # Ğ•ÑĞ»Ğ¸ Ñ…Ğ¾Ñ‚Ğ¸Ñ‚Ğµ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ñ‡Ñ‚Ğ¾ Ğ²ÑÑ‘ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ â€” Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²ÑŒÑ‚Ğµ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚.
    # None = Ğ±ĞµĞ· Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ğ° (Ğ¿ĞµÑ€ĞµĞ±Ñ€Ğ°Ñ‚ÑŒ Ğ’Ğ¡Ğ)
    #
    max_combos=None,  # â† Ğ˜Ğ—ĞœĞ•ĞĞ˜Ğ¢Ğ•: None (Ğ²ÑÑ‘) Ğ¸Ğ»Ğ¸ Ñ‡Ğ¸ÑĞ»Ğ¾, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ 5 Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ°
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸŒ³ ĞŸĞĞ ĞĞœĞ•Ğ¢Ğ Ğ« ĞœĞĞ”Ğ•Ğ›Ğ˜ XGBoost (Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ½Ğµ Ğ½ÑƒĞ¶Ğ½Ğ¾)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    #
    # Ğ­Ñ‚Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ”ĞµÑ„Ğ¾Ğ»Ñ‚Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾.
    # ĞœĞµĞ½ÑĞ¹Ñ‚Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞµÑĞ»Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚Ğµ Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚Ğµ.
    #
    model_params={
        "n_estimators": 500,      # ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´ĞµÑ€ĞµĞ²ÑŒĞµĞ² (Ğ±Ğ¾Ğ»ÑŒÑˆĞµ = Ñ‚Ğ¾Ñ‡Ğ½ĞµĞµ, Ğ½Ğ¾ Ğ´Ğ¾Ğ»ÑŒÑˆĞµ)
        "max_depth": 3,           # Ğ“Ğ»ÑƒĞ±Ğ¸Ğ½Ğ° Ğ´ĞµÑ€ĞµĞ²Ğ° (Ğ¼ĞµĞ½ÑŒÑˆĞµ = Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ)
        "learning_rate": 0.03,    # Ğ¡ĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (Ğ¼ĞµĞ½ÑŒÑˆĞµ = ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½ĞµĞµ)
        "subsample": 0.8,         # Ğ”Ğ¾Ğ»Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğµ Ğ´ĞµÑ€ĞµĞ²Ğ¾
        "colsample_bytree": 0.8,  # Ğ”Ğ¾Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğµ Ğ´ĞµÑ€ĞµĞ²Ğ¾
    },
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸš€ Ğ—ĞĞŸĞ£Ğ¡Ğš GRID SEARCH
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# n_workers=1  â€” Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ (Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾, Ğ½Ğ¾ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾)
# n_workers=4  â€” Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ° 4 Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°Ñ… (Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ!)
# n_workers=-1 â€” Ğ²ÑĞµ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ ÑĞ´Ñ€Ğ°
results_df = run_grid_search(df_market, GRID_CONFIG, n_workers=4)

# ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ğ¿-20 Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹
print("\nğŸ† Ğ¢Ğ¾Ğ¿-20 Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹:")
print(results_df.head(20))

# %%
# EVALUATE AND VISUALIZE BEST RESULT
from RESEARCH.grid_search import evaluate_and_plot_best

best_result = evaluate_and_plot_best(df_market, results_df.iloc[0])

# %%
# BODY ABLATION ONLY (uncomment to run)
# Use current params, test all body exclusion combinations

# from RESEARCH.grid_search import run_body_ablation_search
# 
# ablation_df = run_body_ablation_search(
#     df_market,
#     orb_mult=ORB_MULTIPLIER,
#     gauss_window=LABEL_CONFIG["gauss_window"],
#     gauss_std=LABEL_CONFIG["gauss_std"],
#     max_exclude=3,  # Try removing up to 3 bodies
# )

# %% [markdown]
# ## 10. Save Model

# %%
# Save model
from joblib import dump

artifact_dir = PROJECT_ROOT / "models_artifacts"
artifact_dir.mkdir(parents=True, exist_ok=True)

artifact = {
    "model": model.model,
    "scaler": model.scaler,
    "feature_names": feature_cols,
    "threshold": best_threshold,
    "config": {
        "label_config": LABEL_CONFIG,
        "orb_multiplier": ORB_MULTIPLIER,
        "model_params": MODEL_PARAMS,
    },
}

out_path = artifact_dir / f"xgb_astro_research.joblib"
dump(artifact, out_path)
print(f"âœ“ Model saved: {out_path}")

# %% [markdown]
# ---
# ## Summary
# 
# This modular pipeline makes it easy to:
# - **Debug**: Each module can be tested independently
# - **Extend**: Add new features, models, or visualizations
# - **Experiment**: Quickly try different configurations
# 
# ### TODO:
# - [ ] Add moon phases to features
# - [ ] Grid search for astro body exclusion
# - [ ] Add houses for birth date grid search
# - [ ] Save best grid search results
