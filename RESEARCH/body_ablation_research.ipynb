{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "BODY ABLATION RESEARCH SCRIPT\n",
    "=============================================================================\n",
    "\n",
    "This script performs comprehensive grid search to find optimal parameters\n",
    "for the BTC price prediction model.\n",
    "\n",
    "WHAT IS ABLATION?\n",
    "-----------------\n",
    "\"Ablation\" means removing something to see how it affects the result.\n",
    "In our case, we remove (exclude) certain astrological bodies from the\n",
    "feature set to see which ones are most important for prediction.\n",
    "\n",
    "For example:\n",
    "    - Exclude Mercury ‚Üí model gets WORSE (Mercury is important!)\n",
    "    - Exclude Pluto ‚Üí model gets BETTER (Pluto adds noise?)\n",
    "\n",
    "WHAT THIS SCRIPT DOES:\n",
    "----------------------\n",
    "1. BASELINE: Evaluate model with default params (no exclusions)\n",
    "2. GRID SEARCH: Try ALL combinations of:\n",
    "   - Coord modes: geo, helio, both\n",
    "   - Gauss params: window √ó std (9 combinations)\n",
    "   - Body exclusions: different planets to exclude (11 variations)\n",
    "3. BEST MODEL: Show detailed evaluation of the best configuration\n",
    "\n",
    "GRID SEARCH SPACE:\n",
    "------------------\n",
    "Total combinations = 3 coord √ó 3 windows √ó 3 stds √ó 11 exclusions = 297\n",
    "\n",
    "This takes about 30-60 minutes on GPU, 2-4 hours on CPU.\n",
    "\n",
    "HOW TO RUN:\n",
    "-----------\n",
    "    cd /home/rut/ostrofun\n",
    "    python -m RESEARCH.body_ablation_research\n",
    "\n",
    "Or use the Jupyter notebook version:\n",
    "    jupyter notebook RESEARCH/body_ablation_research.ipynb\n",
    "\n",
    "=============================================================================\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'RESEARCH'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# IMPORTS FROM RESEARCH PACKAGE\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# These are our custom modules for the research pipeline\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mRESEARCH\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_market_data\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mRESEARCH\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlabeling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_balanced_labels\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mRESEARCH\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mastro_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     init_ephemeris,                   \u001b[38;5;66;03m# Initialize Swiss Ephemeris\u001b[39;00m\n\u001b[32m     10\u001b[39m     calculate_bodies_for_dates_multi, \u001b[38;5;66;03m# Calculate planet positions\u001b[39;00m\n\u001b[32m     11\u001b[39m     calculate_aspects_for_dates,      \u001b[38;5;66;03m# Calculate aspects (angular relationships)\u001b[39;00m\n\u001b[32m     12\u001b[39m     calculate_phases_for_dates,       \u001b[38;5;66;03m# Calculate moon phases, elongations\u001b[39;00m\n\u001b[32m     13\u001b[39m )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'RESEARCH'"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS FROM RESEARCH PACKAGE\n",
    "# =============================================================================\n",
    "# These are our custom modules for the research pipeline\n",
    "\n",
    "from RESEARCH.data_loader import load_market_data\n",
    "from RESEARCH.labeling import create_balanced_labels\n",
    "from RESEARCH.astro_engine import (\n",
    "    init_ephemeris,                   # Initialize Swiss Ephemeris\n",
    "    calculate_bodies_for_dates_multi, # Calculate planet positions\n",
    "    calculate_aspects_for_dates,      # Calculate aspects (angular relationships)\n",
    "    calculate_phases_for_dates,       # Calculate moon phases, elongations\n",
    ")\n",
    "from RESEARCH.features import build_full_features, merge_features_with_labels\n",
    "from RESEARCH.model_training import (\n",
    "    split_dataset,                    # Split into train/val/test\n",
    "    prepare_xy,                       # Prepare X, y arrays\n",
    "    train_xgb_model,                  # Train XGBoost model\n",
    "    tune_threshold,                   # Find best probability threshold\n",
    "    predict_with_threshold,           # Predict using custom threshold\n",
    "    check_cuda_available,             # Check if GPU is available\n",
    ")\n",
    "from RESEARCH.evaluation import evaluate_model_full, compare_models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - MODEL HYPERPARAMETERS (FIXED)\n",
    "# =============================================================================\n",
    "# These XGBoost parameters were found to work well in previous experiments.\n",
    "# We keep them fixed during grid search to focus on astro parameters.\n",
    "\n",
    "MODEL_PARAMS = {\n",
    "    # -------------------------------------------------------------------------\n",
    "    # n_estimators: Number of boosting rounds (trees in the ensemble)\n",
    "    # Higher = more capacity but slower and risk of overfitting\n",
    "    # 500 is a good balance for our dataset size (~3000 samples)\n",
    "    # -------------------------------------------------------------------------\n",
    "    'n_estimators': 500,\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # max_depth: Maximum depth of each tree\n",
    "    # Deeper trees can capture more complex patterns but overfit easier\n",
    "    # depth=6 is standard, we use it to prevent overfitting on small data\n",
    "    # -------------------------------------------------------------------------\n",
    "    'max_depth': 6,\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # learning_rate: Step size shrinkage\n",
    "    # Smaller = more conservative updates, better generalization\n",
    "    # 0.03 works well with 500 estimators\n",
    "    # -------------------------------------------------------------------------\n",
    "    'learning_rate': 0.03,\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # colsample_bytree: Fraction of features to use per tree\n",
    "    # 0.6 = only 60% of features used for each tree (reduces correlation)\n",
    "    # This acts as regularization to prevent overfitting\n",
    "    # -------------------------------------------------------------------------\n",
    "    'colsample_bytree': 0.6,\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # subsample: Fraction of samples to use per tree\n",
    "    # 0.8 = only 80% of training data used for each tree\n",
    "    # This is \"stochastic gradient boosting\" - adds randomness\n",
    "    # -------------------------------------------------------------------------\n",
    "    'subsample': 0.8,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - GRID SEARCH PARAMETERS\n",
    "# =============================================================================\n",
    "# These are the astrological/labeling parameters we will search over.\n",
    "\n",
    "GRID_PARAMS = {\n",
    "    # -------------------------------------------------------------------------\n",
    "    # gauss_windows: Size of Gaussian smoothing window (in days)\n",
    "    # \n",
    "    # WHY WE SMOOTH: Raw price data is noisy. A big move one day might\n",
    "    # reverse the next. Smoothing helps identify \"real\" trends.\n",
    "    #\n",
    "    # Smaller window (150) = more sensitive to short-term moves\n",
    "    # Larger window (250) = only captures major trends\n",
    "    # -------------------------------------------------------------------------\n",
    "    'gauss_windows': [150, 200, 250],\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # gauss_stds: Standard deviation of Gaussian kernel\n",
    "    #\n",
    "    # This controls the \"shape\" of the smoothing:\n",
    "    # Smaller std (50) = narrow bell, sharp cutoff, focuses on center\n",
    "    # Larger std (90) = wide bell, includes more surrounding days\n",
    "    # -------------------------------------------------------------------------\n",
    "    'gauss_stds': [50.0, 70.0, 90.0],\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # coord_modes: Coordinate system for planetary calculations\n",
    "    #\n",
    "    # 'geo' = Geocentric (Earth-centered) - traditional astrology\n",
    "    #         Moon, Sun, planets as seen FROM Earth\n",
    "    #\n",
    "    # 'helio' = Heliocentric (Sun-centered) - astronomical\n",
    "    #           Planets in their actual orbits around Sun\n",
    "    #           Note: Moon and Sun are excluded in helio mode!\n",
    "    #\n",
    "    # 'both' = BOTH systems combined\n",
    "    #          Doubles the number of features (geo_Mars_lon, helio_Mars_lon)\n",
    "    #          More info but also more noise and overfitting risk\n",
    "    # -------------------------------------------------------------------------\n",
    "    'coord_modes': ['geo', 'helio', 'both'],\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # orb_mults: Orb multiplier for aspect detection\n",
    "    #\n",
    "    # \"Orb\" is the tolerance for aspects. An exact conjunction is 0¬∞,\n",
    "    # but we allow some deviation (e.g., 8¬∞).\n",
    "    #\n",
    "    # orb_mult=0.1 means we multiply standard orbs by 0.1, making them\n",
    "    # very strict (only near-exact aspects). This reduces false positives.\n",
    "    # -------------------------------------------------------------------------\n",
    "    'orb_mults': [0.1],\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - BODY EXCLUSION COMBINATIONS\n",
    "# =============================================================================\n",
    "# Based on single-body ablation study results (see previous experiments),\n",
    "# these are the most promising bodies to exclude.\n",
    "#\n",
    "# WHY EXCLUDE BODIES?\n",
    "# -------------------\n",
    "# Some astrological bodies might ADD NOISE rather than signal.\n",
    "# If excluding a body IMPROVES the model, that body was probably\n",
    "# not contributing useful information for BTC prediction.\n",
    "#\n",
    "# TOP 5 BODIES TO EXCLUDE (from single-body study):\n",
    "# 1. MeanNode - R_MIN improved by 7.5%\n",
    "# 2. Pluto - R_MIN improved by 4.5%\n",
    "# 3. Saturn - slight improvement\n",
    "# 4. Venus - best MCC improvement\n",
    "# 5. Neptune - slight improvement\n",
    "\n",
    "ABLATION_BODIES = [\n",
    "    # -------------------------------------------------------------------------\n",
    "    # BASELINE: No exclusions (use all bodies)\n",
    "    # -------------------------------------------------------------------------\n",
    "    [],\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # SINGLE BODY EXCLUSIONS: Remove one body at a time\n",
    "    # -------------------------------------------------------------------------\n",
    "    ['MeanNode'],   # Top performer in single-body study\n",
    "    ['Pluto'],      # Second best\n",
    "    ['Saturn'],     # Third\n",
    "    ['Venus'],      # Best MCC improvement\n",
    "    ['Neptune'],    # Fifth\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # PAIR EXCLUSIONS: Remove two bodies together\n",
    "    # Testing if combined exclusion is better than single\n",
    "    # -------------------------------------------------------------------------\n",
    "    ['MeanNode', 'Pluto'],   # Top 2 combined\n",
    "    ['MeanNode', 'Saturn'],  # #1 + #3\n",
    "    ['MeanNode', 'Venus'],   # #1 + best MCC\n",
    "    ['Pluto', 'Saturn'],     # #2 + #3\n",
    "    ['Pluto', 'Venus'],      # #2 + best MCC\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HELPER FUNCTION: TRAIN AND EVALUATE ONE CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "def train_and_evaluate(\n",
    "    df_market: pd.DataFrame,\n",
    "    df_bodies: pd.DataFrame,\n",
    "    geo_by_date: dict,\n",
    "    settings,\n",
    "    gauss_window: int,\n",
    "    gauss_std: float,\n",
    "    orb_mult: float,\n",
    "    exclude_bodies: list = None,\n",
    "    device: str = 'cpu',\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a model with specific parameters and return evaluation metrics.\n",
    "    \n",
    "    THIS IS THE CORE FUNCTION that:\n",
    "    1. Creates labels (UP/DOWN) using Gaussian smoothing\n",
    "    2. Calculates astrological features (aspects, phases)\n",
    "    3. Trains XGBoost model\n",
    "    4. Evaluates on test set\n",
    "    \n",
    "    PARAMETERS:\n",
    "    -----------\n",
    "    df_market : pd.DataFrame\n",
    "        Market data with 'date' and 'close' columns.\n",
    "        \n",
    "    df_bodies : pd.DataFrame\n",
    "        Pre-calculated planetary positions (from calculate_bodies_for_dates_multi).\n",
    "        \n",
    "    geo_by_date : dict\n",
    "        Dictionary mapping date -> list of BodyPosition objects.\n",
    "        Used for aspect calculations.\n",
    "        \n",
    "    settings : AstroSettings\n",
    "        Ephemeris settings (list of bodies, aspects, etc.)\n",
    "        \n",
    "    gauss_window : int\n",
    "        Gaussian smoothing window size (days).\n",
    "        \n",
    "    gauss_std : float\n",
    "        Gaussian standard deviation.\n",
    "        \n",
    "    orb_mult : float\n",
    "        Orb multiplier for aspects.\n",
    "        \n",
    "    exclude_bodies : list, optional\n",
    "        List of body names to exclude from features.\n",
    "        Example: ['MeanNode', 'Pluto']\n",
    "        \n",
    "    device : str\n",
    "        'cpu' or 'cuda' for GPU acceleration.\n",
    "        \n",
    "    verbose : bool\n",
    "        Whether to print threshold tuning progress.\n",
    "    \n",
    "    RETURNS:\n",
    "    --------\n",
    "    Dict with model and metrics, or None if not enough data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 1: Create labels (UP=1, DOWN=0) using Gaussian smoothing\n",
    "    # -------------------------------------------------------------------------\n",
    "    # This applies Gaussian filter to price, then labels days as\n",
    "    # UP (smoothed price increasing) or DOWN (smoothed price decreasing)\n",
    "    df_labels = create_balanced_labels(\n",
    "        df_market,\n",
    "        gauss_window=gauss_window,\n",
    "        gauss_std=gauss_std,\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 2: Calculate aspects using pre-cached body positions\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Aspects = angular relationships between planets (conjunction, square, etc.)\n",
    "    df_aspects = calculate_aspects_for_dates(\n",
    "        geo_by_date, settings, orb_mult=orb_mult, progress=False\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 3: Calculate moon phases and planet elongations\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Moon phase = new moon, full moon, quarters\n",
    "    # Elongation = angular distance of planet from Sun (morning/evening star)\n",
    "    df_phases = calculate_phases_for_dates(geo_by_date, progress=False)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 4: Build full feature matrix (excluding specified bodies)\n",
    "    # -------------------------------------------------------------------------\n",
    "    df_features = build_full_features(\n",
    "        df_bodies, df_aspects,\n",
    "        df_phases=df_phases,\n",
    "        exclude_bodies=exclude_bodies  # This is where ablation happens!\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 5: Merge features with labels\n",
    "    # -------------------------------------------------------------------------\n",
    "    df_dataset = merge_features_with_labels(df_features, df_labels)\n",
    "    \n",
    "    # Need at least 100 samples to train meaningfully\n",
    "    if len(df_dataset) < 100:\n",
    "        return None\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 6: Split into train/validation/test sets\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Time-based split (no shuffling!) to simulate real trading scenario\n",
    "    # Train: first 70%, Val: next 15%, Test: final 15%\n",
    "    train_df, val_df, test_df = split_dataset(df_dataset)\n",
    "    \n",
    "    # Extract feature columns (everything except 'date' and 'target')\n",
    "    feature_cols = [c for c in df_dataset.columns if c not in ['date', 'target']]\n",
    "    \n",
    "    # Prepare X (features) and y (labels) arrays\n",
    "    X_train, y_train = prepare_xy(train_df, feature_cols)\n",
    "    X_val, y_val = prepare_xy(val_df, feature_cols)\n",
    "    X_test, y_test = prepare_xy(test_df, feature_cols)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 7: Train XGBoost model\n",
    "    # -------------------------------------------------------------------------\n",
    "    model = train_xgb_model(\n",
    "        X_train, y_train, X_val, y_val,\n",
    "        feature_cols, n_classes=2, device=device,\n",
    "        **MODEL_PARAMS\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 8: Tune probability threshold for best recall_min\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Default threshold is 0.5 (predict UP if P(UP) > 0.5)\n",
    "    # But we can adjust this to balance recall between classes\n",
    "    best_t, _ = tune_threshold(model, X_val, y_val, metric='recall_min', verbose=verbose)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 9: Predict on test set using tuned threshold\n",
    "    # -------------------------------------------------------------------------\n",
    "    y_pred = predict_with_threshold(model, X_test, threshold=best_t)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 10: Calculate evaluation metrics\n",
    "    # -------------------------------------------------------------------------\n",
    "    from sklearn.metrics import classification_report, balanced_accuracy_score, matthews_corrcoef\n",
    "    \n",
    "    report = classification_report(\n",
    "        y_test, y_pred, labels=[0, 1],\n",
    "        target_names=['DOWN', 'UP'], output_dict=True, zero_division=0\n",
    "    )\n",
    "    \n",
    "    recall_down = report['DOWN']['recall']\n",
    "    recall_up = report['UP']['recall']\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'threshold': best_t,\n",
    "        'n_features': len(feature_cols),\n",
    "        'recall_min': min(recall_down, recall_up),\n",
    "        'recall_gap': abs(recall_down - recall_up),\n",
    "        'recall_down': recall_down,\n",
    "        'recall_up': recall_up,\n",
    "        'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),\n",
    "        'mcc': matthews_corrcoef(y_test, y_pred),\n",
    "        'f1_macro': report['macro avg']['f1-score'],\n",
    "        'y_test': y_test,\n",
    "        'y_pred': y_pred,\n",
    "        'test_dates': test_df['date'].reset_index(drop=True),\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MAIN FUNCTION: RUN FULL GRID SEARCH\n",
    "# =============================================================================\n",
    "\n",
    "def run_full_grid_search(df_market, settings, device='cpu'):\n",
    "    \"\"\"\n",
    "    Run comprehensive grid search over all parameter combinations.\n",
    "    \n",
    "    This function:\n",
    "    1. Pre-calculates body positions for each coord mode (expensive, do once)\n",
    "    2. Iterates through all parameter combinations\n",
    "    3. Trains and evaluates a model for each combination\n",
    "    4. Returns DataFrame with all results\n",
    "    \n",
    "    ESTIMATED TIME:\n",
    "    ---------------\n",
    "    - GPU (CUDA): 30-60 minutes for 297 combinations\n",
    "    - CPU: 2-4 hours\n",
    "    \n",
    "    RETURNS:\n",
    "    --------\n",
    "    pd.DataFrame with columns:\n",
    "        - coord_mode, gauss_window, gauss_std, orb_mult, exclude_bodies\n",
    "        - n_features, recall_min, recall_gap, balanced_accuracy, mcc, f1_macro\n",
    "    \"\"\"\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Generate all combinations using itertools.product\n",
    "    # -------------------------------------------------------------------------\n",
    "    combos = list(product(\n",
    "        GRID_PARAMS['coord_modes'],\n",
    "        GRID_PARAMS['gauss_windows'],\n",
    "        GRID_PARAMS['gauss_stds'],\n",
    "        GRID_PARAMS['orb_mults'],\n",
    "        ABLATION_BODIES,\n",
    "    ))\n",
    "    \n",
    "    print(f\"\\nüìä Total combinations: {len(combos)}\")\n",
    "    print(f\"   Coord modes: {GRID_PARAMS['coord_modes']}\")\n",
    "    print(f\"   Gauss windows: {GRID_PARAMS['gauss_windows']}\")\n",
    "    print(f\"   Gauss stds: {GRID_PARAMS['gauss_stds']}\")\n",
    "    print(f\"   Body exclusions: {len(ABLATION_BODIES)}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Pre-calculate body positions for each coord mode\n",
    "    # This is EXPENSIVE (takes ~1 minute per mode) so we cache results\n",
    "    # -------------------------------------------------------------------------\n",
    "    cached_bodies = {}\n",
    "    \n",
    "    for coord_mode in GRID_PARAMS['coord_modes']:\n",
    "        print(f\"\\nüìç Pre-calculating bodies for {coord_mode}...\")\n",
    "        df_bodies, geo_by_date, helio_by_date = calculate_bodies_for_dates_multi(\n",
    "            df_market['date'], settings, coord_mode=coord_mode, progress=True\n",
    "        )\n",
    "        cached_bodies[coord_mode] = (df_bodies, geo_by_date, helio_by_date)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Run grid search\n",
    "    # -------------------------------------------------------------------------\n",
    "    results = []\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üöÄ STARTING GRID SEARCH\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for i, (coord, gw, gs, orb, excl) in enumerate(combos):\n",
    "        # Create string representation of excluded bodies\n",
    "        excl_str = ','.join(excl) if excl else 'none'\n",
    "        \n",
    "        # Get cached body positions for this coord mode\n",
    "        df_bodies, geo_by_date, _ = cached_bodies[coord]\n",
    "        \n",
    "        # Train and evaluate\n",
    "        result = train_and_evaluate(\n",
    "            df_market, df_bodies, geo_by_date, settings,\n",
    "            gauss_window=gw,\n",
    "            gauss_std=gs,\n",
    "            orb_mult=orb,\n",
    "            exclude_bodies=excl if excl else None,\n",
    "            device=device,\n",
    "            verbose=False,  # Quiet mode for grid search\n",
    "        )\n",
    "        \n",
    "        if result is None:\n",
    "            continue\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'coord_mode': coord,\n",
    "            'gauss_window': gw,\n",
    "            'gauss_std': gs,\n",
    "            'orb_mult': orb,\n",
    "            'exclude_bodies': excl_str,\n",
    "            'n_features': result['n_features'],\n",
    "            'recall_min': result['recall_min'],\n",
    "            'recall_gap': result['recall_gap'],\n",
    "            'balanced_accuracy': result['balanced_accuracy'],\n",
    "            'mcc': result['mcc'],\n",
    "            'f1_macro': result['f1_macro'],\n",
    "            'threshold': result['threshold'],\n",
    "        })\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"[{i+1:3d}/{len(combos)}] {coord:5s} W={gw} S={gs:.0f} O={orb} \"\n",
    "              f\"excl={excl_str:20s} ‚Üí R_MIN={result['recall_min']:.3f} \"\n",
    "              f\"MCC={result['mcc']:.3f}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MAIN ENTRY POINT\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function that orchestrates the entire research workflow.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üî¨ BODY ABLATION RESEARCH - FULL GRID SEARCH\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Check if GPU is available (CUDA)\n",
    "    # -------------------------------------------------------------------------\n",
    "    _, device = check_cuda_available()\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Load market data (BTC/USD daily OHLCV)\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\nüì• Loading market data...\")\n",
    "    df_market = load_market_data()\n",
    "    \n",
    "    # Filter to start from 2017-11-01 (after significant BTC history)\n",
    "    df_market = df_market[df_market['date'] >= '2017-11-01'].reset_index(drop=True)\n",
    "    print(f\"Market data: {len(df_market)} rows\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Initialize Swiss Ephemeris\n",
    "    # -------------------------------------------------------------------------\n",
    "    settings = init_ephemeris()\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Run comprehensive grid search\n",
    "    # -------------------------------------------------------------------------\n",
    "    results_df = run_full_grid_search(df_market, settings, device=device)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Sort and display results\n",
    "    # -------------------------------------------------------------------------\n",
    "    results_df = results_df.sort_values('recall_min', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä TOP 20 RESULTS (by R_MIN)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(results_df.head(20).to_string(index=False))\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Show best result details\n",
    "    # -------------------------------------------------------------------------\n",
    "    best = results_df.iloc[0]\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üèÜ BEST CONFIGURATION\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Coord mode:     {best['coord_mode']}\")\n",
    "    print(f\"Gauss window:   {best['gauss_window']}\")\n",
    "    print(f\"Gauss std:      {best['gauss_std']}\")\n",
    "    print(f\"Orb mult:       {best['orb_mult']}\")\n",
    "    print(f\"Exclude bodies: {best['exclude_bodies']}\")\n",
    "    print(f\"Features:       {best['n_features']}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"R_MIN:          {best['recall_min']:.4f}\")\n",
    "    print(f\"BAL_ACC:        {best['balanced_accuracy']:.4f}\")\n",
    "    print(f\"MCC:            {best['mcc']:.4f}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Run full evaluation of best model with visualizations\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìà FULL EVALUATION OF BEST MODEL\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Re-calculate bodies for best coord mode\n",
    "    coord_mode = best['coord_mode']\n",
    "    df_bodies, geo_by_date, _ = calculate_bodies_for_dates_multi(\n",
    "        df_market['date'], settings, coord_mode=coord_mode, progress=False\n",
    "    )\n",
    "    \n",
    "    # Parse exclusion list\n",
    "    excl = best['exclude_bodies'].split(',') if best['exclude_bodies'] != 'none' else None\n",
    "    \n",
    "    # Train and evaluate with verbose output\n",
    "    result = train_and_evaluate(\n",
    "        df_market, df_bodies, geo_by_date, settings,\n",
    "        gauss_window=int(best['gauss_window']),\n",
    "        gauss_std=float(best['gauss_std']),\n",
    "        orb_mult=float(best['orb_mult']),\n",
    "        exclude_bodies=excl,\n",
    "        device=device,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    # Show full evaluation with confusion matrix and plots\n",
    "    evaluate_model_full(\n",
    "        result['y_test'], result['y_pred'],\n",
    "        dates=result['test_dates'],\n",
    "        title=f\"Best Model: {best['exclude_bodies']} ({best['coord_mode']})\",\n",
    "        show_plot=True,\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Save results to CSV\n",
    "    # -------------------------------------------------------------------------\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_path = f\"RESEARCH/reports/grid_search_{timestamp}.csv\"\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "    print(f\"\\nüíæ Results saved to: {results_path}\")\n",
    "    \n",
    "    return results_df, result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SCRIPT ENTRY POINT\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results_df, best_result = main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "btc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
