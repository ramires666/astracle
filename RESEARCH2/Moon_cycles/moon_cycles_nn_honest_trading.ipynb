{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4604c4a3",
   "metadata": {},
   "source": [
    "# Full Ephemeris — Honest NN + Trading Check (No Test Tuning)\n",
    "\n",
    "**Goal (simple):**\n",
    "\n",
    "- We want to know if the strong-looking trading curve from an `MLP` is a real edge, or just an accident from tuning on the TEST period.\n",
    "\n",
    "**The main trap we avoid:**\n",
    "\n",
    "- If we choose trading parameters (stop-loss, neutral-zone thresholds, etc.) by looking at TEST,\n",
    "  we can often create an impressive equity curve even from a weak or random signal.\n",
    "\n",
    "**Honest protocol (what we do here):**\n",
    "\n",
    "1. Train the model on **TRAIN**.\n",
    "2. Tune classification threshold and trading parameters using **VALIDATION only**.\n",
    "3. Freeze everything.\n",
    "4. Evaluate once on **TEST**.\n",
    "\n",
    "**Models we compare (pragmatic choice):**\n",
    "\n",
    "- `xgb` and `rf` are strong tabular baselines.\n",
    "- `sklearn_mlp` is the simple neural net we already use.\n",
    "- `keras_mlp` is a slightly more flexible neural net (dropout + L2) to test whether a better regularized NN can help.\n",
    "\n",
    "We keep the model list short on purpose:\n",
    "- too many models/variants = many comparisons = more false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bde5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Imports and project path setup\n",
    "# ------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROJECT_ROOT = Path('/home/rut/ostrofun')\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from RESEARCH2.Moon_cycles.moon_data import (\n",
    "    MoonLabelConfig,\n",
    "    load_market_slice,\n",
    "    build_moon_dataset_for_gauss,\n",
    "    get_moon_feature_columns,\n",
    ")\n",
    "from RESEARCH2.Moon_cycles.splits import make_classic_split\n",
    "from RESEARCH2.Moon_cycles.ephemeris_data import (\n",
    "    EphemerisFeatureConfig,\n",
    "    build_ephemeris_feature_set,\n",
    ")\n",
    "from RESEARCH2.Moon_cycles.eval_utils import (\n",
    "    compute_binary_metrics,\n",
    "    compute_statistical_significance,\n",
    ")\n",
    "from RESEARCH2.Moon_cycles.threshold_utils import tune_threshold_with_balance\n",
    "\n",
    "from RESEARCH.model_training import train_xgb_model\n",
    "from RESEARCH2.Moon_cycles.eval_visuals import VisualizationConfig\n",
    "from RESEARCH2.Moon_cycles.trading_utils import (\n",
    "    TradingConfig,\n",
    "    backtest_long_flat_signals,\n",
    "    build_signal_from_proba,\n",
    "    plot_backtest_price_and_equity,\n",
    ")\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.width', 160)\n",
    "\n",
    "print('tf version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f9a05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Configuration\n",
    "# ------------------------------\n",
    "\n",
    "# Market range.\n",
    "START_DATE = '2017-11-01'\n",
    "END_DATE = None\n",
    "\n",
    "# Feature mode: full ephemeris.\n",
    "ORB_MULT = 0.25  # you can change this after v3 finds a better orb\n",
    "CACHE_NAMESPACE = 'research2_ephem'  # orb=0.25 baseline cache\n",
    "\n",
    "# Label config.\n",
    "LABEL_CFG = MoonLabelConfig(\n",
    "    horizon=1,\n",
    "    move_share=0.5,\n",
    "    label_mode='balanced_detrended',\n",
    "    price_mode='raw',\n",
    ")\n",
    "\n",
    "# We fix one gaussian label config here to keep the notebook focused.\n",
    "# If you want, you can replace this with the best gauss from bakeoff.\n",
    "GAUSS_WINDOW = 201\n",
    "GAUSS_STD = 40.0\n",
    "\n",
    "# Threshold tuning penalties (validation-only).\n",
    "THRESHOLD_GAP_PENALTY = 0.25\n",
    "THRESHOLD_PRIOR_PENALTY = 0.05\n",
    "\n",
    "# Trading params grid (tuned on VALIDATION only).\n",
    "FEE_RATE = 0.001\n",
    "STOP_LOSSES = [0.0, 0.01, 0.02, 0.03, 0.05, 0.08]\n",
    "EXIT_ON_NO_SIGNAL_OPTIONS = [False, True]\n",
    "\n",
    "# Neutral-zone width around 0.50.\n",
    "# delta=0.00 -> always decide (0.50/0.50)\n",
    "# delta=0.02 -> (0.52/0.48)\n",
    "NEUTRAL_DELTAS = [0.00, 0.005, 0.01, 0.02, 0.03]\n",
    "\n",
    "# Visual style (dark theme) for trading plots.\n",
    "VIS_CFG = VisualizationConfig(\n",
    "    rolling_window_days=90,\n",
    "    rolling_min_periods=30,\n",
    "    probability_bins=64,\n",
    ")\n",
    "\n",
    "USE_CACHE = True\n",
    "VERBOSE = True\n",
    "\n",
    "\n",
    "def make_ephem_cfg(orb_mult: float) -> EphemerisFeatureConfig:\n",
    "    \"\"\"Build a full-ephemeris config for a given orb multiplier.\"\"\"\n",
    "\n",
    "    return EphemerisFeatureConfig(\n",
    "        coord_mode='both',\n",
    "        orb_mult=float(orb_mult),\n",
    "        include_pair_aspects=True,\n",
    "        include_phases=True,\n",
    "        add_trig_for_longitudes=True,\n",
    "        add_trig_for_moon_phase=True,\n",
    "        add_trig_for_elongations=True,\n",
    "        exclude_bodies=(),\n",
    "    )\n",
    "\n",
    "\n",
    "print('Config loaded.')\n",
    "print('ORB_MULT=', ORB_MULT)\n",
    "print('GAUSS_WINDOW/STD=', GAUSS_WINDOW, GAUSS_STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942092ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# Load market + build full ephemeris features\n",
    "# -------------------------------------------\n",
    "\n",
    "df_market = load_market_slice(\n",
    "    start_date=START_DATE,\n",
    "    end_date=END_DATE,\n",
    "    use_cache=USE_CACHE,\n",
    "    verbose=VERBOSE,\n",
    ")\n",
    "\n",
    "df_features = build_ephemeris_feature_set(\n",
    "    df_market=df_market,\n",
    "    cfg=make_ephem_cfg(ORB_MULT),\n",
    "    cache_namespace=CACHE_NAMESPACE,\n",
    "    use_cache=USE_CACHE,\n",
    "    verbose=VERBOSE,\n",
    "    progress=True,\n",
    ")\n",
    "\n",
    "print('Market rows :', len(df_market))\n",
    "print('Feature rows:', len(df_features))\n",
    "print('Feature cols:', len([c for c in df_features.columns if c != 'date']))\n",
    "print('Market range:', df_market['date'].min().date(), '->', df_market['date'].max().date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd158b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# Build one dataset + classic split\n",
    "# -------------------------------------------\n",
    "#\n",
    "# Important: we keep ONE label setup fixed to avoid endless tuning.\n",
    "# The goal here is not to find the best gauss; it is to check if\n",
    "# trading results are robust when we follow an honest protocol.\n",
    "\n",
    "df_dataset = build_moon_dataset_for_gauss(\n",
    "    df_market=df_market,\n",
    "    df_moon_features=df_features,\n",
    "    gauss_window=GAUSS_WINDOW,\n",
    "    gauss_std=GAUSS_STD,\n",
    "    label_cfg=LABEL_CFG,\n",
    "    cache_namespace=CACHE_NAMESPACE,\n",
    "    use_cache=USE_CACHE,\n",
    "    verbose=VERBOSE,\n",
    ")\n",
    "\n",
    "feature_cols = get_moon_feature_columns(df_dataset)\n",
    "split = make_classic_split(df_dataset)\n",
    "\n",
    "train_df = df_dataset.iloc[split.train_idx].copy().reset_index(drop=True)\n",
    "val_df = df_dataset.iloc[split.val_idx].copy().reset_index(drop=True)\n",
    "test_df = df_dataset.iloc[split.test_idx].copy().reset_index(drop=True)\n",
    "\n",
    "print('Rows:', {'train': len(train_df), 'val': len(val_df), 'test': len(test_df)})\n",
    "print('UP share:', {\n",
    "    'train': float((train_df['target'] == 1).mean()),\n",
    "    'val': float((val_df['target'] == 1).mean()),\n",
    "    'test': float((test_df['target'] == 1).mean()),\n",
    "})\n",
    "print('Num features:', len(feature_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c5fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# Helpers: training + predictions in a common format\n",
    "# -------------------------------------------\n",
    "\n",
    "def _make_pred_frame(df_part: pd.DataFrame, split_role: str, proba_up: np.ndarray) -> pd.DataFrame:\n",
    "    \"\"\"Create a small standardized prediction frame for one split.\"\"\"\n",
    "\n",
    "    out = df_part[['date', 'close', 'target']].copy().reset_index(drop=True)\n",
    "    out['split_role'] = split_role\n",
    "    out['pred_proba_up'] = np.asarray(proba_up, dtype=float)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _train_predict_xgb(train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Train XGBoost using the existing project wrapper and return predictions for all splits.\"\"\"\n",
    "\n",
    "    X_train = train_df[feature_cols].to_numpy(dtype=float)\n",
    "    y_train = train_df['target'].to_numpy(dtype=int)\n",
    "    X_val = val_df[feature_cols].to_numpy(dtype=float)\n",
    "    y_val = val_df['target'].to_numpy(dtype=int)\n",
    "    X_test = test_df[feature_cols].to_numpy(dtype=float)\n",
    "\n",
    "    model = train_xgb_model(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_val=X_val,\n",
    "        y_val=y_val,\n",
    "        feature_names=feature_cols,\n",
    "        n_classes=2,\n",
    "        device='cpu',\n",
    "        verbose=False,\n",
    "        early_stopping_rounds=50,\n",
    "        n_estimators=500,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.03,\n",
    "        colsample_bytree=0.8,\n",
    "        subsample=0.8,\n",
    "        weight_power=1.0,\n",
    "        sideways_penalty=1.0,\n",
    "    )\n",
    "\n",
    "    p_train = model.predict_proba(X_train)[:, 1]\n",
    "    p_val = model.predict_proba(X_val)[:, 1]\n",
    "    p_test = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    df_pred = pd.concat(\n",
    "        [\n",
    "            _make_pred_frame(train_df, 'train', p_train),\n",
    "            _make_pred_frame(val_df, 'val', p_val),\n",
    "            _make_pred_frame(test_df, 'test', p_test),\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    return df_pred\n",
    "\n",
    "\n",
    "def _train_predict_rf(train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Train a RandomForest and return predictions for all splits.\"\"\"\n",
    "\n",
    "    X_train = train_df[feature_cols].to_numpy(dtype=float)\n",
    "    y_train = train_df['target'].to_numpy(dtype=int)\n",
    "    X_val = val_df[feature_cols].to_numpy(dtype=float)\n",
    "    X_test = test_df[feature_cols].to_numpy(dtype=float)\n",
    "\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=800,\n",
    "        max_depth=6,\n",
    "        min_samples_leaf=8,\n",
    "        random_state=42,\n",
    "        n_jobs=1,\n",
    "    )\n",
    "\n",
    "    # RF supports sample_weight.\n",
    "    w_train = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "    model.fit(X_train, y_train, sample_weight=w_train)\n",
    "\n",
    "    p_train = model.predict_proba(X_train)[:, 1]\n",
    "    p_val = model.predict_proba(X_val)[:, 1]\n",
    "    p_test = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    df_pred = pd.concat(\n",
    "        [\n",
    "            _make_pred_frame(train_df, 'train', p_train),\n",
    "            _make_pred_frame(val_df, 'val', p_val),\n",
    "            _make_pred_frame(test_df, 'test', p_test),\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    return df_pred\n",
    "\n",
    "\n",
    "def _train_predict_sklearn_mlp(train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame, seed: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"Train sklearn MLP (with scaling) and return predictions for all splits.\"\"\"\n",
    "\n",
    "    X_train = train_df[feature_cols].to_numpy(dtype=float)\n",
    "    y_train = train_df['target'].to_numpy(dtype=int)\n",
    "    X_val = val_df[feature_cols].to_numpy(dtype=float)\n",
    "    X_test = test_df[feature_cols].to_numpy(dtype=float)\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_val_s = scaler.transform(X_val)\n",
    "    X_test_s = scaler.transform(X_test)\n",
    "\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(64, 32),\n",
    "        activation='relu',\n",
    "        alpha=1e-3,\n",
    "        learning_rate_init=1e-3,\n",
    "        max_iter=400,\n",
    "        random_state=int(seed),\n",
    "    )\n",
    "\n",
    "    w_train = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "    model.fit(X_train_s, y_train, sample_weight=w_train)\n",
    "\n",
    "    p_train = model.predict_proba(X_train_s)[:, 1]\n",
    "    p_val = model.predict_proba(X_val_s)[:, 1]\n",
    "    p_test = model.predict_proba(X_test_s)[:, 1]\n",
    "\n",
    "    df_pred = pd.concat(\n",
    "        [\n",
    "            _make_pred_frame(train_df, 'train', p_train),\n",
    "            _make_pred_frame(val_df, 'val', p_val),\n",
    "            _make_pred_frame(test_df, 'test', p_test),\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    return df_pred\n",
    "\n",
    "\n",
    "def _build_keras_mlp(input_dim: int, seed: int) -> tf.keras.Model:\n",
    "    \"\"\"A small, regularized tabular MLP in Keras.\n",
    "\n",
    "    Design goal:\n",
    "    - keep the model small to reduce overfitting risk,\n",
    "    - add dropout + L2, and early stopping on validation.\n",
    "    \"\"\"\n",
    "\n",
    "    tf.keras.utils.set_random_seed(int(seed))\n",
    "\n",
    "    inp = tf.keras.Input(shape=(int(input_dim),), name='x')\n",
    "    x = tf.keras.layers.Dense(\n",
    "        128,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "    )(inp)\n",
    "    x = tf.keras.layers.Dropout(0.25)(x)\n",
    "    x = tf.keras.layers.Dense(\n",
    "        64,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "    )(x)\n",
    "    x = tf.keras.layers.Dropout(0.25)(x)\n",
    "    out = tf.keras.layers.Dense(1, activation='sigmoid', name='p_up')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss='binary_crossentropy',\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def _train_predict_keras_mlp(train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame, seed: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"Train Keras MLP (with scaling) and return predictions for all splits.\"\"\"\n",
    "\n",
    "    X_train = train_df[feature_cols].to_numpy(dtype=float)\n",
    "    y_train = train_df['target'].to_numpy(dtype=float)\n",
    "    X_val = val_df[feature_cols].to_numpy(dtype=float)\n",
    "    y_val = val_df['target'].to_numpy(dtype=float)\n",
    "    X_test = test_df[feature_cols].to_numpy(dtype=float)\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_val_s = scaler.transform(X_val)\n",
    "    X_test_s = scaler.transform(X_test)\n",
    "\n",
    "    # Class balancing via sample weights.\n",
    "    w_train = compute_sample_weight(class_weight='balanced', y=y_train.astype(int))\n",
    "\n",
    "    model = _build_keras_mlp(input_dim=X_train_s.shape[1], seed=int(seed))\n",
    "\n",
    "    es = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=12,\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train_s,\n",
    "        y_train,\n",
    "        sample_weight=w_train,\n",
    "        validation_data=(X_val_s, y_val),\n",
    "        epochs=200,\n",
    "        batch_size=64,\n",
    "        callbacks=[es],\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    p_train = model.predict(X_train_s, verbose=0).reshape(-1)\n",
    "    p_val = model.predict(X_val_s, verbose=0).reshape(-1)\n",
    "    p_test = model.predict(X_test_s, verbose=0).reshape(-1)\n",
    "\n",
    "    df_pred = pd.concat(\n",
    "        [\n",
    "            _make_pred_frame(train_df, 'train', p_train),\n",
    "            _make_pred_frame(val_df, 'val', p_val),\n",
    "            _make_pred_frame(test_df, 'test', p_test),\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655856ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# Helper: honest trading tuning on VALIDATION only\n",
    "# -------------------------------------------\n",
    "\n",
    "def tune_trading_on_validation(\n",
    "    pred_all: pd.DataFrame,\n",
    "    model_name: str,\n",
    "    fee_rate: float,\n",
    "    stop_losses: list[float],\n",
    "    exit_on_no_signal_options: list[bool],\n",
    "    neutral_deltas: list[float],\n",
    ") -> dict:\n",
    "    \"\"\"Tune trading params on validation only, then evaluate on test.\n",
    "\n",
    "    We tune:\n",
    "    - neutral zone width around 0.50 (delta)\n",
    "    - stop_loss_pct\n",
    "    - exit_on_no_signal\n",
    "\n",
    "    Important:\n",
    "    - We NEVER choose these params by looking at the TEST period.\n",
    "    \"\"\"\n",
    "\n",
    "    df_val = pred_all[pred_all['split_role'] == 'val'].copy().reset_index(drop=True)\n",
    "    df_test = pred_all[pred_all['split_role'] == 'test'].copy().reset_index(drop=True)\n",
    "\n",
    "    best = None\n",
    "\n",
    "    combos = [(d, sl, ex) for d in neutral_deltas for sl in stop_losses for ex in exit_on_no_signal_options]\n",
    "    total = len(combos)\n",
    "\n",
    "    for i, (delta, sl, ex) in enumerate(combos, start=1):\n",
    "        up_th = 0.5 + float(delta)\n",
    "        down_th = 0.5 - float(delta)\n",
    "\n",
    "        # Build signals from probabilities.\n",
    "        df_val_tmp = df_val.copy()\n",
    "        df_val_tmp['signal'] = build_signal_from_proba(\n",
    "            df_val_tmp['pred_proba_up'].to_numpy(dtype=float),\n",
    "            threshold_up=up_th,\n",
    "            threshold_down=down_th,\n",
    "        )\n",
    "\n",
    "        cfg = TradingConfig(\n",
    "            fee_rate=float(fee_rate),\n",
    "            stop_loss_pct=float(sl),\n",
    "            exit_on_no_signal=bool(ex),\n",
    "            close_final_position=True,\n",
    "            initial_cash=1.0,\n",
    "        )\n",
    "\n",
    "        run_val = backtest_long_flat_signals(df_val_tmp, signal_col='signal', cfg=cfg)\n",
    "        m_val = dict(run_val['metrics'])\n",
    "\n",
    "        # A small guard against \"degenerate\" solutions.\n",
    "        # If there are literally 0 trades, the strategy is basically \"do nothing\".\n",
    "        # That can still be a valid outcome, but we want to see it clearly.\n",
    "        score = float(m_val.get('ulcer_adjusted_return', float('nan')))\n",
    "\n",
    "        cand = {\n",
    "            'model': model_name,\n",
    "            'delta': float(delta),\n",
    "            'up_th': float(up_th),\n",
    "            'down_th': float(down_th),\n",
    "            'stop_loss_pct': float(sl),\n",
    "            'exit_on_no_signal': bool(ex),\n",
    "            'val_metrics': m_val,\n",
    "            'val_run': run_val,\n",
    "            'score': score,\n",
    "        }\n",
    "\n",
    "        if best is None or (np.isfinite(score) and score > float(best['score'])):\n",
    "            best = cand\n",
    "\n",
    "        # Primitive progress.\n",
    "        left = total - i\n",
    "        if i == 1 or i % 20 == 0 or i == total:\n",
    "            print(\n",
    "                f\"[{model_name}] tune {i}/{total} left={left} \"\n",
    "                f\"delta={delta:.3f} stop={sl:.3f} exit_no_sig={ex} \"\n",
    "                f\"| VAL ret={m_val['return_pct']:.2%} UI={m_val['ulcer_index']:.2f} trades={m_val['num_trades']} \"\n",
    "                f\"| BEST score={best['score']:.4g}\"\n",
    "            )\n",
    "\n",
    "    assert best is not None\n",
    "\n",
    "    # Apply the best params to TEST (frozen).\n",
    "    df_test_tmp = df_test.copy()\n",
    "    df_test_tmp['signal'] = build_signal_from_proba(\n",
    "        df_test_tmp['pred_proba_up'].to_numpy(dtype=float),\n",
    "        threshold_up=float(best['up_th']),\n",
    "        threshold_down=float(best['down_th']),\n",
    "    )\n",
    "\n",
    "    cfg_best = TradingConfig(\n",
    "        fee_rate=float(fee_rate),\n",
    "        stop_loss_pct=float(best['stop_loss_pct']),\n",
    "        exit_on_no_signal=bool(best['exit_on_no_signal']),\n",
    "        close_final_position=True,\n",
    "        initial_cash=1.0,\n",
    "    )\n",
    "\n",
    "    run_test = backtest_long_flat_signals(df_test_tmp, signal_col='signal', cfg=cfg_best)\n",
    "\n",
    "    best['test_run'] = run_test\n",
    "    best['test_metrics'] = dict(run_test['metrics'])\n",
    "\n",
    "    return best\n",
    "\n",
    "\n",
    "def eval_classification_with_val_threshold(pred_all: pd.DataFrame, model_name: str) -> dict:\n",
    "    \"\"\"Tune a classification threshold on validation and report metrics.\"\"\"\n",
    "\n",
    "    df_val = pred_all[pred_all['split_role'] == 'val']\n",
    "    df_test = pred_all[pred_all['split_role'] == 'test']\n",
    "\n",
    "    y_val = df_val['target'].to_numpy(dtype=int)\n",
    "    p_val = df_val['pred_proba_up'].to_numpy(dtype=float)\n",
    "\n",
    "    t, score = tune_threshold_with_balance(\n",
    "        y_val=y_val,\n",
    "        proba_up=p_val,\n",
    "        gap_penalty=THRESHOLD_GAP_PENALTY,\n",
    "        prior_penalty=THRESHOLD_PRIOR_PENALTY,\n",
    "    )\n",
    "\n",
    "    out = {'model': model_name, 'val_threshold': float(t), 'val_threshold_score': float(score)}\n",
    "\n",
    "    for role, df_part in [('val', df_val), ('test', df_test)]:\n",
    "        y = df_part['target'].to_numpy(dtype=int)\n",
    "        p = df_part['pred_proba_up'].to_numpy(dtype=float)\n",
    "        pred = (p >= float(t)).astype(int)\n",
    "\n",
    "        m = compute_binary_metrics(y_true=y, y_pred=pred)\n",
    "        s = compute_statistical_significance(y_true=y, y_pred=pred, random_baseline=0.5)\n",
    "\n",
    "        for k, v in m.items():\n",
    "            out[f'{role}_{k}'] = float(v) if isinstance(v, (float, int)) else v\n",
    "        out[f'{role}_p_value_vs_random'] = float(s['p_value_vs_random'])\n",
    "        out[f'{role}_acc_ci95_low'] = float(s['ci95_low'])\n",
    "        out[f'{role}_acc_ci95_high'] = float(s['ci95_high'])\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ca9e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "            # Train models, run honest trading, compare\n",
    "            # -------------------------------------------\n",
    "\n",
    "            preds = {}\n",
    "\n",
    "            print('\n",
    "Training XGB...')\n",
    "            preds['xgb'] = _train_predict_xgb(train_df, val_df, test_df)\n",
    "\n",
    "            print('Training RF...')\n",
    "            preds['rf'] = _train_predict_rf(train_df, val_df, test_df)\n",
    "\n",
    "            print('Training sklearn MLP...')\n",
    "            preds['sklearn_mlp'] = _train_predict_sklearn_mlp(train_df, val_df, test_df, seed=42)\n",
    "\n",
    "            print('Training keras MLP...')\n",
    "            preds['keras_mlp'] = _train_predict_keras_mlp(train_df, val_df, test_df, seed=42)\n",
    "\n",
    "            # Classification report (threshold tuned on validation).\n",
    "            cls_rows = []\n",
    "            for name, df_pred in preds.items():\n",
    "                cls_rows.append(eval_classification_with_val_threshold(df_pred, model_name=name))\n",
    "\n",
    "            df_cls = pd.DataFrame(cls_rows).sort_values(['val_recall_min','val_recall_gap','val_mcc','val_accuracy'], ascending=[False, True, False, False])\n",
    "            print('\n",
    "Classification comparison (val-threshold, sorted by val rule):')\n",
    "            display(df_cls[[\n",
    "                'model','val_threshold',\n",
    "                'val_accuracy','val_mcc','val_recall_min','val_recall_gap','val_p_value_vs_random',\n",
    "                'test_accuracy','test_mcc','test_recall_min','test_recall_gap','test_p_value_vs_random',\n",
    "            ]])\n",
    "\n",
    "            # Honest trading: tune on VAL, apply once on TEST.\n",
    "            trading_rows = []\n",
    "            best_trading = {}\n",
    "\n",
    "            for name, df_pred in preds.items():\n",
    "                print('\n",
    "' + '=' * 120)\n",
    "                print('HONEST TRADING TUNE (VAL only) -> APPLY (TEST):', name)\n",
    "                best = tune_trading_on_validation(\n",
    "                    pred_all=df_pred,\n",
    "                    model_name=name,\n",
    "                    fee_rate=FEE_RATE,\n",
    "                    stop_losses=STOP_LOSSES,\n",
    "                    exit_on_no_signal_options=EXIT_ON_NO_SIGNAL_OPTIONS,\n",
    "                    neutral_deltas=NEUTRAL_DELTAS,\n",
    "                )\n",
    "                best_trading[name] = best\n",
    "\n",
    "                row = {\n",
    "                    'model': name,\n",
    "                    'delta': best['delta'],\n",
    "                    'stop_loss_pct': best['stop_loss_pct'],\n",
    "                    'exit_on_no_signal': best['exit_on_no_signal'],\n",
    "                    'val_return_pct': best['val_metrics']['return_pct'],\n",
    "                    'val_ulcer': best['val_metrics']['ulcer_index'],\n",
    "                    'val_uarr': best['val_metrics']['ulcer_adjusted_return'],\n",
    "                    'val_trades': best['val_metrics']['num_trades'],\n",
    "                    'test_return_pct': best['test_metrics']['return_pct'],\n",
    "                    'test_hold_return_pct': best['test_metrics']['hold_return_pct'],\n",
    "                    'test_excess_return_pct': best['test_metrics']['excess_return_pct'],\n",
    "                    'test_ulcer': best['test_metrics']['ulcer_index'],\n",
    "                    'test_uarr': best['test_metrics']['ulcer_adjusted_return'],\n",
    "                    'test_trades': best['test_metrics']['num_trades'],\n",
    "                    'test_max_dd_pct': best['test_metrics']['max_drawdown_pct'],\n",
    "                    'test_exposure_pct': best['test_metrics']['exposure_pct'],\n",
    "                }\n",
    "                trading_rows.append(row)\n",
    "\n",
    "                # Plot the TEST equity for the chosen params.\n",
    "                plot_backtest_price_and_equity(\n",
    "                    best['test_run'],\n",
    "                    title=(\n",
    "                        f\"{name.upper()} — HONEST trading (params tuned on VAL) \"\n",
    "                        f\"delta={best['delta']:.3f} stop={best['stop_loss_pct']:.1%} exit_no_sig={best['exit_on_no_signal']}\"\n",
    "                    ),\n",
    "                    vis_cfg=VIS_CFG,\n",
    "                )\n",
    "\n",
    "            df_trading = pd.DataFrame(trading_rows).sort_values(['test_uarr','test_return_pct'], ascending=[False, False]).reset_index(drop=True)\n",
    "            print('\n",
    "Trading comparison (params tuned on VAL, evaluated on TEST):')\n",
    "            display(df_trading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f21b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "            # Stability check for NNs (multiple seeds)\n",
    "            # -------------------------------------------\n",
    "            #\n",
    "            # If the \"edge\" is real, NN results should be somewhat stable\n",
    "            # across random seeds. If the result jumps wildly, it is a red flag.\n",
    "\n",
    "            SEEDS = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "            def run_seed_stability(kind: str) -> pd.DataFrame:\n",
    "                rows = []\n",
    "                for seed in SEEDS:\n",
    "                    if kind == 'sklearn_mlp':\n",
    "                        df_pred = _train_predict_sklearn_mlp(train_df, val_df, test_df, seed=int(seed))\n",
    "                    elif kind == 'keras_mlp':\n",
    "                        df_pred = _train_predict_keras_mlp(train_df, val_df, test_df, seed=int(seed))\n",
    "                    else:\n",
    "                        raise ValueError('unknown kind')\n",
    "\n",
    "                    # Tune trading params on VAL, apply to TEST.\n",
    "                    best = tune_trading_on_validation(\n",
    "                        pred_all=df_pred,\n",
    "                        model_name=kind,\n",
    "                        fee_rate=FEE_RATE,\n",
    "                        stop_losses=STOP_LOSSES,\n",
    "                        exit_on_no_signal_options=EXIT_ON_NO_SIGNAL_OPTIONS,\n",
    "                        neutral_deltas=NEUTRAL_DELTAS,\n",
    "                    )\n",
    "\n",
    "                    rows.append({\n",
    "                        'seed': int(seed),\n",
    "                        'delta': best['delta'],\n",
    "                        'stop_loss_pct': best['stop_loss_pct'],\n",
    "                        'exit_on_no_signal': best['exit_on_no_signal'],\n",
    "                        'val_uarr': best['val_metrics']['ulcer_adjusted_return'],\n",
    "                        'test_uarr': best['test_metrics']['ulcer_adjusted_return'],\n",
    "                        'test_return_pct': best['test_metrics']['return_pct'],\n",
    "                        'test_excess_return_pct': best['test_metrics']['excess_return_pct'],\n",
    "                        'test_ulcer': best['test_metrics']['ulcer_index'],\n",
    "                        'test_trades': best['test_metrics']['num_trades'],\n",
    "                    })\n",
    "\n",
    "                    print(f'[{kind}] seed={seed} -> test_ret={rows[-1][\"test_return_pct\"]:.2%} test_uarr={rows[-1][\"test_uarr\"]:.4g}')\n",
    "\n",
    "                return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "            df_sklearn = run_seed_stability('sklearn_mlp')\n",
    "            df_keras = run_seed_stability('keras_mlp')\n",
    "\n",
    "            print('\n",
    "Seed stability summary (sklearn_mlp):')\n",
    "            display(df_sklearn.describe(include='all'))\n",
    "            print('\n",
    "Seed stability summary (keras_mlp):')\n",
    "            display(df_keras.describe(include='all'))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
