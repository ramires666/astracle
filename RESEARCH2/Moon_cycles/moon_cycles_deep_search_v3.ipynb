{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca5ce3ff",
   "metadata": {},
   "source": [
    "# Moon Cycles Deep Search (v3) \u2014 Orb Multiplier Sweep (Full Ephemeris Features)\n",
    "\n",
    "**Research question (simple):**\n",
    "\n",
    "- In the full-ephemeris experiment (all bodies + aspects + phases), does changing the *aspect orb strictness* (`orb_mult`) create any *real* predictive edge?\n",
    "\n",
    "Why we care about `orb_mult`:\n",
    "\n",
    "- In earlier research we saw a pattern: **narrower orbs** (stricter aspect detection) sometimes improved results.\n",
    "- If that pattern is real, it should also show up in the full-feature setup.\n",
    "\n",
    "Why we do a 2-stage process (speed + honesty):\n",
    "\n",
    "- A full grid over `(orb_mult \u00d7 gauss_window \u00d7 gauss_std \u00d7 models)` is very expensive.\n",
    "- It also increases the risk of \"false discoveries\" (you can always find a lucky configuration by brute force).\n",
    "\n",
    "So we do:\n",
    "\n",
    "1. **FAST scan**: sweep `orb_mult` with **XGBoost only** and a **smaller Gaussian grid**.\n",
    "   - Goal: quickly see if any orb range looks promising on *validation*.\n",
    "\n",
    "2. **FINAL check**: for the best orb(s) from validation, run the **full Gaussian grid** and compare **multiple model families**.\n",
    "   - Goal: confirm the result is not just one model being lucky.\n",
    "\n",
    "Honesty rules we follow:\n",
    "\n",
    "- We select hyperparameters using **validation**.\n",
    "- We look at **test** only as a final report.\n",
    "- We print statistical checks (p-value vs 50% and Wilson 95% CI for accuracy) because small improvements can be pure noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d275a2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Imports and project path setup\n",
    "# ------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "PROJECT_ROOT = Path('/home/rut/ostrofun')\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from RESEARCH2.Moon_cycles.moon_data import (\n",
    "    MoonLabelConfig,\n",
    "    load_market_slice,\n",
    ")\n",
    "from RESEARCH2.Moon_cycles.ephemeris_data import (\n",
    "    EphemerisFeatureConfig,\n",
    "    build_ephemeris_feature_set,\n",
    ")\n",
    "from RESEARCH2.Moon_cycles.bakeoff_utils import (\n",
    "    run_moon_model_bakeoff,\n",
    "    default_model_specs,\n",
    ")\n",
    "from RESEARCH2.Moon_cycles.eval_visuals import (\n",
    "    VisualizationConfig,\n",
    "    evaluate_with_visuals,\n",
    ")\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.width', 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12defd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Configuration block\n",
    "# ------------------------------\n",
    "\n",
    "# Market date range.\n",
    "START_DATE = '2017-11-01'\n",
    "END_DATE = None\n",
    "\n",
    "# Caching is essential because ephemeris + model grids are expensive.\n",
    "USE_CACHE = True\n",
    "VERBOSE = True\n",
    "\n",
    "# We are doing a FULL ephemeris experiment in this notebook.\n",
    "# (Moon-only does not have aspect orbs, so `orb_mult` is irrelevant there.)\n",
    "FEATURE_SET = 'ephemeris_all'\n",
    "\n",
    "# Label configuration (same idea as other notebooks).\n",
    "LABEL_CFG = MoonLabelConfig(\n",
    "    horizon=1,\n",
    "    move_share=0.5,\n",
    "    label_mode='balanced_detrended',\n",
    "    price_mode='raw',\n",
    ")\n",
    "\n",
    "# Orb multipliers we want to test.\n",
    "# Smaller = stricter aspect detection.\n",
    "ORB_MULTS = [0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70]\n",
    "\n",
    "# Optional: include the historical baseline (0.25) because we already have cached runs for it.\n",
    "# This makes comparisons easier and saves time.\n",
    "INCLUDE_BASELINE_ORB_025 = True\n",
    "BASELINE_ORB_025 = 0.25\n",
    "\n",
    "# Two-stage search control.\n",
    "# Stage 1: fast orb scan (smaller Gaussian grid, usually XGB only).\n",
    "DO_FAST_ORB_SCAN = True\n",
    "\n",
    "# Stage 2: final confirmation on the best orb(s).\n",
    "DO_FINAL_FULL_GRID = True\n",
    "\n",
    "# FAST scan Gaussian grid.\n",
    "# Why smaller: orb sweep already multiplies runtime.\n",
    "GAUSS_WINDOWS_SCAN = [151, 201, 251, 301]\n",
    "GAUSS_STDS_SCAN = [20.0, 30.0, 40.0, 50.0, 70.0]\n",
    "\n",
    "# FULL confirmation Gaussian grid (same as v2 wide grid).\n",
    "GAUSS_WINDOWS_FULL = [51, 101, 151, 201, 251, 301, 351, 401]\n",
    "GAUSS_STDS_FULL = [10.0, 15.0, 20.0, 25.0, 30.0, 40.0, 50.0, 70.0, 90.0]\n",
    "\n",
    "# Threshold tuning penalties (same as other notebooks).\n",
    "THRESHOLD_GAP_PENALTY = 0.25\n",
    "THRESHOLD_PRIOR_PENALTY = 0.05\n",
    "\n",
    "# XGBoost params.\n",
    "XGB_PARAMS = {\n",
    "    'n_estimators': 500,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.03,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'subsample': 0.8,\n",
    "    'early_stopping_rounds': 50,\n",
    "}\n",
    "\n",
    "# Visual style (dark theme).\n",
    "VIS_CFG = VisualizationConfig(\n",
    "    rolling_window_days=90,\n",
    "    rolling_min_periods=30,\n",
    "    probability_bins=64,\n",
    ")\n",
    "\n",
    "\n",
    "def cache_namespace_for_orb(orb_mult: float) -> str:\n",
    "    \"\"\"Return a cache namespace that is SAFE for this orb value.\n",
    "\n",
    "    Why we need this:\n",
    "    - `run_moon_model_bakeoff()` caches runs using a key that does NOT include orb_mult.\n",
    "    - So if we reused one namespace for multiple orb values, we could accidentally\n",
    "      load cached model runs trained on a DIFFERENT orb.\n",
    "\n",
    "    To keep research honest, we isolate cache namespaces per orb.\n",
    "\n",
    "    Special case:\n",
    "    - orb=0.25 is the baseline from v2 and already has cache under 'research2_ephem'.\n",
    "      We reuse it to save time.\n",
    "    \"\"\"\n",
    "\n",
    "    orb_mult = float(orb_mult)\n",
    "    if abs(orb_mult - BASELINE_ORB_025) < 1e-12:\n",
    "        return 'research2_ephem'\n",
    "\n",
    "    tag = f\"{orb_mult:.2f}\".replace('.', 'p')\n",
    "    return f\"research2_ephem_orb{tag}\"\n",
    "\n",
    "\n",
    "def make_ephem_cfg(orb_mult: float) -> EphemerisFeatureConfig:\n",
    "    \"\"\"Create a feature config for the full-ephemeris feature builder.\"\"\"\n",
    "\n",
    "    return EphemerisFeatureConfig(\n",
    "        coord_mode='both',\n",
    "        orb_mult=float(orb_mult),\n",
    "        include_pair_aspects=True,\n",
    "        include_phases=True,\n",
    "        add_trig_for_longitudes=True,\n",
    "        add_trig_for_moon_phase=True,\n",
    "        add_trig_for_elongations=True,\n",
    "        exclude_bodies=(),\n",
    "    )\n",
    "\n",
    "\n",
    "# Expand ORB_MULTS with baseline if requested.\n",
    "if INCLUDE_BASELINE_ORB_025 and BASELINE_ORB_025 not in ORB_MULTS:\n",
    "    ORB_MULTS = sorted(list(ORB_MULTS) + [BASELINE_ORB_025])\n",
    "\n",
    "print('Config loaded.')\n",
    "print('ORB_MULTS =', ORB_MULTS)\n",
    "print('FAST scan grid size =', len(GAUSS_WINDOWS_SCAN) * len(GAUSS_STDS_SCAN))\n",
    "print('FULL grid size      =', len(GAUSS_WINDOWS_FULL) * len(GAUSS_STDS_FULL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc61cdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# Load market data (cached)\n",
    "# -------------------------------------------\n",
    "\n",
    "df_market = load_market_slice(\n",
    "    start_date=START_DATE,\n",
    "    end_date=END_DATE,\n",
    "    use_cache=USE_CACHE,\n",
    "    verbose=VERBOSE,\n",
    ")\n",
    "\n",
    "print('Market rows :', len(df_market))\n",
    "print('Market range:', df_market['date'].min().date(), '->', df_market['date'].max().date())\n",
    "display(df_market.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985fd746",
   "metadata": {},
   "source": [
    "## Quick sanity: do we need many models?\n",
    "\n",
    "Before we run a big orb sweep, it is useful to answer:\n",
    "\n",
    "- Are all models doing equally badly (random)?\n",
    "- Is one model family clearly the most stable / best on validation?\n",
    "\n",
    "Practical decision rule for this notebook:\n",
    "\n",
    "- For the **FAST orb scan** we default to **XGBoost only** (speed).\n",
    "- For the **FINAL confirmation** we run **multiple families** (honesty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46f902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# Baseline multi-model comparison (orb=0.25)\n",
    "# -------------------------------------------\n",
    "#\n",
    "# This cell reuses the existing v2 cache (namespace 'research2_ephem').\n",
    "# If you do not care, you can skip this cell.\n",
    "\n",
    "BASELINE_ORB = 0.25\n",
    "base_ns = cache_namespace_for_orb(BASELINE_ORB)\n",
    "\n",
    "base_cfg = make_ephem_cfg(BASELINE_ORB)\n",
    "df_base_features = build_ephemeris_feature_set(\n",
    "    df_market=df_market,\n",
    "    cfg=base_cfg,\n",
    "    cache_namespace=base_ns,\n",
    "    use_cache=USE_CACHE,\n",
    "    verbose=VERBOSE,\n",
    "    progress=True,\n",
    ")\n",
    "\n",
    "bakeoff_base = run_moon_model_bakeoff(\n",
    "    df_market=df_market,\n",
    "    df_moon_features=df_base_features,\n",
    "    gauss_windows=GAUSS_WINDOWS_FULL,\n",
    "    gauss_stds=GAUSS_STDS_FULL,\n",
    "    label_cfg=LABEL_CFG,\n",
    "    model_specs=None,            # None => default model list\n",
    "    include_xgb=True,\n",
    "    xgb_params=XGB_PARAMS,\n",
    "    threshold_gap_penalty=THRESHOLD_GAP_PENALTY,\n",
    "    threshold_prior_penalty=THRESHOLD_PRIOR_PENALTY,\n",
    "    cache_namespace=base_ns,\n",
    "    use_cache=USE_CACHE,\n",
    "    verbose=False,               # keep output shorter (cache loads can be noisy)\n",
    ")\n",
    "\n",
    "best_by_val_base = bakeoff_base['best_by_val_table']\n",
    "\n",
    "print('Baseline orb=0.25 (best-by-validation per model):')\n",
    "display(best_by_val_base)\n",
    "\n",
    "# Helpful small view: just the key columns.\n",
    "show_cols = [\n",
    "    'model',\n",
    "    'gauss_window',\n",
    "    'gauss_std',\n",
    "    'val_recall_min', 'val_recall_gap', 'val_mcc', 'val_acc',\n",
    "    'test_recall_min','test_recall_gap','test_mcc','test_acc',\n",
    "    'p_value_vs_random',\n",
    "]\n",
    "display(best_by_val_base[show_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be2854c",
   "metadata": {},
   "source": [
    "## Stage 1 \u2014 FAST orb scan (XGB only)\n",
    "\n",
    "What we do:\n",
    "\n",
    "- For each `orb_mult` we build ephemeris features.\n",
    "- We run a small Gaussian grid and fit **XGBoost only**.\n",
    "- We record the best-by-validation row.\n",
    "\n",
    "Why this makes sense:\n",
    "\n",
    "- If orb_mult does not matter at all, we should see validation metrics stay near random for all orbs.\n",
    "- If orb_mult really matters, we should see one orb range consistently better on validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f639f3d3",
   "metadata": {},
   "outputs": [],
   "source": "# -------------------------------------------\n# FAST scan loop\n# -------------------------------------------\n\n\nif not DO_FAST_ORB_SCAN:\n    # If we skip the scan, we must still define `best_orb` for the final stage.\n    # We default to the historical baseline orb=0.25 when it is included.\n    if INCLUDE_BASELINE_ORB_025:\n        best_orb = float(BASELINE_ORB_025)\n    else:\n        best_orb = float(ORB_MULTS[0])\n    print('DO_FAST_ORB_SCAN=False -> skipping scan. Using best_orb =', best_orb)\nelse:\n    orb_rows = []\n    best_so_far = None\n\n    for i, orb in enumerate(ORB_MULTS, start=1):\n        ns = cache_namespace_for_orb(orb)\n        cfg = make_ephem_cfg(orb)\n\n        print() \n        print('#' * 110)\n        print(f'[ORB {i}/{len(ORB_MULTS)}] orb_mult={orb:.2f}  cache_namespace={ns}')\n\n        df_features = build_ephemeris_feature_set(\n            df_market=df_market,\n            cfg=cfg,\n            cache_namespace=ns,\n            use_cache=USE_CACHE,\n            verbose=VERBOSE,\n            progress=True,\n        )\n\n        # XGB-only: we pass model_specs=[] (supported by bakeoff_utils).\n        bakeoff = run_moon_model_bakeoff(\n            df_market=df_market,\n            df_moon_features=df_features,\n            gauss_windows=GAUSS_WINDOWS_SCAN,\n            gauss_stds=GAUSS_STDS_SCAN,\n            label_cfg=LABEL_CFG,\n            model_specs=[],\n            include_xgb=True,\n            xgb_params=XGB_PARAMS,\n            threshold_gap_penalty=THRESHOLD_GAP_PENALTY,\n            threshold_prior_penalty=THRESHOLD_PRIOR_PENALTY,\n            cache_namespace=ns,\n            use_cache=USE_CACHE,\n            verbose=False,\n        )\n\n        best_by_val = bakeoff['best_by_val_table']\n        if best_by_val.empty:\n            raise RuntimeError('Unexpected: best_by_val_table is empty.')\n\n        row = best_by_val.iloc[0].to_dict()\n        row['orb_mult'] = float(orb)\n        row['n_features'] = int(len([c for c in df_features.columns if c != 'date']))\n\n        orb_rows.append(row)\n\n        # Primitive progress feedback for humans.\n        # We show VAL-first metrics because that is our honest selection rule.\n        print(\n            f\"FAST best: val_acc={row['val_acc']:.3f} val_mcc={row['val_mcc']:.3f} \"\n            f\"val_recall_min={row['val_recall_min']:.3f} | \"\n            f\"test_acc={row['test_acc']:.3f} test_mcc={row['test_mcc']:.3f}\"\n        )\n\n        # Track best orb so far by the SAME sorting rule we use everywhere.\n        if best_so_far is None:\n            best_so_far = row\n        else:\n            cur = row\n            prev = best_so_far\n\n            cur_key = (cur['val_recall_min'], -cur['val_recall_gap'], cur['val_mcc'], cur['val_acc'])\n            prev_key = (prev['val_recall_min'], -prev['val_recall_gap'], prev['val_mcc'], prev['val_acc'])\n\n            if cur_key > prev_key:\n                best_so_far = row\n                print(f'NEW BEST ORB SO FAR (by validation rule): orb_mult={orb:.2f}')\n\n    df_orb_scan = pd.DataFrame(orb_rows)\n\n    # Sort by the same validation rule (recall-min, then balance, then MCC, then ACC).\n    df_orb_scan_sorted = df_orb_scan.sort_values(\n        ['val_recall_min','val_recall_gap','val_mcc','val_acc'],\n        ascending=[False, True, False, False],\n    ).reset_index(drop=True)\n\n    print() \n    print('FAST orb scan summary (sorted by validation rule):')\n    display(\n        df_orb_scan_sorted[[\n            'orb_mult','n_features','gauss_window','gauss_std',\n            'val_recall_min','val_recall_gap','val_mcc','val_acc',\n            'test_recall_min','test_recall_gap','test_mcc','test_acc',\n            'p_value_vs_random',\n        ]].head(30)\n    )\n\n    best_orb = float(df_orb_scan_sorted.iloc[0]['orb_mult'])\n    print('Selected best orb by validation sorting:', best_orb)\n"
  },
  {
   "cell_type": "markdown",
   "id": "a9932877",
   "metadata": {},
   "source": [
    "## Stage 2 \u2014 FINAL confirmation (full grid + multiple models)\n",
    "\n",
    "Now we take the best orb from Stage 1 (selected by validation) and:\n",
    "\n",
    "- run the full Gaussian grid\n",
    "- compare multiple model families (XGB + sklearn baselines)\n",
    "- produce the full set of visual diagnostics\n",
    "\n",
    "This is the closest thing to a \"final answer\" inside this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2245b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# FINAL confirmation on the best orb\n",
    "# -------------------------------------------\n",
    "\n",
    "if not DO_FINAL_FULL_GRID:\n",
    "    print('DO_FINAL_FULL_GRID=False, skipping final confirmation.')\n",
    "else:\n",
    "    orb = float(best_orb)\n",
    "    ns = cache_namespace_for_orb(orb)\n",
    "    cfg = make_ephem_cfg(orb)\n",
    "\n",
    "    print() \n",
    "    print('#' * 110)\n",
    "    print(f'FINAL CONFIRMATION: orb_mult={orb:.2f}  cache_namespace={ns}')\n",
    "\n",
    "    df_features = build_ephemeris_feature_set(\n",
    "        df_market=df_market,\n",
    "        cfg=cfg,\n",
    "        cache_namespace=ns,\n",
    "        use_cache=USE_CACHE,\n",
    "        verbose=VERBOSE,\n",
    "        progress=True,\n",
    "    )\n",
    "\n",
    "    # Full bakeoff: multiple models.\n",
    "    bakeoff_final = run_moon_model_bakeoff(\n",
    "        df_market=df_market,\n",
    "        df_moon_features=df_features,\n",
    "        gauss_windows=GAUSS_WINDOWS_FULL,\n",
    "        gauss_stds=GAUSS_STDS_FULL,\n",
    "        label_cfg=LABEL_CFG,\n",
    "        model_specs=None,  # None => default list (logreg + rf + mlp)\n",
    "        include_xgb=True,\n",
    "        xgb_params=XGB_PARAMS,\n",
    "        threshold_gap_penalty=THRESHOLD_GAP_PENALTY,\n",
    "        threshold_prior_penalty=THRESHOLD_PRIOR_PENALTY,\n",
    "        cache_namespace=ns,\n",
    "        use_cache=USE_CACHE,\n",
    "        verbose=VERBOSE,\n",
    "    )\n",
    "\n",
    "    best_by_val_final = bakeoff_final['best_by_val_table']\n",
    "    best_runs_final = bakeoff_final['best_runs']\n",
    "\n",
    "    print('Best-by-validation table (one row per model):')\n",
    "    display(best_by_val_final)\n",
    "\n",
    "    # Visual diagnostics for each winner model.\n",
    "    #\n",
    "    # NOTE (important): in bakeoff_utils we currently store model probabilities and\n",
    "    # predicted labels only for the TEST rows (to keep caching smaller and to avoid\n",
    "    # accidental confusion with train/val).\n",
    "    #\n",
    "    # That is why we slice split_role=='test' below.\n",
    "    for model_name, run in best_runs_final.items():\n",
    "        pred = run['predictions'].copy()\n",
    "        df_test_plot = pred[pred['split_role'] == 'test'].copy().reset_index(drop=True)\n",
    "\n",
    "        # Defensive cleaning: visuals and metrics should never see NaNs.\n",
    "        df_test_plot = df_test_plot.dropna(subset=['close', 'target', 'pred_label', 'pred_proba_up']).reset_index(drop=True)\n",
    "\n",
    "        y_true = df_test_plot['target'].to_numpy(dtype=int)\n",
    "        y_pred = df_test_plot['pred_label'].to_numpy(dtype=int)\n",
    "        y_prob = df_test_plot['pred_proba_up'].to_numpy(dtype=float)\n",
    "\n",
    "        evaluate_with_visuals(\n",
    "            df_plot=df_test_plot,\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            y_prob_up=y_prob,\n",
    "            title=f\"FINAL (orb={orb:.2f}) \u2014 {model_name.upper()} \u2014 Full ephemeris (TEST only)\",\n",
    "            vis_cfg=VIS_CFG,\n",
    "            show_visuals=True,\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}